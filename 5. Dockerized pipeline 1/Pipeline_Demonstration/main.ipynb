{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting to the server, creating database and etl user ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create the engine and connect to the PostgreSQL server\n",
    "engine = create_engine('postgresql://postgres:P%4055w076@localhost:5432/cohort_4')\n",
    "connection = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "ProgrammingError",
     "evalue": "(psycopg2.errors.DuplicateDatabase) database \"sampledb\" already exists\n\n[SQL: CREATE DATABASE Sampledb OWNER postgres ENCODING 'UTF8' LC_COLLATE 'English_United States.1252' LC_CTYPE 'English_United States.1252' TABLESPACE pg_default CONNECTION LIMIT -1]\n(Background on this error at: https://sqlalche.me/e/14/f405)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDuplicateDatabase\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sqlalchemy\\engine\\base.py:1771\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[1;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[0;32m   1770\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m evt_handled:\n\u001b[1;32m-> 1771\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdialect\u001b[39m.\u001b[39;49mdo_execute(\n\u001b[0;32m   1772\u001b[0m             cursor, statement, parameters, context\n\u001b[0;32m   1773\u001b[0m         )\n\u001b[0;32m   1775\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_events \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mengine\u001b[39m.\u001b[39m_has_events:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sqlalchemy\\engine\\default.py:717\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[1;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[0;32m    716\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdo_execute\u001b[39m(\u001b[39mself\u001b[39m, cursor, statement, parameters, context\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 717\u001b[0m     cursor\u001b[39m.\u001b[39;49mexecute(statement, parameters)\n",
      "\u001b[1;31mDuplicateDatabase\u001b[0m: database \"sampledb\" already exists\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m     database_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mSampledb\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     10\u001b[0m     create_database_query \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCREATE DATABASE \u001b[39m\u001b[39m{\u001b[39;00mdatabase_name\u001b[39m}\u001b[39;00m\u001b[39m OWNER postgres ENCODING \u001b[39m\u001b[39m'\u001b[39m\u001b[39mUTF8\u001b[39m\u001b[39m'\u001b[39m\u001b[39m LC_COLLATE \u001b[39m\u001b[39m'\u001b[39m\u001b[39mEnglish_United States.1252\u001b[39m\u001b[39m'\u001b[39m\u001b[39m LC_CTYPE \u001b[39m\u001b[39m'\u001b[39m\u001b[39mEnglish_United States.1252\u001b[39m\u001b[39m'\u001b[39m\u001b[39m TABLESPACE pg_default CONNECTION LIMIT -1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 11\u001b[0m     connection\u001b[39m.\u001b[39;49mexecute(create_database_query)\n\u001b[0;32m     13\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     \u001b[39m# Close the connection\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     connection\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sqlalchemy\\engine\\base.py:1248\u001b[0m, in \u001b[0;36mConnection.execute\u001b[1;34m(self, statement, *multiparams, **params)\u001b[0m\n\u001b[0;32m   1239\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(statement, util\u001b[39m.\u001b[39mstring_types):\n\u001b[0;32m   1240\u001b[0m     util\u001b[39m.\u001b[39mwarn_deprecated_20(\n\u001b[0;32m   1241\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPassing a string to Connection.execute() is \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1242\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdeprecated and will be removed in version 2.0.  Use the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1245\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdriver-level SQL string.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1246\u001b[0m     )\n\u001b[1;32m-> 1248\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_exec_driver_sql(\n\u001b[0;32m   1249\u001b[0m         statement,\n\u001b[0;32m   1250\u001b[0m         multiparams,\n\u001b[0;32m   1251\u001b[0m         params,\n\u001b[0;32m   1252\u001b[0m         _EMPTY_EXECUTION_OPTS,\n\u001b[0;32m   1253\u001b[0m         future\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   1254\u001b[0m     )\n\u001b[0;32m   1256\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1257\u001b[0m     meth \u001b[39m=\u001b[39m statement\u001b[39m.\u001b[39m_execute_on_connection\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sqlalchemy\\engine\\base.py:1547\u001b[0m, in \u001b[0;36mConnection._exec_driver_sql\u001b[1;34m(self, statement, multiparams, params, execution_options, future)\u001b[0m\n\u001b[0;32m   1538\u001b[0m         (\n\u001b[0;32m   1539\u001b[0m             distilled_params,\n\u001b[0;32m   1540\u001b[0m             event_multiparams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1543\u001b[0m             statement, distilled_parameters, execution_options\n\u001b[0;32m   1544\u001b[0m         )\n\u001b[0;32m   1546\u001b[0m dialect \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdialect\n\u001b[1;32m-> 1547\u001b[0m ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_context(\n\u001b[0;32m   1548\u001b[0m     dialect,\n\u001b[0;32m   1549\u001b[0m     dialect\u001b[39m.\u001b[39;49mexecution_ctx_cls\u001b[39m.\u001b[39;49m_init_statement,\n\u001b[0;32m   1550\u001b[0m     statement,\n\u001b[0;32m   1551\u001b[0m     distilled_parameters,\n\u001b[0;32m   1552\u001b[0m     execution_options,\n\u001b[0;32m   1553\u001b[0m     statement,\n\u001b[0;32m   1554\u001b[0m     distilled_parameters,\n\u001b[0;32m   1555\u001b[0m )\n\u001b[0;32m   1557\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m future:\n\u001b[0;32m   1558\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_events \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mengine\u001b[39m.\u001b[39m_has_events:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sqlalchemy\\engine\\base.py:1814\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[1;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[0;32m   1811\u001b[0m             branched\u001b[39m.\u001b[39mclose()\n\u001b[0;32m   1813\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m-> 1814\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle_dbapi_exception(\n\u001b[0;32m   1815\u001b[0m         e, statement, parameters, cursor, context\n\u001b[0;32m   1816\u001b[0m     )\n\u001b[0;32m   1818\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sqlalchemy\\engine\\base.py:1995\u001b[0m, in \u001b[0;36mConnection._handle_dbapi_exception\u001b[1;34m(self, e, statement, parameters, cursor, context)\u001b[0m\n\u001b[0;32m   1993\u001b[0m     util\u001b[39m.\u001b[39mraise_(newraise, with_traceback\u001b[39m=\u001b[39mexc_info[\u001b[39m2\u001b[39m], from_\u001b[39m=\u001b[39me)\n\u001b[0;32m   1994\u001b[0m \u001b[39melif\u001b[39;00m should_wrap:\n\u001b[1;32m-> 1995\u001b[0m     util\u001b[39m.\u001b[39;49mraise_(\n\u001b[0;32m   1996\u001b[0m         sqlalchemy_exception, with_traceback\u001b[39m=\u001b[39;49mexc_info[\u001b[39m2\u001b[39;49m], from_\u001b[39m=\u001b[39;49me\n\u001b[0;32m   1997\u001b[0m     )\n\u001b[0;32m   1998\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1999\u001b[0m     util\u001b[39m.\u001b[39mraise_(exc_info[\u001b[39m1\u001b[39m], with_traceback\u001b[39m=\u001b[39mexc_info[\u001b[39m2\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sqlalchemy\\util\\compat.py:207\u001b[0m, in \u001b[0;36mraise_\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    204\u001b[0m     exception\u001b[39m.\u001b[39m__cause__ \u001b[39m=\u001b[39m replace_context\n\u001b[0;32m    206\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 207\u001b[0m     \u001b[39mraise\u001b[39;00m exception\n\u001b[0;32m    208\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     \u001b[39m# credit to\u001b[39;00m\n\u001b[0;32m    210\u001b[0m     \u001b[39m# https://cosmicpercolator.com/2016/01/13/exception-leaks-in-python-2-and-3/\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     \u001b[39m# as the __traceback__ object creates a cycle\u001b[39;00m\n\u001b[0;32m    212\u001b[0m     \u001b[39mdel\u001b[39;00m exception, replace_context, from_, with_traceback\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sqlalchemy\\engine\\base.py:1771\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[1;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[0;32m   1769\u001b[0m                 \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m   1770\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m evt_handled:\n\u001b[1;32m-> 1771\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdialect\u001b[39m.\u001b[39;49mdo_execute(\n\u001b[0;32m   1772\u001b[0m             cursor, statement, parameters, context\n\u001b[0;32m   1773\u001b[0m         )\n\u001b[0;32m   1775\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_events \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mengine\u001b[39m.\u001b[39m_has_events:\n\u001b[0;32m   1776\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mafter_cursor_execute(\n\u001b[0;32m   1777\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[0;32m   1778\u001b[0m         cursor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1782\u001b[0m         context\u001b[39m.\u001b[39mexecutemany,\n\u001b[0;32m   1783\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sqlalchemy\\engine\\default.py:717\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[1;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[0;32m    716\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdo_execute\u001b[39m(\u001b[39mself\u001b[39m, cursor, statement, parameters, context\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 717\u001b[0m     cursor\u001b[39m.\u001b[39;49mexecute(statement, parameters)\n",
      "\u001b[1;31mProgrammingError\u001b[0m: (psycopg2.errors.DuplicateDatabase) database \"sampledb\" already exists\n\n[SQL: CREATE DATABASE Sampledb OWNER postgres ENCODING 'UTF8' LC_COLLATE 'English_United States.1252' LC_CTYPE 'English_United States.1252' TABLESPACE pg_default CONNECTION LIMIT -1]\n(Background on this error at: https://sqlalche.me/e/14/f405)"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create a connection to the PostgreSQL server without a transaction\n",
    "engine = create_engine('postgresql://postgres:P%4055w076@localhost:5432/cohort_4', isolation_level='AUTOCOMMIT')\n",
    "connection = engine.connect()\n",
    "\n",
    "try:\n",
    "    # Create the database\n",
    "    database_name = 'Sampledb'\n",
    "    create_database_query = f\"CREATE DATABASE {database_name} OWNER postgres ENCODING 'UTF8' LC_COLLATE 'English_United States.1252' LC_CTYPE 'English_United States.1252' TABLESPACE pg_default CONNECTION LIMIT -1\"\n",
    "    connection.execute(create_database_query)\n",
    "\n",
    "finally:\n",
    "    # Close the connection\n",
    "    connection.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "ProgrammingError",
     "evalue": "(psycopg2.errors.DuplicateObject) role \"etl\" already exists\n\n[SQL: \nCREATE USER etl PASSWORD 'demopass';\nGRANT CONNECT ON DATABASE Sampledb TO etl;\nGRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO etl;\n]\n(Background on this error at: https://sqlalche.me/e/14/f405)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDuplicateObject\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sqlalchemy\\engine\\base.py:1771\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[1;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[0;32m   1770\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m evt_handled:\n\u001b[1;32m-> 1771\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdialect\u001b[39m.\u001b[39;49mdo_execute(\n\u001b[0;32m   1772\u001b[0m             cursor, statement, parameters, context\n\u001b[0;32m   1773\u001b[0m         )\n\u001b[0;32m   1775\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_events \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mengine\u001b[39m.\u001b[39m_has_events:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sqlalchemy\\engine\\default.py:717\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[1;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[0;32m    716\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdo_execute\u001b[39m(\u001b[39mself\u001b[39m, cursor, statement, parameters, context\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 717\u001b[0m     cursor\u001b[39m.\u001b[39;49mexecute(statement, parameters)\n",
      "\u001b[1;31mDuplicateObject\u001b[0m: role \"etl\" already exists\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 15\u001b[0m\n\u001b[0;32m      9\u001b[0m     database_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mSampledb\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     10\u001b[0m     create_user_query \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'''\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[39mCREATE USER etl PASSWORD \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdemopass\u001b[39m\u001b[39m'\u001b[39m\u001b[39m;\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[39mGRANT CONNECT ON DATABASE Sampledb TO etl;\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[39mGRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO etl;\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[39m'''\u001b[39m\n\u001b[1;32m---> 15\u001b[0m     connection\u001b[39m.\u001b[39;49mexecute(create_user_query)\n\u001b[0;32m     17\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     18\u001b[0m     \u001b[39m# Close the connection\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     connection\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sqlalchemy\\engine\\base.py:1248\u001b[0m, in \u001b[0;36mConnection.execute\u001b[1;34m(self, statement, *multiparams, **params)\u001b[0m\n\u001b[0;32m   1239\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(statement, util\u001b[39m.\u001b[39mstring_types):\n\u001b[0;32m   1240\u001b[0m     util\u001b[39m.\u001b[39mwarn_deprecated_20(\n\u001b[0;32m   1241\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPassing a string to Connection.execute() is \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1242\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdeprecated and will be removed in version 2.0.  Use the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1245\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdriver-level SQL string.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1246\u001b[0m     )\n\u001b[1;32m-> 1248\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_exec_driver_sql(\n\u001b[0;32m   1249\u001b[0m         statement,\n\u001b[0;32m   1250\u001b[0m         multiparams,\n\u001b[0;32m   1251\u001b[0m         params,\n\u001b[0;32m   1252\u001b[0m         _EMPTY_EXECUTION_OPTS,\n\u001b[0;32m   1253\u001b[0m         future\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   1254\u001b[0m     )\n\u001b[0;32m   1256\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1257\u001b[0m     meth \u001b[39m=\u001b[39m statement\u001b[39m.\u001b[39m_execute_on_connection\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sqlalchemy\\engine\\base.py:1547\u001b[0m, in \u001b[0;36mConnection._exec_driver_sql\u001b[1;34m(self, statement, multiparams, params, execution_options, future)\u001b[0m\n\u001b[0;32m   1538\u001b[0m         (\n\u001b[0;32m   1539\u001b[0m             distilled_params,\n\u001b[0;32m   1540\u001b[0m             event_multiparams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1543\u001b[0m             statement, distilled_parameters, execution_options\n\u001b[0;32m   1544\u001b[0m         )\n\u001b[0;32m   1546\u001b[0m dialect \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdialect\n\u001b[1;32m-> 1547\u001b[0m ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_context(\n\u001b[0;32m   1548\u001b[0m     dialect,\n\u001b[0;32m   1549\u001b[0m     dialect\u001b[39m.\u001b[39;49mexecution_ctx_cls\u001b[39m.\u001b[39;49m_init_statement,\n\u001b[0;32m   1550\u001b[0m     statement,\n\u001b[0;32m   1551\u001b[0m     distilled_parameters,\n\u001b[0;32m   1552\u001b[0m     execution_options,\n\u001b[0;32m   1553\u001b[0m     statement,\n\u001b[0;32m   1554\u001b[0m     distilled_parameters,\n\u001b[0;32m   1555\u001b[0m )\n\u001b[0;32m   1557\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m future:\n\u001b[0;32m   1558\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_events \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mengine\u001b[39m.\u001b[39m_has_events:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sqlalchemy\\engine\\base.py:1814\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[1;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[0;32m   1811\u001b[0m             branched\u001b[39m.\u001b[39mclose()\n\u001b[0;32m   1813\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m-> 1814\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle_dbapi_exception(\n\u001b[0;32m   1815\u001b[0m         e, statement, parameters, cursor, context\n\u001b[0;32m   1816\u001b[0m     )\n\u001b[0;32m   1818\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sqlalchemy\\engine\\base.py:1995\u001b[0m, in \u001b[0;36mConnection._handle_dbapi_exception\u001b[1;34m(self, e, statement, parameters, cursor, context)\u001b[0m\n\u001b[0;32m   1993\u001b[0m     util\u001b[39m.\u001b[39mraise_(newraise, with_traceback\u001b[39m=\u001b[39mexc_info[\u001b[39m2\u001b[39m], from_\u001b[39m=\u001b[39me)\n\u001b[0;32m   1994\u001b[0m \u001b[39melif\u001b[39;00m should_wrap:\n\u001b[1;32m-> 1995\u001b[0m     util\u001b[39m.\u001b[39;49mraise_(\n\u001b[0;32m   1996\u001b[0m         sqlalchemy_exception, with_traceback\u001b[39m=\u001b[39;49mexc_info[\u001b[39m2\u001b[39;49m], from_\u001b[39m=\u001b[39;49me\n\u001b[0;32m   1997\u001b[0m     )\n\u001b[0;32m   1998\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1999\u001b[0m     util\u001b[39m.\u001b[39mraise_(exc_info[\u001b[39m1\u001b[39m], with_traceback\u001b[39m=\u001b[39mexc_info[\u001b[39m2\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sqlalchemy\\util\\compat.py:207\u001b[0m, in \u001b[0;36mraise_\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    204\u001b[0m     exception\u001b[39m.\u001b[39m__cause__ \u001b[39m=\u001b[39m replace_context\n\u001b[0;32m    206\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 207\u001b[0m     \u001b[39mraise\u001b[39;00m exception\n\u001b[0;32m    208\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     \u001b[39m# credit to\u001b[39;00m\n\u001b[0;32m    210\u001b[0m     \u001b[39m# https://cosmicpercolator.com/2016/01/13/exception-leaks-in-python-2-and-3/\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     \u001b[39m# as the __traceback__ object creates a cycle\u001b[39;00m\n\u001b[0;32m    212\u001b[0m     \u001b[39mdel\u001b[39;00m exception, replace_context, from_, with_traceback\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sqlalchemy\\engine\\base.py:1771\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[1;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[0;32m   1769\u001b[0m                 \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m   1770\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m evt_handled:\n\u001b[1;32m-> 1771\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdialect\u001b[39m.\u001b[39;49mdo_execute(\n\u001b[0;32m   1772\u001b[0m             cursor, statement, parameters, context\n\u001b[0;32m   1773\u001b[0m         )\n\u001b[0;32m   1775\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_events \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mengine\u001b[39m.\u001b[39m_has_events:\n\u001b[0;32m   1776\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mafter_cursor_execute(\n\u001b[0;32m   1777\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[0;32m   1778\u001b[0m         cursor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1782\u001b[0m         context\u001b[39m.\u001b[39mexecutemany,\n\u001b[0;32m   1783\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sqlalchemy\\engine\\default.py:717\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[1;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[0;32m    716\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdo_execute\u001b[39m(\u001b[39mself\u001b[39m, cursor, statement, parameters, context\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 717\u001b[0m     cursor\u001b[39m.\u001b[39;49mexecute(statement, parameters)\n",
      "\u001b[1;31mProgrammingError\u001b[0m: (psycopg2.errors.DuplicateObject) role \"etl\" already exists\n\n[SQL: \nCREATE USER etl PASSWORD 'demopass';\nGRANT CONNECT ON DATABASE Sampledb TO etl;\nGRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO etl;\n]\n(Background on this error at: https://sqlalche.me/e/14/f405)"
     ]
    }
   ],
   "source": [
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create a connection to the PostgreSQL server without a transaction\n",
    "engine = create_engine('postgresql://postgres:P%4055w076@localhost:5432/cohort_4', isolation_level='AUTOCOMMIT')\n",
    "connection = engine.connect()\n",
    "\n",
    "try:\n",
    "    # Create the user\n",
    "    database_name = 'Sampledb'\n",
    "    create_user_query = f'''\n",
    "CREATE USER etl PASSWORD 'demopass';\n",
    "GRANT CONNECT ON DATABASE Sampledb TO etl;\n",
    "GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO etl;\n",
    "'''\n",
    "    connection.execute(create_user_query)\n",
    "\n",
    "finally:\n",
    "    # Close the connection\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting to website API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'OK',\n",
       " 'request_id': 'f2ef9808-c6e6-4a7d-a409-0b0bfcb92f37',\n",
       " 'parameters': {'query': 'data engineer in ontario, canada',\n",
       "  'page': 1,\n",
       "  'num_pages': 1,\n",
       "  'date_posted': 'month'},\n",
       " 'data': [{'employer_name': 'Wave HQ',\n",
       "   'employer_logo': 'https://upload.wikimedia.org/wikipedia/commons/1/15/Wave_logo_RGB.png',\n",
       "   'employer_website': 'http://www.wavegp.com',\n",
       "   'employer_company_type': 'Information',\n",
       "   'job_publisher': 'LinkedIn',\n",
       "   'job_id': 'BiwWJEFt2-cAAAAAAAAAAA==',\n",
       "   'job_employment_type': 'FULLTIME',\n",
       "   'job_title': 'Data Engineer',\n",
       "   'job_apply_link': 'https://ca.linkedin.com/jobs/view/data-engineer-at-wave-hq-3665942790',\n",
       "   'job_apply_is_direct': False,\n",
       "   'job_apply_quality_score': 0.6017,\n",
       "   'job_description': \"About the Role:\\n\\nReporting to the Senior Manager of Data Platform and Operations, as a Data Engineer you will be building tools and infrastructure to support efforts of the analytics, ML and the business as a whole.\\n\\nWe’re looking for a talented, curious self-starter who is driven to solve complex problems and can juggle multiple domains and stakeholders. This highly technical individual will collaborate with all levels of the Data team as well as the various engineering teams to develop data solutions, scale our data infrastructure and advance Wave to the next stage in our transformation as a data-centric organization.\\n\\nThis role is for someone with proven experience in complicated product environments. Strong communication skills are a must to bridge the gap between technical and non-technical audiences across a spectrum of data maturity.\\n\\nHere's how you'll make a difference:\\n• You’re a builder. You’ll be responsible for the design, build and deployment of our data pipelines - batch, incremental and stream-based.\\n• You’ll make things better. You will collaborate within a cross-functional team in the planning and roll-out of data infrastructure services.\\n• You'll build relationships. As a strong software engineer who works with data, you’ll have people coming to you for technical assistance. You will be helping them succeed, and your outstanding ability to communicate with people will help them do that.\\n• We love our customers at Wave. Your customers are internal, and external too. You can take a look at existing structures and systems and know how to help our internal customers surface the data they need to excel in serving our external customers.\\n• You’ll drive process and tool improvements to enable data-driven decisions across Wave. Your work will mean something and have an impact on the company - our team relies on data, analytics and ML insights being delivered reliably to make smarter business decisions.\\n\\nYou'll thrive here if:\\n• You’re self-motivated and have the ability to work autonomously. No one’s going to be peering over your shoulder here. We count on you to get your work done, in ambiguous conditions, with tight deadlines, while still producing high-quality work.\\n• You are all about collaboration. You enjoy working with different teams across Wave. We follow Scrum practices within an agile framework.\\n• You value personal and team development. You enjoy mentoring junior engineers in honing new skills, while helping your team to identify the most important aspects of engineering and best practices.\\n• You are a stellar communicator. This means you know how to translate technical terms into non-technical language that your grandma could understand.\\n• Enjoy the challenge of helping us build and manage a fault-tolerant data platform that scales.\\n\\nThese will help you succeed:\\n• At least 3 years of experience in data engineering, specifically in building data pipelines and data infrastructure. This is important because this is what you’ll be doing most of the time, and we need someone who’s done this a lot.\\n• At least 3 years of experience working with cloud infrastructure, including container development with Kubernetes and Docker infrastructure as code (IaC) using Terraform and GitOps or other infrastructure automation on AWS.\\n• Experience building messaging and stream processing capabilities using Confluent or Kafka MSK and its related components.\\n• Experience working with multi-stage workflows using serverless services.\\n• Previous experience building data lakes using Delta Lake or Apache Hudi.\\n• Experience performing hands-on development, leading code reviews and testing, and leveraging automated frameworks.\\n• Experience developing and deploying solutions leveraging CI/CD processes to orchestrate automated batch and NRT (Near Real Time) pipelines running AWS Glue and dbt data transformations.\\n• Experience using Python, SQL and dbt.\\n• Experience working with cloud integration tools such as AWS Glue or AWS EMR.\\n• Working knowledge of data integration tools such as FiveTran, Stitch and Census.\\n• Knowledge and practical experience with Data Vault 2.0 on Redshift or another data warehouse is a definite bonus!\",\n",
       "   'job_is_remote': False,\n",
       "   'job_posted_at_timestamp': 1689349494,\n",
       "   'job_posted_at_datetime_utc': '2023-07-14T15:44:54.000Z',\n",
       "   'job_city': 'Toronto',\n",
       "   'job_state': 'ON',\n",
       "   'job_country': 'CA',\n",
       "   'job_latitude': 43.653225,\n",
       "   'job_longitude': -79.38319,\n",
       "   'job_benefits': None,\n",
       "   'job_google_link': 'https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+ontario,+canada&start=0&chips=date_posted:month&schips=date_posted;month&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+ontario,+canada&htidocid=BiwWJEFt2-cAAAAAAAAAAA%3D%3D',\n",
       "   'job_offer_expiration_datetime_utc': '2023-08-13T15:44:54.000Z',\n",
       "   'job_offer_expiration_timestamp': 1691941494,\n",
       "   'job_required_experience': {'no_experience_required': False,\n",
       "    'required_experience_in_months': 36,\n",
       "    'experience_mentioned': True,\n",
       "    'experience_preferred': False},\n",
       "   'job_required_skills': None,\n",
       "   'job_required_education': {'postgraduate_degree': False,\n",
       "    'professional_certification': False,\n",
       "    'high_school': False,\n",
       "    'associates_degree': False,\n",
       "    'bachelors_degree': True,\n",
       "    'degree_mentioned': True,\n",
       "    'degree_preferred': False,\n",
       "    'professional_certification_mentioned': False},\n",
       "   'job_experience_in_place_of_education': False,\n",
       "   'job_min_salary': None,\n",
       "   'job_max_salary': None,\n",
       "   'job_salary_currency': None,\n",
       "   'job_salary_period': None,\n",
       "   'job_highlights': {},\n",
       "   'job_job_title': 'Data engineer',\n",
       "   'job_posting_language': 'en',\n",
       "   'job_onet_soc': '15113200',\n",
       "   'job_onet_job_zone': '4',\n",
       "   'job_naics_code': '5112',\n",
       "   'job_naics_name': 'Software Publishers'},\n",
       "  {'employer_name': 'Sun Life Financial',\n",
       "   'employer_logo': 'https://www.sunlife.ca/content/dam/sunlife/folder1/Sun_Life_weblogo_127x31.svg',\n",
       "   'employer_website': 'http://www.sunlife.com',\n",
       "   'employer_company_type': 'Finance',\n",
       "   'job_publisher': 'The Vector Digital Talent Hub',\n",
       "   'job_id': 'FVDNLvtQe-UAAAAAAAAAAA==',\n",
       "   'job_employment_type': 'FULLTIME',\n",
       "   'job_title': 'Senior Data Engineer',\n",
       "   'job_apply_link': 'https://talenthub.vectorinstitute.ai/jobs/243155652-senior-data-engineer-at-sun-life-financial',\n",
       "   'job_apply_is_direct': False,\n",
       "   'job_apply_quality_score': 0.614,\n",
       "   'job_description': \"You are as unique as your background, experience and point of view. Here, you’ll be encouraged, empowered and challenged to be your best self. You'll work with dynamic colleagues - experts in their fields - who are eager to share their knowledge with you. Your leaders will inspire and help you reach your potential and soar to new heights. Every day, you'll have new and exciting opportunities to make life brighter for our Clients - who are at the heart of everything we do. Discover how you can make a difference in the lives of individuals, families and communities around the world.\\n\\nJob Description:\\n\\nAt Sun Life, we work together, share common values and encourage growth and achievement. We offer many career paths that attract a wide variety of talent and skills. Follow a path that lets your talents shine.\\n\\nRole Summary:\\n\\nThe Client Experience Office (CXO) is an important pillar of Sun Life’s corporate strategy and as such, is seeking experienced data engineering individuals with a proven track record working with data consumers to extract meaningful insights from enterprise data. Senior Business data engineers play an indispensable role in assembling the necessary data products that fuel myriad of analytical models used by our various business units across Canada.\\n\\nReporting to the Manager of Data Engineering, the Senior Business Data Engineer will be a key member of Data Enablement Team responsible for building enriched data products that provide deep insights about our Client, Advisor, Product, Transactions along with their intricate relationships. The Senior Business Data Engineer will work with data analysts and data scientists to help answer complex business enquires such as identifying new business opportunities, understanding our client behavior and optimizing areas of our sales cycles.\\n\\nWhat will you do?\\n• Obsessed with understanding our enterprise data and interpreting it in the best way possible to derive business value\\n• Design and engineer data models that represent our key business entities such as our Advisor, Client & Product\\n• Ensure high level of data quality and data integrity are maintained across data sources for business\\n• Convert complex business and technical rules into logic for data flows and data pipelines\\n• Work with our Data Science teams to implement data products for various use cases\\n• Collaborate with stakeholders and incorporate into solution design for various use case requirements\\n• Standardize metadata into a common glossary along with the necessary documentation for data consumers\\n\\nWhat do you need to succeed?\\n• 3-5 years hands-on experience working on AWS technologies for data processing and analytics\\n• 2+ years experience with AWS Glue for ETL\\n• 3 years experience in Pyspark and SQL\\n• 5 years hands-on experience with relational database systems, relational models, dimensional models etc.\\n• 3-5 years experience with engineering data products as input to various analytical models\\n• 3-5 years building feedback loops between model deployment and its data – i.e. tune & tweak data products to achieve scale, optimization, etc.\\n• Sound understanding of data management principles (data warehousing, data quality, master data management, etc.)\\n• Sound understanding of data modelling and a passion for analytics\\n• Proven track record in leveraging SQL and SQL-based programming to solve business problems\\n• Ability to translate complex business requirements into a set of data ingestion pseudocode\\n• Proven ability to leverage knowledge of data engineering to extract, conform and integrate a variety of operational data sources into pro‐duction-grade data products\\n• Demonstrate blend of tenacity, creativity, and discipline required to develop ground breaking self-serve data models that data scientists can consume\\n• Excellent verbal and written communication skills; have executive presence and proven ability to partner with Data Scientists to articulate insights gleaned from the model and the underlying data\\n\\nEducation and Qualifications:\\n• Undergraduate degree in Computer Science, Mathematics, Engineering, or equivalent\\n• Graduate degree in business or quantitative science strongly preferred\\n\\nUnique Requirements\\n• The candidate selected for this role is required to attain Canadian Reliability Security Clearance (administered by submitting fingerprints to the RCMP, who then conduct min. 5 year history checks)\\n• To see if you are eligible for this clearance, please review the section 201 on the Federal Government site (https://www.tpsgc-pwgsc.gc.ca/esc-src/personnel/pdcf-rsrp-eng.html)\\n\\nWhat would be nice to have?\\n• Understand and Embrace Agile Operating Model\\n• Proven Experienced working as Product Owner in Scrum or Service Request Manager in Kanban projects\\n• Experience working in a large financial services organization\\n• Knowledge of fundamental statistical models & techniques\\n\\nWhat's In It For You?\\n• Great Place to Work® Certified in Canada and the US - 2022\\n• Great Place to Work® award for Best Workplaces for #HybridWork - 2022\\n• Named “Best Places to Work” by Glassdoor - 2021\\n• Canada Award for Excellence for Mental Health at Work - 2021\\n• Flex hours and Flexible hybrid work model including in-country work-from-home if you prefer. #LI-Hybrid, #LI-Remote\\n• Competitive salary and bonus structure influenced by market range data\\n• Pension, stock and savings programs to help build and enhance your future financial security\\n• A friendly, collaborative, and inclusive culture\\n• Be part of our continuous improvement journey in developing the next greatest digital enterprise experience.\\n• Work and professional development that is united by our Purpose: to help Clients and Employees achieve lifetime financial security and live healthier lives\\n\\nThe Base Pay range is for the primary location for which the job is posted. It may vary depending on the work location of the successful candidate or other factors. In addition to Base Pay, eligible Sun Life employees participate in various incentive plans, payment under which is discretionary and subject to individual and company performance. Certain sales focused roles have sales incentive plans based on individual or group sales results.\\n\\nDiversity and inclusion have always been at the core of our values at Sun Life. A diverse workforce with wide perspectives and creative ideas benefits our clients, the communities where we operate and all of us as colleagues. We welcome applications from qualified individuals from all backgrounds.\\n\\nPersons with disabilities who need accommodation in the application process or those needing job postings in an alternative format may e-mail a request to thebrightside@sunlife.com.\\n\\nAt Sun Life we strive to create a flexible work environment where our employees are empowered to do their best work. Several flexible work options are available and can be discussed throughout the selection process depending on the role requirements and individual needs.\\n\\nWe thank all applicants for showing an interest in this position. Only those selected for an interview will be contacted.\\n\\nSalary Range:\\n\\n74,100/74 100 - 120,800/120 800\\n\\nJob Category:\\n\\nAdvanced Analytics\\n\\nPosting End Date:\\n\\n09/07/2023\",\n",
       "   'job_is_remote': False,\n",
       "   'job_posted_at_timestamp': 1688428800,\n",
       "   'job_posted_at_datetime_utc': '2023-07-04T00:00:00.000Z',\n",
       "   'job_city': 'Toronto',\n",
       "   'job_state': 'ON',\n",
       "   'job_country': 'CA',\n",
       "   'job_latitude': 43.653225,\n",
       "   'job_longitude': -79.38319,\n",
       "   'job_benefits': None,\n",
       "   'job_google_link': 'https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+ontario,+canada&start=0&chips=date_posted:month&schips=date_posted;month&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+ontario,+canada&htidocid=FVDNLvtQe-UAAAAAAAAAAA%3D%3D',\n",
       "   'job_offer_expiration_datetime_utc': '2023-08-03T17:59:19.000Z',\n",
       "   'job_offer_expiration_timestamp': 1691085559,\n",
       "   'job_required_experience': {'no_experience_required': False,\n",
       "    'required_experience_in_months': 36,\n",
       "    'experience_mentioned': True,\n",
       "    'experience_preferred': False},\n",
       "   'job_required_skills': None,\n",
       "   'job_required_education': {'postgraduate_degree': False,\n",
       "    'professional_certification': False,\n",
       "    'high_school': False,\n",
       "    'associates_degree': False,\n",
       "    'bachelors_degree': False,\n",
       "    'degree_mentioned': True,\n",
       "    'degree_preferred': True,\n",
       "    'professional_certification_mentioned': True},\n",
       "   'job_experience_in_place_of_education': False,\n",
       "   'job_min_salary': None,\n",
       "   'job_max_salary': None,\n",
       "   'job_salary_currency': None,\n",
       "   'job_salary_period': None,\n",
       "   'job_highlights': {},\n",
       "   'job_job_title': 'Data engineer',\n",
       "   'job_posting_language': 'en',\n",
       "   'job_onet_soc': '15113200',\n",
       "   'job_onet_job_zone': '4',\n",
       "   'job_naics_code': '524113',\n",
       "   'job_naics_name': 'Direct Life Insurance Carriers'},\n",
       "  {'employer_name': '407 ETR',\n",
       "   'employer_logo': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSYDLjWbqYtzrfSCbKZ8L4SUqphbdAh4YWKpOOZ&s=0',\n",
       "   'employer_website': None,\n",
       "   'employer_company_type': None,\n",
       "   'job_publisher': 'LinkedIn',\n",
       "   'job_id': 'BwdJN3KD3wIAAAAAAAAAAA==',\n",
       "   'job_employment_type': 'FULLTIME',\n",
       "   'job_title': 'Data Engineer',\n",
       "   'job_apply_link': 'https://ca.linkedin.com/jobs/view/data-engineer-at-407-etr-3662209673',\n",
       "   'job_apply_is_direct': False,\n",
       "   'job_apply_quality_score': 0.6017,\n",
       "   'job_description': 'As an AWS Data Engineer, you know the importance of data to the business. You design and set up solutions that bring together information from a variety of sources, to enable analysis and decision-making. 407 ETR’s Data & Analytics team build solutions that help our clients solve their most complex business challenges. You will design and build cutting-edge cloud solutions in team-based projects, and develop the skills of those around you, technically and professionally.\\n\\nDuties and Responsibilities:\\n• Work with stakeholders to understand data sources and\\u202fData, Analytics and Reporting team\\u202fstrategy in supporting\\u202fwithin our on-premises environment and enterprise AWS cloud solution\\n• Work closely with\\u202fData, Analytics and Reporting\\u202fData Management and Data Governance teams to ensure all industry standards and best practices are met\\n• Ensure metadata and data lineage is captured and compatible with enterprise metadata and data management tools and processes\\n• Run quality assurance and data integrity checks to ensure accurate reporting and data records\\n• Ensure ETL pipelines are produced with the highest quality standards, metadata and validated for completeness and accuracy\\n• Develops and maintains scalable data pipelines and builds out new API integrations to support continuing increases in data volume and complexity.\\n• Collaborates with analytics and business teams to improve data models that feed business intelligence tools, increasing data accessibility, and fostering data-driven decision making across the organization.\\n• Implements processes and systems to monitor data quality, ensuring production data is always accurate and available for key stakeholders and business processes that depend on it.\\n• Writes unit/integration tests, contributes to engineering wiki, and documents work.\\n• Performs data analysis required to troubleshoot data related issues and assist in the resolution of data issues.\\n• Defines company data assets (data models), spark, sparkSQL jobs to populate data models.\\n• Designs data integrations and data quality framework.\\n• Designs and evaluates open source and vendor tools for data lineage.\\n• Works closely with all business units and engineering teams to develop strategy for long term data platform architecture.\\n• Database, storage, collection and aggregation models, techniques, and technologies – and how to apply them in business\\n• Focusing on structured problem solving\\n• Phenomenal communication and business awareness\\n• Working with ETL tools, Querying languages, and data repositories\\n• Working experience with the following Cloud platforms is a plus: Amazon Web Services, Google Cloud Platform, Azure\\u202f\\n• Support of technical Data Management Solutions\\n• Provide support to the development and testing teams to resolve data issues\\n• Working knowledge of source code control tool such as GIT\\n• Knowledge about file formats (e.g. XML, CSV, JSON), databases (e.g. Redshift, Oracle) and different type of connectivity is also very useful.\\n• Working experience with data modeling, relational modeling, and dimensional modeling.\\n• The interpersonal skills. You have a way of speaking that engages your audience and instills confidence and credibility. You know how to leverage communication tools and methodologies. You can build relationships internal and external team members, positioning yourself as a trusted advisor. You are always looking for ways to improve processes, and you always ensure your communications have been received and are clearly understood. Your commitment and focus influence those around you to do better\\n\\n\\u202fQualifications:\\n• A Strong foundation in Object-Oriented Design and Programming (Design Patterns, SOLID principles, etc.)\\n• Experience in projects involving AWS Lamba, Glue, Redshift, Step Functions, CloudFormation Athena, QuickSight, and AppFlow\\n• Must have intermediate to advanced-level programming experience in Python and SQL\\n• Experience delivering continuous integration/continuous delivery pipelines\\n• Capability to perform Performance analysis, troubleshooting, and remediation\\n• Understanding of cloud ecosystem and leading-edge cloud emerging technologies\\n• A Software Developer that obsesses over their craft to write smart, simple, and clean code\\n• Experience with Test-Driven Development\\n• Experience with refactoring, and able to manage technical debt within each iteration\\n• Comfortable with Collective Code Ownership and Pair Programming\\n• Understand and able to balance the trade-offs between up-front and emergent architecture\\n• Skilled with Automated Builds and Continuous Integration\\n• Experience with data models and DB interface development and optimization\\n• Skilled with Linux including scripting\\n• Experience with SFDC is an asset\\n• Bachelor’s degree in Computer Science, Computer Engineering or equivalent training and experience.\\n• 5 years related experience in data engineering\\n\\nOur Offer\\n• Free 407ETR usage\\n• Competitive salary, bonus structure\\n• Competitive vacation package\\n• Hybrid with flexible working hours\\n• Community culture\\n• Continued education budget\\n• Expect excellence: Collaborative team, learn, and grow with a high-performance team.\\n\\nAbout 407ETR\\n\\nHighway 407 ETR is an all-electronic open-access toll highway located in the Greater Toronto Area. The highway spans 108 kilometres from Burlington in the west to Pickering in the east.\\n\\nOn Highway 407 ETR, we offer peace of mind and a fast, safe and reliable trip. That’s why millions of drivers rely on Highway 407 ETR to get around the GTA. Every driver we serve is one less vehicle on a congested alternate route, giving drivers more time for the things that matter most.\\n\\n407 International Inc. is the sole shareholder of 407 ETR and is owned by:\\n• Canada Pension Plan Investment Board (CPP Investments) through indirectly-owned subsidiaries (50.01%);\\n• Cintra Global S.E. which is a wholly-owned subsidiary of Ferrovial S.A. (43.23%); and\\n• SNC-Lavalin (6.76%).\\n\\nWe’re a majority Canadian owned company and employ over 460 Ontarians. Learn more at 407etr.com\\n\\nNote: This job description is not intended to be all-inclusive. Employee may perform other related duties as negotiated to meet the ongoing needs of the organization.\\n\\nAccommodations for disabilities or other grounds protected by human rights legislation are available upon request for candidates taking part in all aspects of the employment selection process.',\n",
       "   'job_is_remote': False,\n",
       "   'job_posted_at_timestamp': 1689188834,\n",
       "   'job_posted_at_datetime_utc': '2023-07-12T19:07:14.000Z',\n",
       "   'job_city': None,\n",
       "   'job_state': 'ON',\n",
       "   'job_country': 'CA',\n",
       "   'job_latitude': 51.253777,\n",
       "   'job_longitude': -85.32321,\n",
       "   'job_benefits': None,\n",
       "   'job_google_link': 'https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+ontario,+canada&start=0&chips=date_posted:month&schips=date_posted;month&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+ontario,+canada&htidocid=BwdJN3KD3wIAAAAAAAAAAA%3D%3D',\n",
       "   'job_offer_expiration_datetime_utc': '2023-08-11T19:07:14.000Z',\n",
       "   'job_offer_expiration_timestamp': 1691780834,\n",
       "   'job_required_experience': {'no_experience_required': False,\n",
       "    'required_experience_in_months': 60,\n",
       "    'experience_mentioned': True,\n",
       "    'experience_preferred': True},\n",
       "   'job_required_skills': None,\n",
       "   'job_required_education': {'postgraduate_degree': False,\n",
       "    'professional_certification': False,\n",
       "    'high_school': False,\n",
       "    'associates_degree': False,\n",
       "    'bachelors_degree': True,\n",
       "    'degree_mentioned': True,\n",
       "    'degree_preferred': True,\n",
       "    'professional_certification_mentioned': False},\n",
       "   'job_experience_in_place_of_education': False,\n",
       "   'job_min_salary': None,\n",
       "   'job_max_salary': None,\n",
       "   'job_salary_currency': None,\n",
       "   'job_salary_period': None,\n",
       "   'job_highlights': {},\n",
       "   'job_job_title': 'Data engineer',\n",
       "   'job_posting_language': 'en',\n",
       "   'job_onet_soc': '15113200',\n",
       "   'job_onet_job_zone': '4'},\n",
       "  {'employer_name': 'Modis',\n",
       "   'employer_logo': None,\n",
       "   'employer_website': None,\n",
       "   'employer_company_type': None,\n",
       "   'job_publisher': 'Modis',\n",
       "   'job_id': '92hPpLOJcUAAAAAAAAAAAA==',\n",
       "   'job_employment_type': 'CONTRACTOR',\n",
       "   'job_title': 'Intermediate Data Engineer',\n",
       "   'job_apply_link': 'https://www.modis.com/en-ca/job-seekers/job/toronto/intermediate-data-engineer-/CA_EN_6_919740_1504864/',\n",
       "   'job_apply_is_direct': False,\n",
       "   'job_apply_quality_score': 0.861,\n",
       "   'job_description': 'Intermediate Data Engineer\\n\\nContract Duration: 6 months\\n\\nLocation: Predominantly remote, Toronto\\n\\nResponsibilities:\\n• Design and implement the infrastructure necessary to store, process, and analyze large sets of data.\\n• Work with stakeholders to understand their business needs and develop scalable and reliable data architectures.\\n• Ensure that the infrastructure is optimized for performance and efficiency.\\n• Assess new data ingestion requirements by working with source data teams/SMEs and the internal data eng senior engineers\\n• Design, plan and align on ingestion pipeline solutions including tech/tools available\\n• Data modelling using existing team-built dbt framework\\n• Bug fixes and delivering new feature request for existing etls\\n• Documenting progress and status in Jira\\n• Documenting business domain knowledge in team’s online wiki/confluence\\n\\nRequired Skills:\\n• Data Engineering – ETL, Orchestration, Data Modeling\\n• Python and SQL experience\\n• Google Cloud Platform experience',\n",
       "   'job_is_remote': False,\n",
       "   'job_posted_at_timestamp': 1689168982,\n",
       "   'job_posted_at_datetime_utc': '2023-07-12T13:36:22.000Z',\n",
       "   'job_city': 'Toronto',\n",
       "   'job_state': 'ON',\n",
       "   'job_country': 'CA',\n",
       "   'job_latitude': 43.653225,\n",
       "   'job_longitude': -79.38319,\n",
       "   'job_benefits': None,\n",
       "   'job_google_link': 'https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+ontario,+canada&start=0&chips=date_posted:month&schips=date_posted;month&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+ontario,+canada&htidocid=92hPpLOJcUAAAAAAAAAAAA%3D%3D',\n",
       "   'job_offer_expiration_datetime_utc': '2023-08-11T23:59:59.000Z',\n",
       "   'job_offer_expiration_timestamp': 1691798399,\n",
       "   'job_required_experience': {'no_experience_required': False,\n",
       "    'required_experience_in_months': None,\n",
       "    'experience_mentioned': True,\n",
       "    'experience_preferred': False},\n",
       "   'job_required_skills': None,\n",
       "   'job_required_education': {'postgraduate_degree': False,\n",
       "    'professional_certification': False,\n",
       "    'high_school': False,\n",
       "    'associates_degree': False,\n",
       "    'bachelors_degree': False,\n",
       "    'degree_mentioned': False,\n",
       "    'degree_preferred': False,\n",
       "    'professional_certification_mentioned': False},\n",
       "   'job_experience_in_place_of_education': False,\n",
       "   'job_min_salary': None,\n",
       "   'job_max_salary': None,\n",
       "   'job_salary_currency': None,\n",
       "   'job_salary_period': None,\n",
       "   'job_highlights': {},\n",
       "   'job_job_title': 'Data engineer',\n",
       "   'job_posting_language': 'en',\n",
       "   'job_onet_soc': '15113200',\n",
       "   'job_onet_job_zone': '4'},\n",
       "  {'employer_name': 'Chainalysis',\n",
       "   'employer_logo': 'https://res.cloudinary.com/crunchbase-production/image/upload/c_lpad,f_auto,q_auto:eco,dpr_1/jxod8zbn0gg5ryjva7ri',\n",
       "   'employer_website': 'http://www.chainalysis.com',\n",
       "   'employer_company_type': None,\n",
       "   'job_publisher': 'Crypto Careers',\n",
       "   'job_id': 'S2mznWapcvwAAAAAAAAAAA==',\n",
       "   'job_employment_type': 'FULLTIME',\n",
       "   'job_title': 'Data Engineer II',\n",
       "   'job_apply_link': 'https://www.crypto-careers.com/jobs/245961654-data-engineer-ii-at-chainalysis',\n",
       "   'job_apply_is_direct': False,\n",
       "   'job_apply_quality_score': 0.5051,\n",
       "   'job_description': \"Blockchain technology is powering a growing wave of innovation. Businesses and governments around the world are using blockchains to make banking more efficient, connect with their customers, and investigate criminal cases. As adoption of blockchain technology grows, more and more organizations seek access to all this ecosystem has to offer. That’s where Chainalysis comes in. We provide complete knowledge of what’s happening on blockchains through our data, services, and solutions. With Chainalysis, organizations can navigate blockchains safely and with confidence.\\n\\nThe global financial ecosystem is changing. Revolutionary blockchain technology has unlocked the potential for people around the world to have more equal access to wealth and information. This transformation has begun with the mass global adoption of cryptocurrencies but like all new financial systems, it needs greater trust to realize its full potential and remain safe from bad actors. That’s where we come in. The Chainalysis blockchain data platform enables businesses, governments, and banks to solve the world’s most high-profile criminal cases, paving the way for an economy built on blockchains.\\n\\nThe Engineering team at Chainalysis is inspired by solving the hardest technical challenges and creating products that build trust in cryptocurrencies. We’re a global organization with teams in the UK, Denmark, and the USA who thrive on the challenging work we do and doing it with other exceptionally talented teammates. Our industry changes every day and our job is to build a flexible platform that will allow us to adapt to those rapid changes.\\n\\nAs a Data Engineer, Analytics, you’ll be responsible for building and maintaining the data layer for our analytics stack, top to bottom. You will connect our analytics stack with the rest of our infrastructure using both streaming and batch processes. You’ll write and maintain ELTs and their orchestration in order to produce meaningful and timely insights. You’ll work closely with our BI engineers to build a system that has a sensible balance between data access and security, empowering analysts to support critical data-driven decisions across the company.\\n\\nIn one year you’ll know you were successful if you and have:\\n• You’ve worked with other engineering teams to understand their data lifecycle, the right integration points and helped them integrate analytics at the design stage of projects.\\n• You’ve developed and managed scalable data pipelines and build out new integrations with internal and external data sources\\n• You’ve maintained optimal data pipeline architecture, including looking for and proposing improvements to the existing architecture.\\n• Together with the rest of the team, you’ve evaluated our current analytics stack and are working towards a system that’s modern, scalable, maintainable and cost-effective.\\n\\nA background like this helps:\\n• Experience in greenfield data engineering projects, specifically in analytics and data infrastructure projects.\\n• Advanced knowledge of modern data pipeline architecture and the AWS ecosystem including Redshift, Snowflake, Fivetran, Stitch, Databricks, Kafka, Airflow on Kubernetes and dbt\\n• Experience performing root cause analysis on data logging and ingestion processes, identifying opportunities to improve instrumentation and observability\\n• Comfort with Python, Java or another JVM language\\n• Experience writing Advanced SQL queries\\n• Have an interest in cryptocurrencies or a desire to learn - we can help!\\n\\n#LI-BD1\\n\\nAt Chainalysis, we help government agencies, cryptocurrency businesses, and financial institutions track and investigate illicit activity on the blockchain, allowing them to engage confidently with cryptocurrency. We take care of our people with great benefits, professional development opportunities, and fun.\\n\\nYou belong here.\\n\\nAt Chainalysis, we believe that diversity of experience and thought makes us stronger. With both customers and employees around the world, we are committed to ensuring our team reflects the unique communities around us. Some of the ways we’re ensuring we keep learning are an internal Diversity Committee, Days of Reflection throughout the year including International Women’s Day, Harvey Milk Day, World Humanitarian Day, and UN International Migrants Day, and a commitment to continue revisiting and reevaluating our diversity culture.\\n\\nWe encourage applicants across any race, ethnicity, gender/gender expression, age, spirituality, ability, experience and more. Additionally, if you need any accommodations to make our interview process more accessible to you due to a disability, don't hesitate to let us know. You can learn more here. We can’t wait to meet you.\\n\\nApplying from the EU? Please review our Chainalysis Applicant Privacy Policy.\\n\\nBy submitting this application, I consent to and authorize Chainalysis to contact my former employers, and any and all other persons and organizations for information bearing upon my qualifications for employment. I further authorize the listed employers, schools and personal references to give Chainalysis (without further notice to me) any and all information about my previous employment and education, along with other pertinent information they may have, and hereby waive any actions which I may have against either party(ies) for providing a reference. I understand any future employment will be contingent on the Company receiving satisfactory employment references.\",\n",
       "   'job_is_remote': False,\n",
       "   'job_posted_at_timestamp': 1689120000,\n",
       "   'job_posted_at_datetime_utc': '2023-07-12T00:00:00.000Z',\n",
       "   'job_city': None,\n",
       "   'job_state': 'ON',\n",
       "   'job_country': 'CA',\n",
       "   'job_latitude': 51.253777,\n",
       "   'job_longitude': -85.32321,\n",
       "   'job_benefits': None,\n",
       "   'job_google_link': 'https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+ontario,+canada&start=0&chips=date_posted:month&schips=date_posted;month&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+ontario,+canada&htidocid=S2mznWapcvwAAAAAAAAAAA%3D%3D',\n",
       "   'job_offer_expiration_datetime_utc': '2023-08-26T19:10:07.000Z',\n",
       "   'job_offer_expiration_timestamp': 1693077007,\n",
       "   'job_required_experience': {'no_experience_required': False,\n",
       "    'required_experience_in_months': None,\n",
       "    'experience_mentioned': True,\n",
       "    'experience_preferred': False},\n",
       "   'job_required_skills': None,\n",
       "   'job_required_education': {'postgraduate_degree': False,\n",
       "    'professional_certification': False,\n",
       "    'high_school': False,\n",
       "    'associates_degree': False,\n",
       "    'bachelors_degree': False,\n",
       "    'degree_mentioned': False,\n",
       "    'degree_preferred': False,\n",
       "    'professional_certification_mentioned': False},\n",
       "   'job_experience_in_place_of_education': False,\n",
       "   'job_min_salary': None,\n",
       "   'job_max_salary': None,\n",
       "   'job_salary_currency': None,\n",
       "   'job_salary_period': None,\n",
       "   'job_highlights': {},\n",
       "   'job_job_title': 'Data engineer',\n",
       "   'job_posting_language': 'en',\n",
       "   'job_onet_soc': '15113200',\n",
       "   'job_onet_job_zone': '4',\n",
       "   'job_occupational_categories': ['15-1133.00']},\n",
       "  {'employer_name': 'Cardinal Health',\n",
       "   'employer_logo': 'https://www.cardinalhealth.com/content/dam/corp/web/logos/logo-main.png',\n",
       "   'employer_website': 'http://www.cardinal.com',\n",
       "   'employer_company_type': 'Wholesale',\n",
       "   'job_publisher': 'Professional Diversity Network',\n",
       "   'job_id': 'RR3-JQr_Md8AAAAAAAAAAA==',\n",
       "   'job_employment_type': 'FULLTIME',\n",
       "   'job_title': 'Sr. Data Engineer',\n",
       "   'job_apply_link': 'https://www.prodivnet.com/job/sr-data-engineer-vaughan-ontario-13236286',\n",
       "   'job_apply_is_direct': False,\n",
       "   'job_apply_quality_score': 0.5754,\n",
       "   'job_description': 'What Data Engineering contributes to Cardinal Health\\n\\nThe Data & Analytics Function oversees the analytics life-cycle in order to identify, analyze and present relevant insights that drive business decisions and anticipate opportunities to achieve a competitive advantage. This function manages analytic data platforms, the access, design and implementation of reporting/business intelligence solutions, and the application of advanced quantitative modeling.\\n\\nData Engineering ensures raw data is usable and accessible for the organization. Data Engineering encompasses numerous specialties of data acquisition, integration, and provisioning.\\n\\nFor more information on what it is like to work for us, please watch this video: Cardinal Health - Wings.\\n\\nQualifications\\n• 8-12 years of experience preferred in data engineering, SQL, and data modeling\\n• BA, BS or equivalent experience in related field preferred. Advance Degree may be required\\n• Experience with machine learning algorithms and models\\n• Strong programming skills in Python, SQL, and shell scripting\\n• Expertise in data warehousing and ETL processes\\n• Experience with cloud-based data platforms such as AWS or GCP\\n• Intermediate level BI dashboarding experience (e.g. Tableau, PowerBI, etc.)\\n• Excellent problem-solving and analytical skills\\n• Strong communication and teamwork skills\\n\\nWhat is expected of you and others at this level\\n• Applies advanced knowledge and understanding of concepts, principles, and technical capabilities to manage a wide variety of projects\\n• Participates in the development of policies and procedures to achieve specific goals\\n• Recommends new practices, processes, metrics, or models\\n• Works on or may lead complex projects of large scope\\n• Projects may have significant and long-term impact\\n• Provides solutions which may set precedent\\n• Independently determines method for completion of new projects\\n• Receives guidance on overall project objectives\\n• Acts as a mentor to less experienced colleagues\\n\\nResponsibilities\\n• Design and build data pipelines that extract, transform, and load (ETL) data from various sources into our data warehouse\\n• Create and maintain data models that ensure data consistency, accuracy, and accessibility\\n• Develop and maintain automated data pipelines and data processing workflows\\n• Collaborate with data scientists and analysts to identify and implement machine learning algorithms and models that extract insights from our data\\n• Create data visualization utilizing complex data to provide meaningful insights\\n• Design and maintain databases that support our data processing and analytics needs\\n• Ensure data security, privacy, and compliance with data regulations\\n• Monitor and troubleshoot data pipelines to ensure data quality and reliability\\n• Develop and maintain documentation for data infrastructure, processes, and workflows\\n• Stay up-to-date with emerging trends and technologies in data engineering, machine learning, and automation\\n• Cardinal Health Canada is dedicated to the health and safety of its employees, customers and communities. Pursuant to this, we have implemented several COVID-19 safety measures, including a COVID-19 Vaccination Policy that requires all employees to be fully vaccinated. This is subject to reasonable accommodation for grounds protected under human rights legislation\\n\\nCardinal Health is committed to employment equity and encourages applications from women, visible minorities, Aboriginal peoples, and persons with disabilities.\\n\\nCardinal Health is committed to accommodating applicants with disabilities throughout the hiring process, in accordance with the Accessibility for Ontarians with Disabilities Act (AODA). Our Human Resources team is responsible for working with applicants requesting accommodation at any stage of the hiring process.\\n\\n#LI-Hybrid\\n\"\"\\n\\nCandidates who are back-to-work, people with disabilities, without a college degree, and Veterans are encouraged to apply.\\n\\nCardinal Health supports an inclusive workplace that values diversity of thought, experience and background. We celebrate the power of our differences to create better solutions for our customers by ensuring employees can be their authentic selves each day. Cardinal Health is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, religion, color, national origin, ancestry, age, physical or mental disability, sex, sexual orientation, gender identity/expression, pregnancy, veteran status, marital status, creed, status with regard to public assistance, genetic status or any other status protected by federal, state or local law.',\n",
       "   'job_is_remote': False,\n",
       "   'job_posted_at_timestamp': 1689126279,\n",
       "   'job_posted_at_datetime_utc': '2023-07-12T01:44:39.000Z',\n",
       "   'job_city': 'Vaughan',\n",
       "   'job_state': 'ON',\n",
       "   'job_country': 'CA',\n",
       "   'job_latitude': 43.856316,\n",
       "   'job_longitude': -79.50854,\n",
       "   'job_benefits': None,\n",
       "   'job_google_link': 'https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+ontario,+canada&start=0&chips=date_posted:month&schips=date_posted;month&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+ontario,+canada&htidocid=RR3-JQr_Md8AAAAAAAAAAA%3D%3D',\n",
       "   'job_offer_expiration_datetime_utc': '2023-10-10T01:44:39.000Z',\n",
       "   'job_offer_expiration_timestamp': 1696902279,\n",
       "   'job_required_experience': {'no_experience_required': False,\n",
       "    'required_experience_in_months': 96,\n",
       "    'experience_mentioned': True,\n",
       "    'experience_preferred': True},\n",
       "   'job_required_skills': None,\n",
       "   'job_required_education': {'postgraduate_degree': False,\n",
       "    'professional_certification': False,\n",
       "    'high_school': False,\n",
       "    'associates_degree': False,\n",
       "    'bachelors_degree': False,\n",
       "    'degree_mentioned': True,\n",
       "    'degree_preferred': True,\n",
       "    'professional_certification_mentioned': False},\n",
       "   'job_experience_in_place_of_education': False,\n",
       "   'job_min_salary': None,\n",
       "   'job_max_salary': None,\n",
       "   'job_salary_currency': None,\n",
       "   'job_salary_period': None,\n",
       "   'job_highlights': {},\n",
       "   'job_job_title': 'Data engineer',\n",
       "   'job_posting_language': 'en',\n",
       "   'job_onet_soc': '15113200',\n",
       "   'job_onet_job_zone': '4',\n",
       "   'job_naics_code': '424210',\n",
       "   'job_naics_name': \"Drugs and Druggists' Sundries Merchant Wholesalers\"},\n",
       "  {'employer_name': '360insights',\n",
       "   'employer_logo': 'https://images.squarespace-cdn.com/content/v1/588259e5893fc087ee049fb8/1560790905003-ECSFONLZW3OLMTTX503M/360insights',\n",
       "   'employer_website': 'http://www.360insights.com',\n",
       "   'employer_company_type': None,\n",
       "   'job_publisher': 'LinkedIn',\n",
       "   'job_id': 'WVwDooRTBy8AAAAAAAAAAA==',\n",
       "   'job_employment_type': 'FULLTIME',\n",
       "   'job_title': 'Data Engineer',\n",
       "   'job_apply_link': 'https://ca.linkedin.com/jobs/view/data-engineer-at-360insights-3661111318',\n",
       "   'job_apply_is_direct': False,\n",
       "   'job_apply_quality_score': 0.6017,\n",
       "   'job_description': 'Experiences that create growth. Growth that creates experiences.\\n\\nHere at 360insights you will be a part of a fast-paced global team that is innovating and leading in the channel incentives and insights industry through our SaaS platform. You will be delivering great solutions for some of the world’s most recognizable brands all within an inspiring culture that has certified us as a Great Place to Work® in Canada, the USA and the UK.\\n\\nAt 360insights you will have lots of experiences and opportunities to grow and the more experience you gain along the way, the more opportunities will open to you.\\n\\nWhat you will be doing:\\n• You will report to the Manager of Data Engineering.\\n• You will have a track record for developing high-quality application by implementing testable and scalable code.\\n• You will be an integral part of the development team for new modules and enhancements to 360’s industry-leading SaaS-based incentive management platform.\\n• You will manage and build ELT and ETL pipelines between internal data and external data stores for the broader data team.\\n• You will work within an Agile development environment to help in determining technical requirements, estimate development effort and develop high performing code.\\n• Documenting data engineering processes, standards, and best practices.\\n• Contribute to the monitoring, troubleshooting, and performance tuning of data pipelines and database systems.\\n• Abide by organization security policies and practices.\\n\\nWhat you’ll bring:\\n• Strong experience with AWS Glue, DMS, AWS Redshift, AWS Step Function, AWS S3, Athena or similar technologies in Azure or Google cloud to design and deliver data pipeline solutions.\\n• Proficiency in GitHub for version control and collaborative software development.\\n• Experience with build automation and continuous integration using TeamCity.\\n• Familiarity with infrastructure - as -code principles and tools like terraform, Jenkins and TeamCity.\\n• Proficiency in AWS CLI for managing AWS services and automating tasks.\\n• Solid programming skills in Python and experience with PySpark for data processing and analysis.\\n• Strong understanding of database systems, data modeling, and SQL.\\n• Ability to troubleshoot, optimize, and fine-tune data processing workflows.\\n• Excellent problem-solving skills and attention to detail.\\n• Strong communication and collaboration skills to work effectively within a team.\\n• Understanding of network concepts, algorithms, data structures, RDMS, OOP (Object Oriented Programming) and design patterns\\n\\nWhy 360?\\n\\nWe are a people-first organization, passionate about our culture which means that we live and breathe our values every day and in our interactions with team members and clients. Being people focused means shows up like this:\\n• In each area of our business there are opportunities for personal and professional growth or to diversify into other disciplines, all supported by L&D and mentorship programs\\n• No matter where you are in the world, we have comprehensive healthcare and retirement plans backed up by people-first policies that support a balanced lifestyle, generous vacation and family leave plans, recognition for the value you bring, employee led groups to keep you connected, company profit share bonus, stock options and much more.\\n• We welcome all future Insighters, from all walks of life. 360insights is committed to providing equal employment opportunities to people of every race, religion or belief and ethnic origin, regardless of age, disability, sexual orientation, or gender identity. We just want you to bring your authentic self, live our Be Real & Have Fun value, do your best work with us, so that together we can all grow and be successful together.',\n",
       "   'job_is_remote': False,\n",
       "   'job_posted_at_timestamp': 1689105514,\n",
       "   'job_posted_at_datetime_utc': '2023-07-11T19:58:34.000Z',\n",
       "   'job_city': 'Whitby',\n",
       "   'job_state': 'ON',\n",
       "   'job_country': 'CA',\n",
       "   'job_latitude': 43.897545,\n",
       "   'job_longitude': -78.94293,\n",
       "   'job_benefits': None,\n",
       "   'job_google_link': 'https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+ontario,+canada&start=0&chips=date_posted:month&schips=date_posted;month&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+ontario,+canada&htidocid=WVwDooRTBy8AAAAAAAAAAA%3D%3D',\n",
       "   'job_offer_expiration_datetime_utc': '2023-08-10T19:58:34.000Z',\n",
       "   'job_offer_expiration_timestamp': 1691697514,\n",
       "   'job_required_experience': {'no_experience_required': False,\n",
       "    'required_experience_in_months': None,\n",
       "    'experience_mentioned': True,\n",
       "    'experience_preferred': False},\n",
       "   'job_required_skills': None,\n",
       "   'job_required_education': {'postgraduate_degree': False,\n",
       "    'professional_certification': False,\n",
       "    'high_school': False,\n",
       "    'associates_degree': False,\n",
       "    'bachelors_degree': True,\n",
       "    'degree_mentioned': False,\n",
       "    'degree_preferred': False,\n",
       "    'professional_certification_mentioned': False},\n",
       "   'job_experience_in_place_of_education': False,\n",
       "   'job_min_salary': None,\n",
       "   'job_max_salary': None,\n",
       "   'job_salary_currency': None,\n",
       "   'job_salary_period': None,\n",
       "   'job_highlights': {},\n",
       "   'job_job_title': 'Data engineer',\n",
       "   'job_posting_language': 'en',\n",
       "   'job_onet_soc': '15113200',\n",
       "   'job_onet_job_zone': '4'},\n",
       "  {'employer_name': 'Canada Life Assurance Company',\n",
       "   'employer_logo': None,\n",
       "   'employer_website': None,\n",
       "   'employer_company_type': None,\n",
       "   'job_publisher': 'Canada Life Assurance Company | Careers Center',\n",
       "   'job_id': 'yKB7up8ofogAAAAAAAAAAA==',\n",
       "   'job_employment_type': 'FULLTIME',\n",
       "   'job_title': 'Data Engineer (Student Position)',\n",
       "   'job_apply_link': 'https://careers-canadalife.icims.com/jobs/25966/data-engineer-%28student-position%29/job',\n",
       "   'job_apply_is_direct': True,\n",
       "   'job_apply_quality_score': 0.7779,\n",
       "   'job_description': 'Job Description\\n\\nOur data aspiration is to unlock the power of connected data to deliver continuous value for our customers, our partners, and our company with trust and integrity. The position is central to our ability to do so. The Data Engineering role is responsible for creating connected data products for self-services across the organization within the boundaries of our data policy, standards, and controls.\\n\\nNote: This is a student position starting in September 2023. We have positions available in Toronto and London.\\n\\nWhat you will do\\n• Build data products for Canadian use: design, enrich, and publish self-service connected datasets within proper controls for use across Canada (one stop data shop)\\n• Build data asset inventory: while building connected data, inventory critical data elements including lineage, classification, and usage (Google for Canada Life data)\\n• Build insights: complete data analysis and discovery to better understand the data and the story it is telling to drive actionable results\\n• Loves teamwork and collaboration: collaborates on a daily basis with the core data squad team and business\\n• Open to learning and presenting: excited to share work across the company, with senior leadership and through community of practice participation\\n\\nWhat you will bring\\n• Applied data management experience (Data & Analytics education an asset)\\n• Experience in SQL, Python and Power BI an asset.\\n• Working knowledge of Azure cloud technologies (ADF, Databricks, Synapse, etc.)\\n• Excellent communication and presentation skills and strong organization skills\\n• Agile, practical, customer service-oriented mindset (doesn’t over complicate processes)\\n• Generally curious and excited to learn new things\\n\\nGiven the size and scope of our organization, we have the flexibility for this position to be located in the following head office locations: Toronto, London\\n\\nBe your best at Canada Life- Apply today!\\n\\nBeing a part of Canada Life means you have a voice. This is a place where your unique background, perspectives and talents are valued, and shape our future success.\\n\\nYou can be your best here. You’re part of a diverse and inclusive workplace where your career and well-being are championed. You’ll have the opportunity to excel in your way, finding new and better ways to deliver exceptional customer and advisor experiences.\\n\\nTogether, as part of a great team, you’ll deliver on our shared purpose to improve the well-being of Canadians. It’s our driving force. Become part of a strong and successful company that’s trusted by millions of Canadians to do the right thing.\\n\\nCanada Life serves\\u202fthe financial security needs of more than 13 million people across Canada, with additional operations in Europe and the United States. As members of the Power Financial Corporation group of companies, we’re one of Canada’s leading insurers with interests in life insurance, health insurance, investment and retirement savings. We offer a broad portfolio of financial and benefit plan solutions for individuals, families, businesses and organizations.\\u202f\\n\\n\\u202f\\u202f\\n\\nWe are committed to providing an inclusive, accessible environment, where all employees and customers feel valued, respected and supported. We are dedicated to building a workforce that reflects the diversity of the communities in which we live, and to creating an environment where every employee\\u202fhas the opportunity to\\u202freach their potential.\\u202f\\n\\nIt is our priority to remove barriers to provide equal access to employment. A Human Resources representative will work with applicants who request a reasonable accommodation during the application process. All information shared during the accommodation request process will be stored and used in a manner that is consistent with applicable laws and Canada Life policies. To request a reasonable accommodation in the application process, contact talentacquisitioncanada@canadalife.com.\\n\\n\\u202f\\u202f\\n\\nCanada Life\\u202fwould like to thank all applicants, however only those who qualify for an interview will be contacted.\\n\\n#LI-Hybrid',\n",
       "   'job_is_remote': False,\n",
       "   'job_posted_at_timestamp': 1689145200,\n",
       "   'job_posted_at_datetime_utc': '2023-07-12T07:00:00.000Z',\n",
       "   'job_city': 'Toronto',\n",
       "   'job_state': 'ON',\n",
       "   'job_country': 'CA',\n",
       "   'job_latitude': 43.653225,\n",
       "   'job_longitude': -79.38319,\n",
       "   'job_benefits': None,\n",
       "   'job_google_link': 'https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+ontario,+canada&start=0&chips=date_posted:month&schips=date_posted;month&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+ontario,+canada&htidocid=yKB7up8ofogAAAAAAAAAAA%3D%3D',\n",
       "   'job_offer_expiration_datetime_utc': '2023-07-26T07:00:00.000Z',\n",
       "   'job_offer_expiration_timestamp': 1690354800,\n",
       "   'job_required_experience': {'no_experience_required': False,\n",
       "    'required_experience_in_months': None,\n",
       "    'experience_mentioned': True,\n",
       "    'experience_preferred': False},\n",
       "   'job_required_skills': None,\n",
       "   'job_required_education': {'postgraduate_degree': False,\n",
       "    'professional_certification': False,\n",
       "    'high_school': False,\n",
       "    'associates_degree': False,\n",
       "    'bachelors_degree': False,\n",
       "    'degree_mentioned': False,\n",
       "    'degree_preferred': False,\n",
       "    'professional_certification_mentioned': False},\n",
       "   'job_experience_in_place_of_education': False,\n",
       "   'job_min_salary': None,\n",
       "   'job_max_salary': None,\n",
       "   'job_salary_currency': None,\n",
       "   'job_salary_period': None,\n",
       "   'job_highlights': {},\n",
       "   'job_job_title': 'Data engineer',\n",
       "   'job_posting_language': 'en',\n",
       "   'job_onet_soc': '15113200',\n",
       "   'job_onet_job_zone': '4',\n",
       "   'job_occupational_categories': ['Information Technology']},\n",
       "  {'employer_name': 'Sheridan College',\n",
       "   'employer_logo': None,\n",
       "   'employer_website': None,\n",
       "   'employer_company_type': None,\n",
       "   'job_publisher': 'Njoyn',\n",
       "   'job_id': '0r9QhenOP1MAAAAAAAAAAA==',\n",
       "   'job_employment_type': 'FULLTIME',\n",
       "   'job_title': 'Part-time Teaching Opportunities, Data Engineer',\n",
       "   'job_apply_link': 'https://sheridan.njoyn.com/cl3/xweb/XWeb.asp?NTKN=c&clid=55117&Page=JobDetails&Jobid=J0723-0837&BRID=229762&lang=1',\n",
       "   'job_apply_is_direct': False,\n",
       "   'job_apply_quality_score': 0.7673,\n",
       "   'job_description': \"Continuing and Professional Studies (CAPS) is looking for outstanding individual to teach Big Data Analytic Tools for our Data Engineer Micro-Credential. The Data Engineer Micro-Credential blends the advancement of cloud technology and computing advancements to achieve strong data solutions to support any type of business. Learners will understand and implement the use of Big Data Tools; execute deep learning scripting; comprehend and conceptualize popular programming languages; and process uninterrupted flow of data between servers and applications.\\n\\nTo view more information about Data Engineer Micro-Credential, please visit the link provided below:\\n• Data Engineer Micro-Credential\\n\\nSheridan’s professors are responsible for developing an effective learning environment for students while respecting their diverse cultural and educational backgrounds, experiences, and individual learning styles.\\n\\nWhat You’ll be Doing\\n• Delivering course curriculum in the classroom and/or online in alignment with course objectives, pedagogical approach, and evaluation policies and procedures;\\n• Setting clear and transparent expectations and instructions to students, providing student-centric academic support, and collaborating with CAPS Leadership, Support Staff, and Sheridan Student Services as required;\\n• Designing active learning strategies and educational experiences using best practices; updating Learning Management System (SLATE) course site and Master sites in collaboration with the CAPS team; utilizing CAPS and Sheridan resources and supports to continually advance and improve scholarship of teaching and learning;\\n• Evaluating student progress/achievement and providing clear formative and summative feedback to students based on objective evaluation of the students’ work; Timely completion of Learning Management System gradebook details; Completing grade entry for official student records according to due dates;\\n• Attending periodic CAPS and CE Instructor meetings and events; completing mandatory employee training; Compliance with all Sheridan Policies and Procedures.\\n\\nAbout You\\n\\nYou have the skills and knowledge to work with an increasingly diverse student and staff population as well as a proven commitment to anti-oppression, equity, and inclusion.\\n\\nYou are a self-directed natural leader and have a highly collaborative spirit which, combined with your keen interpersonal skills, empowers you to build positive and collaborative relationships and create and maintain a positive organizational culture.\\n\\nYou have excellent oral and written communication skills and proven effectiveness as an instructor.\\n\\nThe successful candidate will also meet the following requirements:\\n• Minimum of 2 years of relevant experience.\\n• Bachelor’s Degree in a related field of study is preferred. Relevant experience and expertise will be considered equivalent to formal education.\\n• Teaching experience at the post-secondary level or equivalent experience as a trainer in industry is preferred.\\n• Excellent communication and interpersonal skills.\\n• Committed to excellence in teaching and learning and to working within a team environment.\\n\\nWho We Are\\n\\nEvery member of the Sheridan community is passionate about the transformational role we play in people’s lives. Our strategic plan, Sheridan 2024: Galvanizing Education for a Complex World (https://sheridan2024.sheridancollege.ca/), charts a path towards a new ground-breaking model of higher education that reshapes post-secondary education and better prepares students for the future. We are committed to demonstrably advancing equity, diversity and inclusivity. Diversity is our strength and fuels our commitment to excellence. Across our campuses, we’re making meaningful strides towards developing an equitable and inclusive community.\\n• For more information, visit: Why Work at Sheridan\\n\\nOther Details\\n\\nFaculty/Department: Continuing and Professional Studies\\n\\nCampus Location: Davis (may be assigned activities at any Sheridan campus). This position is currently working on-site.\\n\\nEmployee Group: Non-Full-Time Faculty\\n\\nSalary Range: Based on relevant educational qualifications and experience\\n\\nStart Date: Fall 2023\\n\\nApplication Deadline: July 23, 2023\\n\\nSheridan is deeply committed to implementing the Calls to Action framed by the Truth and Reconciliation Commission. We acknowledge that we live and work on the traditional territory of the Mississauga's of the Credit First Nations, Anishinaabe Nation, Huron-Wendat and the Haudenosaunee Confederacy. Sheridan is situated on these lands, and it is our collective responsibility to honour and respect those who have gone before us, those who are here, and those who have yet to come. We are grateful for the opportunity to be working on this land.\\n\\nSheridan values the diverse and intersectional identities of its students, faculty, and staff. Sheridan regards equity and diversity as an integral part of academic excellence and is committed to accessibility for all employees. Sheridan seeks applicants who embrace our values of equity, anti-racism, and inclusion. As such, we encourage applications from qualified candidates who have been historically disadvantaged and marginalized, including those who identify as First Nations, Métis and/or Inuit/Inuk, Black, members of racialized communities, persons with disabilities, women and/or 2SLGBTQ+.\\n\\nSheridan is committed to accessibility for persons with disabilities. If you have any application, interview, or workplace accommodation requests, please contact Human Resources.\\n\\nYou may be asked to provide copies of your educational credentials at the time of interview. Upon hire, we require official confirmation of educational credentials and Canadian equivalency assessments, if applicable.\",\n",
       "   'job_is_remote': False,\n",
       "   'job_posted_at_timestamp': 1689206400,\n",
       "   'job_posted_at_datetime_utc': '2023-07-13T00:00:00.000Z',\n",
       "   'job_city': 'Brampton',\n",
       "   'job_state': 'ON',\n",
       "   'job_country': 'CA',\n",
       "   'job_latitude': 43.73155,\n",
       "   'job_longitude': -79.76242,\n",
       "   'job_benefits': None,\n",
       "   'job_google_link': 'https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+ontario,+canada&start=0&chips=date_posted:month&schips=date_posted;month&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+ontario,+canada&htidocid=0r9QhenOP1MAAAAAAAAAAA%3D%3D',\n",
       "   'job_offer_expiration_datetime_utc': '2023-07-23T00:00:00.000Z',\n",
       "   'job_offer_expiration_timestamp': 1690070400,\n",
       "   'job_required_experience': {'no_experience_required': False,\n",
       "    'required_experience_in_months': 24,\n",
       "    'experience_mentioned': True,\n",
       "    'experience_preferred': True},\n",
       "   'job_required_skills': None,\n",
       "   'job_required_education': {'postgraduate_degree': False,\n",
       "    'professional_certification': False,\n",
       "    'high_school': False,\n",
       "    'associates_degree': False,\n",
       "    'bachelors_degree': False,\n",
       "    'degree_mentioned': True,\n",
       "    'degree_preferred': True,\n",
       "    'professional_certification_mentioned': False},\n",
       "   'job_experience_in_place_of_education': False,\n",
       "   'job_min_salary': None,\n",
       "   'job_max_salary': None,\n",
       "   'job_salary_currency': None,\n",
       "   'job_salary_period': None,\n",
       "   'job_highlights': {},\n",
       "   'job_job_title': 'Teaching',\n",
       "   'job_posting_language': 'en',\n",
       "   'job_onet_soc': '19309200',\n",
       "   'job_onet_job_zone': '5'},\n",
       "  {'employer_name': 'Sun Life',\n",
       "   'employer_logo': 'https://www.sunlife.ca/content/dam/sunlife/folder1/Sun_Life_weblogo_127x31.svg',\n",
       "   'employer_website': 'http://www.sunlife.com',\n",
       "   'employer_company_type': 'Finance',\n",
       "   'job_publisher': 'LinkedIn',\n",
       "   'job_id': 'yY-aXg7oF_oAAAAAAAAAAA==',\n",
       "   'job_employment_type': 'FULLTIME',\n",
       "   'job_title': 'Senior Data Engineer',\n",
       "   'job_apply_link': 'https://ca.linkedin.com/jobs/view/senior-data-engineer-at-sun-life-3658967270',\n",
       "   'job_apply_is_direct': False,\n",
       "   'job_apply_quality_score': 0.5558,\n",
       "   'job_description': \"You are as unique as your background, experience and point of view. Here, you’ll be encouraged, empowered and challenged to be your best self. You'll work with dynamic colleagues - experts in their fields - who are eager to share their knowledge with you. Your leaders will inspire and help you reach your potential and soar to new heights. Every day, you'll have new and exciting opportunities to make life brighter for our Clients - who are at the heart of everything we do. Discover how you can make a difference in the lives of individuals, families and communities around the world.\\n\\nJob Description:\\n\\nAt Sun Life, we work together, share common values and encourage growth and achievement. We offer many career paths that attract a wide variety of talent and skills. Follow a path that lets your talents shine.\\n\\nRole Summary:\\n\\nThe Client Experience Office (CXO) is an important pillar of Sun Life’s corporate strategy and as such, is seeking experienced data engineering individuals with a proven track record working with data consumers to extract meaningful insights from enterprise data. Senior Business data engineers play an indispensable role in assembling the necessary data products that fuel myriad of analytical models used by our various business units across Canada.\\n\\nReporting to the Manager of Data Engineering, the Senior Business Data Engineer will be a key member of Data Enablement Team responsible for building enriched data products that provide deep insights about our Client, Advisor, Product, Transactions along with their intricate relationships. The Senior Business Data Engineer will work with data analysts and data scientists to help answer complex business enquires such as identifying new business opportunities, understanding our client behavior and optimizing areas of our sales cycles.\\n\\nWhat will you do?\\n• Obsessed with understanding our enterprise data and interpreting it in the best way possible to derive business value\\n• Design and engineer data models that represent our key business entities such as our Advisor, Client & Product\\n• Ensure high level of data quality and data integrity are maintained across data sources for business\\n• Convert complex business and technical rules into logic for data flows and data pipelines\\n• Work with our Data Science teams to implement data products for various use cases\\n• Collaborate with stakeholders and incorporate into solution design for various use case requirements\\n• Standardize metadata into a common glossary along with the necessary documentation for data consumers\\n\\nWhat do you need to succeed?\\n• 3-5 years hands-on experience working on AWS technologies for data processing and analytics\\n• 2+ years experience with AWS Glue for ETL\\n• 3 years experience in Pyspark and SQL\\n• 5 years hands-on experience with relational database systems, relational models, dimensional models etc.\\n• 3-5 years experience with engineering data products as input to various analytical models\\n• 3-5 years building feedback loops between model deployment and its data – i.e. tune & tweak data products to achieve scale, optimization, etc.\\n• Sound understanding of data management principles (data warehousing, data quality, master data management, etc.)\\n• Sound understanding of data modelling and a passion for analytics\\n• Proven track record in leveraging SQL and SQL-based programming to solve business problems\\n• Ability to translate complex business requirements into a set of data ingestion pseudocode\\n• Proven ability to leverage knowledge of data engineering to extract, conform and integrate a variety of operational data sources into pro‐duction-grade data products\\n• Demonstrate blend of tenacity, creativity, and discipline required to develop ground breaking self-serve data models that data scientists can consume\\n• Excellent verbal and written communication skills; have executive presence and proven ability to partner with Data Scientists to articulate insights gleaned from the model and the underlying data\\n\\nEducation and Qualifications:\\n• Undergraduate degree in Computer Science, Mathematics, Engineering, or equivalent\\n• Graduate degree in business or quantitative science strongly preferred\\n\\nUnique Requirements\\n• The candidate selected for this role is required to attain Canadian Reliability Security Clearance (administered by submitting fingerprints to the RCMP, who then conduct min. 5 year history checks)\\n• To see if you are eligible for this clearance, please review the section 201 on the Federal Government site (https://www.tpsgc-pwgsc.gc.ca/esc-src/personnel/pdcf-rsrp-eng.html)\\n\\nWhat would be nice to have?\\n• Understand and Embrace Agile Operating Model\\n• Proven Experienced working as Product Owner in Scrum or Service Request Manager in Kanban projects\\n• Experience working in a large financial services organization\\n• Knowledge of fundamental statistical models & techniques\\n\\nWhat's In It For You?\\n• Great Place to Work® Certified in Canada and the US - 2022\\n• Great Place to Work® award for Best Workplaces for #HybridWork - 2022\\n• Named “Best Places to Work” by Glassdoor - 2021\\n• Canada Award for Excellence for Mental Health at Work - 2021\\n• Flex hours and Flexible hybrid work model including in-country work-from-home if you prefer. ,\\n• Competitive salary and bonus structure influenced by market range data\\n• Pension, stock and savings programs to help build and enhance your future financial security\\n• A friendly, collaborative, and inclusive culture\\n• Be part of our continuous improvement journey in developing the next greatest digital enterprise experience.\\n• Work and professional development that is united by our Purpose: to help Clients and Employees achieve lifetime financial security and live healthier lives\\n\\nThe Base Pay range is for the primary location for which the job is posted. It may vary depending on the work location of the successful candidate or other factors. In addition to Base Pay, eligible Sun Life employees participate in various incentive plans, payment under which is discretionary and subject to individual and company performance. Certain sales focused roles have sales incentive plans based on individual or group sales results.\\n\\nDiversity and inclusion have always been at the core of our values at Sun Life. A diverse workforce with wide perspectives and creative ideas benefits our clients, the communities where we operate and all of us as colleagues. We welcome applications from qualified individuals from all backgrounds.\\n\\nPersons with disabilities who need accommodation in the application process or those needing job postings in an alternative format may e-mail a request to thebrightside@sunlife.com.\\n\\nAt Sun Life we strive to create a flexible work environment where our employees are empowered to do their best work. Several flexible work options are available and can be discussed throughout the selection process depending on the role requirements and individual needs.\\n\\nWe thank all applicants for showing an interest in this position. Only those selected for an interview will be contacted.\\n\\nSalary Range:\\n\\n74,100/74 100 - 120,800/120 800\\n\\nJob Category:\\n\\nAdvanced Analytics\\n\\nPosting End Date:\\n\\n30/07/2023\",\n",
       "   'job_is_remote': False,\n",
       "   'job_posted_at_timestamp': 1689210032,\n",
       "   'job_posted_at_datetime_utc': '2023-07-13T01:00:32.000Z',\n",
       "   'job_city': 'Waterloo',\n",
       "   'job_state': 'ON',\n",
       "   'job_country': 'CA',\n",
       "   'job_latitude': 43.464256,\n",
       "   'job_longitude': -80.52041,\n",
       "   'job_benefits': None,\n",
       "   'job_google_link': 'https://www.google.com/search?gl=us&hl=en&rciv=jb&q=data+engineer+in+ontario,+canada&start=0&chips=date_posted:month&schips=date_posted;month&ibp=htl;jobs#fpstate=tldetail&htivrt=jobs&htiq=data+engineer+in+ontario,+canada&htidocid=yY-aXg7oF_oAAAAAAAAAAA%3D%3D',\n",
       "   'job_offer_expiration_datetime_utc': '2023-08-12T01:00:32.000Z',\n",
       "   'job_offer_expiration_timestamp': 1691802032,\n",
       "   'job_required_experience': {'no_experience_required': False,\n",
       "    'required_experience_in_months': 36,\n",
       "    'experience_mentioned': True,\n",
       "    'experience_preferred': False},\n",
       "   'job_required_skills': None,\n",
       "   'job_required_education': {'postgraduate_degree': False,\n",
       "    'professional_certification': False,\n",
       "    'high_school': False,\n",
       "    'associates_degree': False,\n",
       "    'bachelors_degree': True,\n",
       "    'degree_mentioned': True,\n",
       "    'degree_preferred': True,\n",
       "    'professional_certification_mentioned': True},\n",
       "   'job_experience_in_place_of_education': False,\n",
       "   'job_min_salary': None,\n",
       "   'job_max_salary': None,\n",
       "   'job_salary_currency': None,\n",
       "   'job_salary_period': None,\n",
       "   'job_highlights': {},\n",
       "   'job_job_title': 'Data engineer',\n",
       "   'job_posting_language': 'en',\n",
       "   'job_onet_soc': '15113200',\n",
       "   'job_onet_job_zone': '4',\n",
       "   'job_naics_code': '524113',\n",
       "   'job_naics_name': 'Direct Life Insurance Carriers'}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://jsearch.p.rapidapi.com/search\"\n",
    "\n",
    "querystring = {\"query\":\"Data Engineer in Ontario, Canada\",\"page\":\"1\",\"num_pages\":\"1\",\"date_posted\":\"month\"}\n",
    "\n",
    "headers = {\n",
    "\t\"X-RapidAPI-Key\": \"e0bbfbf5e4msh1ca295e627d2f36p1add45jsn88983127afe1\",\n",
    "\t\"X-RapidAPI-Host\": \"jsearch.p.rapidapi.com\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers, params=querystring)\n",
    "\n",
    "data = response.json()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 1\n",
    "# Job Description Extraction using Job Discription synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_role(i):\n",
    "    # Extract job title and description from data\n",
    "    job_title = data['data'][i]['job_title']\n",
    "    job_apply_link = data['data'][i]['job_apply_link']\n",
    "    job_required_education = data['data'][i]['job_required_education']\n",
    "    job_offer_expiration_datetime_utc = data['data'][i]['job_offer_expiration_datetime_utc'][:10]\n",
    "    job_description_raw = data['data'][i]['job_description']\n",
    "\n",
    "    # Convert the description to lower case\n",
    "    job_description_lower = job_description_raw.lower()\n",
    "\n",
    "    # Define keywords to look for in the description\n",
    "    keywords = [\"responsibilities:\", \"responsible\", \"description:\", \"what will you do?\"]\n",
    "\n",
    "    # Initialize the index and keyword_found variables\n",
    "    index = -1\n",
    "    keyword_found = \"\"\n",
    "\n",
    "    # Iterate over each keyword\n",
    "    for keyword in keywords:\n",
    "        # Find the index of the keyword in the description\n",
    "        keyword_index = job_description_lower.rfind(keyword.lower())\n",
    "\n",
    "        # If the keyword is found and it is less than 3 characters away from a new line\n",
    "        if keyword_index != -1 and (job_description_lower[keyword_index:].find(\"\\n\") <= 3 or job_description_lower[keyword_index:].find(\"\\\\n\") <= 3):\n",
    "            # If this keyword is lower positioned than the previously found keyword\n",
    "            if keyword_index > index:\n",
    "                # Update the index and keyword_found variables\n",
    "                index = keyword_index\n",
    "                keyword_found = keyword.lower()\n",
    "\n",
    "    # If a keyword is found, extract the part of the description from the keyword onwards\n",
    "    # If no keyword is found, return a default message\n",
    "    if keyword_found:\n",
    "        description = job_description_lower[index:]\n",
    "    else:\n",
    "        description = \"No keyword found in description\"\n",
    "\n",
    "    # Return the job title and extracted description\n",
    "    return job_title, job_apply_link, job_required_education, job_offer_expiration_datetime_utc, description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_role_1(i):\n",
    "    # Extract job description from data\n",
    "    job_description_raw = data['data'][i]['job_description']\n",
    "\n",
    "    # Convert the description to lower case\n",
    "    job_description_lower = job_description_raw.lower()\n",
    "\n",
    "    # Define keywords to look for in the description\n",
    "    keywords = [\"responsibilities\", \"responsible\", \"description\", \"what will you do?\"]\n",
    "\n",
    "    # Initialize the index and keyword_found variables\n",
    "    index = -1\n",
    "    keyword_found = \"\"\n",
    "\n",
    "    # Iterate over each keyword\n",
    "    for keyword in keywords:\n",
    "        # Find the index of the keyword in the description\n",
    "        keyword_index = job_description_lower.rfind(keyword.lower())\n",
    "\n",
    "        # If the keyword is found and it is less than 3 characters away from a new line\n",
    "        if keyword_index != -1 and (job_description_lower[keyword_index:].find(\"\\n\") <= 3 or job_description_lower[keyword_index:].find(\"\\\\n\") <= 3):\n",
    "            # If this keyword is lower positioned than the previously found keyword\n",
    "            if keyword_index > index:\n",
    "                # Update the index and keyword_found variables\n",
    "                index = keyword_index\n",
    "                keyword_found = keyword.lower()\n",
    "\n",
    "    # If a keyword is found, extract the part of the description from the keyword onwards\n",
    "    # If no keyword is found, return a default message\n",
    "    if keyword_found:\n",
    "        description = job_description_lower[index:]\n",
    "    else:\n",
    "        description = \"No keyword found in description\"\n",
    "\n",
    "    # Return the extracted description\n",
    "    return description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "employer_website = []\n",
    "job_id = []\n",
    "job_employment_type = []\n",
    "job_title = []\n",
    "job_apply_link = []\n",
    "job_description = []\n",
    "job_responsibilities = []\n",
    "job_description_text = []\n",
    "job_city = []\n",
    "job_country = []\n",
    "job_posted_at_date = [] \n",
    "employer_company_type = []\n",
    "job_required_education = []\n",
    "job_offer_expiration_datetime_utc = []\n",
    "\n",
    "for i in range(len(data['data'])):\n",
    "    employer_website.append(data['data'][i]['employer_website'])\n",
    "    job_id.append(data['data'][i]['job_id'])\n",
    "    job_employment_type.append(data['data'][i]['job_employment_type'])\n",
    "    job_apply_link.append(data['data'][i]['job_apply_link'])\n",
    "    job_city.append(data['data'][i]['job_city'])\n",
    "    job_country.append(data['data'][i]['job_country'])\n",
    "    job_posted_at_date.append(data['data'][i]['job_posted_at_datetime_utc'][:10])\n",
    "    employer_company_type.append(data['data'][i]['employer_company_type'])\n",
    "\n",
    "    # Extract the job title, required education, and offer expiration date from the data\n",
    "    job_title_data = data['data'][i]['job_title']\n",
    "    job_required_education_data = data['data'][i]['job_required_education']\n",
    "    job_offer_expiration_datetime_utc_data = data['data'][i]['job_offer_expiration_datetime_utc']\n",
    "\n",
    "    # Call the extract_role function and unpack the returned tuple\n",
    "    description = extract_role_1(i)\n",
    "\n",
    "    # Append the extracted values and the description to the corresponding lists\n",
    "    job_title.append(job_title_data)\n",
    "    job_required_education.append(job_required_education_data)\n",
    "    job_offer_expiration_datetime_utc.append(job_offer_expiration_datetime_utc_data)\n",
    "    job_description.append(description)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>employer_website</th>\n",
       "      <th>job_employment_type</th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_description</th>\n",
       "      <th>job_apply_link</th>\n",
       "      <th>job_id</th>\n",
       "      <th>job_city</th>\n",
       "      <th>job_country</th>\n",
       "      <th>job_posted_at_date</th>\n",
       "      <th>employer_company_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.wavegp.com</td>\n",
       "      <td>FULLTIME</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>responsible for the design, build and deployme...</td>\n",
       "      <td>https://ca.linkedin.com/jobs/view/data-enginee...</td>\n",
       "      <td>BiwWJEFt2-cAAAAAAAAAAA==</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>CA</td>\n",
       "      <td>2023-07-14</td>\n",
       "      <td>Information</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.sunlife.com</td>\n",
       "      <td>FULLTIME</td>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>what will you do?\\n• obsessed with understandi...</td>\n",
       "      <td>https://talenthub.vectorinstitute.ai/jobs/2431...</td>\n",
       "      <td>FVDNLvtQe-UAAAAAAAAAAA==</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>CA</td>\n",
       "      <td>2023-07-04</td>\n",
       "      <td>Finance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>FULLTIME</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>description is not intended to be all-inclusiv...</td>\n",
       "      <td>https://ca.linkedin.com/jobs/view/data-enginee...</td>\n",
       "      <td>BwdJN3KD3wIAAAAAAAAAAA==</td>\n",
       "      <td>None</td>\n",
       "      <td>CA</td>\n",
       "      <td>2023-07-12</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>CONTRACTOR</td>\n",
       "      <td>Intermediate Data Engineer</td>\n",
       "      <td>responsibilities:\\n• design and implement the ...</td>\n",
       "      <td>https://www.modis.com/en-ca/job-seekers/job/to...</td>\n",
       "      <td>92hPpLOJcUAAAAAAAAAAAA==</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>CA</td>\n",
       "      <td>2023-07-12</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.chainalysis.com</td>\n",
       "      <td>FULLTIME</td>\n",
       "      <td>Data Engineer II</td>\n",
       "      <td>responsible for building and maintaining the d...</td>\n",
       "      <td>https://www.crypto-careers.com/jobs/245961654-...</td>\n",
       "      <td>S2mznWapcvwAAAAAAAAAAA==</td>\n",
       "      <td>None</td>\n",
       "      <td>CA</td>\n",
       "      <td>2023-07-12</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://www.cardinal.com</td>\n",
       "      <td>FULLTIME</td>\n",
       "      <td>Sr. Data Engineer</td>\n",
       "      <td>responsible for working with applicants reques...</td>\n",
       "      <td>https://www.prodivnet.com/job/sr-data-engineer...</td>\n",
       "      <td>RR3-JQr_Md8AAAAAAAAAAA==</td>\n",
       "      <td>Vaughan</td>\n",
       "      <td>CA</td>\n",
       "      <td>2023-07-12</td>\n",
       "      <td>Wholesale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>http://www.360insights.com</td>\n",
       "      <td>FULLTIME</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>No keyword found in description</td>\n",
       "      <td>https://ca.linkedin.com/jobs/view/data-enginee...</td>\n",
       "      <td>WVwDooRTBy8AAAAAAAAAAA==</td>\n",
       "      <td>Whitby</td>\n",
       "      <td>CA</td>\n",
       "      <td>2023-07-11</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>None</td>\n",
       "      <td>FULLTIME</td>\n",
       "      <td>Data Engineer (Student Position)</td>\n",
       "      <td>responsible for creating connected data produc...</td>\n",
       "      <td>https://careers-canadalife.icims.com/jobs/2596...</td>\n",
       "      <td>yKB7up8ofogAAAAAAAAAAA==</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>CA</td>\n",
       "      <td>2023-07-12</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>None</td>\n",
       "      <td>FULLTIME</td>\n",
       "      <td>Part-time Teaching Opportunities, Data Engineer</td>\n",
       "      <td>responsible for developing an effective learni...</td>\n",
       "      <td>https://sheridan.njoyn.com/cl3/xweb/XWeb.asp?N...</td>\n",
       "      <td>0r9QhenOP1MAAAAAAAAAAA==</td>\n",
       "      <td>Brampton</td>\n",
       "      <td>CA</td>\n",
       "      <td>2023-07-13</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>http://www.sunlife.com</td>\n",
       "      <td>FULLTIME</td>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>what will you do?\\n• obsessed with understandi...</td>\n",
       "      <td>https://ca.linkedin.com/jobs/view/senior-data-...</td>\n",
       "      <td>yY-aXg7oF_oAAAAAAAAAAA==</td>\n",
       "      <td>Waterloo</td>\n",
       "      <td>CA</td>\n",
       "      <td>2023-07-13</td>\n",
       "      <td>Finance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             employer_website job_employment_type   \n",
       "0       http://www.wavegp.com            FULLTIME  \\\n",
       "1      http://www.sunlife.com            FULLTIME   \n",
       "2                        None            FULLTIME   \n",
       "3                        None          CONTRACTOR   \n",
       "4  http://www.chainalysis.com            FULLTIME   \n",
       "5     http://www.cardinal.com            FULLTIME   \n",
       "6  http://www.360insights.com            FULLTIME   \n",
       "7                        None            FULLTIME   \n",
       "8                        None            FULLTIME   \n",
       "9      http://www.sunlife.com            FULLTIME   \n",
       "\n",
       "                                         job_title   \n",
       "0                                    Data Engineer  \\\n",
       "1                             Senior Data Engineer   \n",
       "2                                    Data Engineer   \n",
       "3                       Intermediate Data Engineer   \n",
       "4                                 Data Engineer II   \n",
       "5                                Sr. Data Engineer   \n",
       "6                                    Data Engineer   \n",
       "7                 Data Engineer (Student Position)   \n",
       "8  Part-time Teaching Opportunities, Data Engineer   \n",
       "9                             Senior Data Engineer   \n",
       "\n",
       "                                     job_description   \n",
       "0  responsible for the design, build and deployme...  \\\n",
       "1  what will you do?\\n• obsessed with understandi...   \n",
       "2  description is not intended to be all-inclusiv...   \n",
       "3  responsibilities:\\n• design and implement the ...   \n",
       "4  responsible for building and maintaining the d...   \n",
       "5  responsible for working with applicants reques...   \n",
       "6                    No keyword found in description   \n",
       "7  responsible for creating connected data produc...   \n",
       "8  responsible for developing an effective learni...   \n",
       "9  what will you do?\\n• obsessed with understandi...   \n",
       "\n",
       "                                      job_apply_link   \n",
       "0  https://ca.linkedin.com/jobs/view/data-enginee...  \\\n",
       "1  https://talenthub.vectorinstitute.ai/jobs/2431...   \n",
       "2  https://ca.linkedin.com/jobs/view/data-enginee...   \n",
       "3  https://www.modis.com/en-ca/job-seekers/job/to...   \n",
       "4  https://www.crypto-careers.com/jobs/245961654-...   \n",
       "5  https://www.prodivnet.com/job/sr-data-engineer...   \n",
       "6  https://ca.linkedin.com/jobs/view/data-enginee...   \n",
       "7  https://careers-canadalife.icims.com/jobs/2596...   \n",
       "8  https://sheridan.njoyn.com/cl3/xweb/XWeb.asp?N...   \n",
       "9  https://ca.linkedin.com/jobs/view/senior-data-...   \n",
       "\n",
       "                     job_id  job_city job_country job_posted_at_date   \n",
       "0  BiwWJEFt2-cAAAAAAAAAAA==   Toronto          CA         2023-07-14  \\\n",
       "1  FVDNLvtQe-UAAAAAAAAAAA==   Toronto          CA         2023-07-04   \n",
       "2  BwdJN3KD3wIAAAAAAAAAAA==      None          CA         2023-07-12   \n",
       "3  92hPpLOJcUAAAAAAAAAAAA==   Toronto          CA         2023-07-12   \n",
       "4  S2mznWapcvwAAAAAAAAAAA==      None          CA         2023-07-12   \n",
       "5  RR3-JQr_Md8AAAAAAAAAAA==   Vaughan          CA         2023-07-12   \n",
       "6  WVwDooRTBy8AAAAAAAAAAA==    Whitby          CA         2023-07-11   \n",
       "7  yKB7up8ofogAAAAAAAAAAA==   Toronto          CA         2023-07-12   \n",
       "8  0r9QhenOP1MAAAAAAAAAAA==  Brampton          CA         2023-07-13   \n",
       "9  yY-aXg7oF_oAAAAAAAAAAA==  Waterloo          CA         2023-07-13   \n",
       "\n",
       "  employer_company_type  \n",
       "0           Information  \n",
       "1               Finance  \n",
       "2                  None  \n",
       "3                  None  \n",
       "4                  None  \n",
       "5             Wholesale  \n",
       "6                  None  \n",
       "7                  None  \n",
       "8                  None  \n",
       "9               Finance  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.reset_option('display.max_colwidth')\n",
    "\n",
    "# Limit the length of all lists to 10\n",
    "employer_website = employer_website[:10]\n",
    "job_employment_type = job_employment_type[:10]\n",
    "job_title = job_title[:10]\n",
    "job_description = job_description[:10]\n",
    "job_responsibilities = job_responsibilities[:10]\n",
    "job_description_text = job_description_text[:10]\n",
    "job_apply_link = job_apply_link[:10]\n",
    "job_id = job_id[:10]\n",
    "job_city = job_city[:10]\n",
    "job_country = job_country[:10]\n",
    "job_posted_at_date = job_posted_at_date[:10]\n",
    "employer_company_type = employer_company_type[:10]\n",
    "\n",
    "\n",
    "# Create a dictionary where the keys are the column names and the values are the lists of data\n",
    "data_dict = {\n",
    "    'employer_website': employer_website,\n",
    "    'job_employment_type': job_employment_type,\n",
    "    'job_title': job_title,\n",
    "    'job_description': job_description,\n",
    "    'job_apply_link': job_apply_link,\n",
    "    'job_id': job_id,\n",
    "    'job_city': job_city,\n",
    "    'job_country': job_country,\n",
    "    'job_posted_at_date': job_posted_at_date,\n",
    "    'employer_company_type': employer_company_type\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the dictionary\n",
    "df_rapid_dict = pd.DataFrame(data_dict)\n",
    "\n",
    "df_rapid_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create an engine that connects to PostgreSQL server\n",
    "engine = create_engine('postgresql://postgres:P%4055w076@localhost:5432/sampledb')\n",
    "#engine = create_engine('postgresql://etl:demopass@127.0.0.1:5432/Sampledb')\n",
    "\n",
    "\n",
    "# Transfer the DataFrame to SQL\n",
    "df_rapid_dict.to_sql('my_table', engine, if_exists='replace', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
