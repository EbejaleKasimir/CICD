job_id,employer_website,job_employment_type,job_title,job_apply_link,job_description,skillset,job_city,job_country,job_posted_at_date,employer_company_type
NyGu5WXCcFUAAAAAAAAAAA==,http://www.apexsystems.com,CONTRACTOR,Data Engineer,https://ca.linkedin.com/jobs/view/data-engineer-at-apex-systems-3670303114,"Job#: 1358155

Job Description:

Data Engineer

Apex Systems is a global IT services provider, and our staffing practice has an opening for a Data Engineer with a strong background GCP services IE BigQuery, Cloud storage, and DataFlow, experience with Python or Java, and proficient in SQL and NoSQL, to place at our client, a major international retailer.

Client: Major International Retailer

Terms: 12 Month Contract.

Location: Hybrid 2 days/week – Mississauga

Application Process: It is best to apply via the medium on which you are seeing this posting. If you encounter technical difficulties submitting your resume, please send a Word version of your resume to Cole at [email protected] . Please reference Data Engineer, #.

Job Description:

We are seeking a highly skilled data engineer with expertise in Google Cloud Platform and Hadoop to join our team. The ideal candidate will have experience with data pipeline development, data modeling, data warehousing, and ETL processing. The candidate should be able to design, build and maintain scalable and efficient data solutions.

Responsibilities:
• Design and implement GCP-based data solutions to meet business requirements
• Develop and maintain data pipelines, data models, and data warehouses
• Optimize data processing and data storage to improve system performance and reliability
• Develop ETL processes to ingest, transform, and load data from various sources into GCP
• Implement data security and governance best practices
• Troubleshoot and resolve data-related issues

Key Qualifications:
• Bachelors degree in Computer Science, Engineering, or related field
• 5 years minimum of experience in data engineering with a focus on GCP
• Expertise in GCP services such as BigQuery, DataProc
• PySpark
• Strong programming skills in Python or Java
• Experience with SQL and NoSQL databases
• Kafka/pub/sub

Nice to Have:
• Experience with containerization and orchestration tools such as Kubernetes

Soft Skills
• Strong problem-solving and analytical skills
• Strong communication and collaboration skills

Interview Process: There will be 2 rounds of Interviews that will take place in the week of March 27th.

This a great opportunity to join a leading household brand and continue your career in the ecommerce domain. Be a part of a great work environment with a very well organized team and colleagues who will help you succeed.

This is a position that impacts the business enterprise wide with great opportunity for career growth within the business.

If you are not a 99% match to the above, and want to be considered for other opportunities at our enterprise clients, register for our Talent Network where you can receive job alerts about new opportunities that match your interests.

Click here to Register for our Talent Network

EEO Employer

Apex Systems is an equal opportunity employer. We do not discriminate or allow discrimination on the basis of race, color, religion, creed, sex (including pregnancy, childbirth, breastfeeding, or related medical conditions), age, sexual orientation, gender identity, national origin, ancestry, citizenship, genetic information, registered domestic partner status, marital status, disability, status as a crime victim, protected veteran status, political affiliation, union membership, or any other characteristic protected by law. Apex will consider qualified applicants with criminal histories in a manner consistent with the requirements of applicable law. If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation in using our website for a search or application, please contact our Employee Services Department at [email protected] or 844-463-6178.

Apex Systems is a world-class IT services company that serves thousands of clients across the globe. When you join Apex, you become part of a team that values innovation, collaboration, and continuous learning. We offer quality career resources, training, certifications, development opportunities, and a comprehensive benefits package. Our commitment to excellence is reflected in many awards, including ClearlyRated's Best of Staffing® in Talent Satisfaction in the United States and Great Place to Work® in the United Kingdom and Mexico.

Apex Systems is a world-class IT services company that serves thousands of clients across the globe. When you join Apex, you become part of a team that values innovation, collaboration, and continuous learning. We offer quality career resources, training, certifications, development opportunities, and a comprehensive benefits package. Our commitment to excellence is reflected in many awards, including ClearlyRated's Best of Staffing® in Talent Satisfaction in the United States and Great Place to Work® in the United Kingdom and Mexico.

4400 Cox Road

Suite 200

Glen Allen, Virginia 23060

Apex Systems is an equal opportunity employer. We do not discriminate or allow discrimination on the basis of race, color, religion, creed, sex (including pregnancy, childbirth, breastfeeding, or related medical conditions), age, sexual orientation, gender identity, national origin, ancestry, citizenship, genetic information, registered domestic partner status, marital status, disability, status as a crime victim, protected veteran status, political affiliation, union membership, or any other characteristic protected by law. Apex will consider qualified applicants with criminal histories in a manner consistent with the requirements of applicable law. If you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation in using our website for a search or application, please contact our Employee Services Department at [email protected] (Do not submit resumes or solicit consultants to this email address). UnitedHealthcare creates and publishes the Transparency in Coverage Machine-Readable Files on behalf of Apex Systems.","['ETL', 'Orchestration', 'Modeling', 'Python', 'SQL', 'GCP', 'Google Cloud', 'Spark', 'Hadoop', 'Kubernetes']",Mississauga,CA,2023-07-20,Staffing
DxikyqCFolIAAAAAAAAAAA==,http://www.hometrust.ca,FULLTIME,Senior Data Engineer,https://ca.linkedin.com/jobs/view/senior-data-engineer-at-home-trust-company-3669568596,"Position Overview

Home Trust is building an Azure-based next generation, modern Data & Analytics environment, reviewed and recognized by Microsoft as a “world-class architecture”. We are seeking team members with extreme talent in Azure components (ADF, Databricks/Delta Lake, Synapse, PBI) to help us complete our vision. If you are an exceptional Data Engineering talent who loves to work closely with business consumers using Agile methods, consider joining us on the ground floor of our data-driven journey.

As a key member of the Reporting & Analytics (R&A) team, the Data Engineer will focus on the design, development, and implementation of robust, scalable, and secure data pipelines. We are looking for a passionate solutions-focused individual possessing a strong background in Cloud-based technology with proven experience implementing Data Warehouse pipelines using the Azure Platform and Databricks. The ideal candidate is a technologist with ETL/ELT development and data modelling experience combined with business acumen.

Position Responsibilities
• Design and implement scalable data pipelines solutions for HTC’s Cloud Data & Analytics environment to support anticipated growth in data volumes, data sources, metadata and complexity.
• Solve complex technical problems in addition to mentor/support other technical team members on Databricks related issues.
• Collaborate across the Reporting & Analytics team and business consumers to improve data feeds and models for Business Intelligence and Advanced Analytical tools and applications.
• Collaborate with IT Engineers, Integration Engineers, and Enterprise Architecture to improve and sustain HTC’s overall data platform architecture.
• Translate requirements into conceptual and detailed designs with the insight to estimate work.
• Develop clean, maintainable code in a CI/CD environment.
• Write unit/integration tests and document work.
• Provide thought leadership in solutioning for our Databricks Delta Lake in Azure Environment.
• This includes ETL D/CUT, break/fix, ETL architecture, Error handling strategy, L/PDM data modeling and Solution architecture, but not limited to

Related Experience
• 3+ years in Data Warehousing, business consultation and solution design.
• 3+ years Implementation experience with Databricks/Delta Lake Platform using Databricks Auto Loader and Azure Data Factory.
• Fluent with Python, SQL, Spark, Scala, Databricks technologies.
• Experience with batch data pipelining framework.
• Good knowledge of streaming data platform and event-driven architecture.
• Comfortable working with SQL Server and other RDBMS.
• Strong understanding of Data Warehouse SDLC, multi-dimensional data modelling, metadata management, data security, and predictive analytics
• Knowledge of source control with Git, Azure DevOps, CI/CD.
• Prior experience in the Financial Industry is an asset.","['ETL', 'Modeling', 'Python', 'SQL', 'Spark', 'Databricks', 'Azure', 'Ci/Cd', 'ELT']",Toronto,CA,2023-07-20,
oErwuQU1ptkAAAAAAAAAAA==,http://manifestclimate.com,FULLTIME,Intermediate Data Engineer,https://manifestclimate.applytojob.com/apply/ArmxrU6Hdz/Intermediate-Data-Engineer,"Intermediate Data Engineer

Manifest Climate, Toronto, Ontario, Canada, Full time

Location: Downtown Toronto (flexible)

We are expanding our world-class team of change makers, and we want you to join our team! Manifest Climate is seeking an intermediate Data Engineer to join our team. In this role, you will play a crucial part in shaping our data infrastructure, contributing to our innovative projects related to climate risk planning.

Who we are

Manifest Climate is a leading SaaS (Software as a Service) Climate Risk Planning (CRP) solution built by climate experts to guide companies through the climate transformation.

We are a company driven by leading experts with a passion to make an impact. If you are a forward thinker and strategic doer, interested in purpose-driven work that helps real-world decision makers and influencers, we think you'd like it here.

What’s in it for you

Learn and grow:

Manifest Climate operates at the leading edge of climate change intelligence and risk planning, innovation, and strategic business opportunity. You will play a role in our collective effort to excel in everything we do. You are comfortable in ambiguity and will thrive in a dynamic and agile environment that encourages everyone to act with a sense of leadership, autonomy, and radical responsibility.

Make an impact:

If you want to channel your passion for climate consciousness into your daily work and embrace a culture of performance, this is the opportunity you’ve been looking for. As a Data Engineer at Manifest Climate, you will have the opportunity to contribute to vital work in the climate risk planning field while growing your skills and career in a dynamic and supportive environment.

How you will make a difference:
• Maintain and optimize our data extraction tools and scripts to enhance operational efficiency.
• Work with team members to develop modular data processing components and pipelines.
• Improve the modularity of our data processing code for easier maintenance and seamless integration with other systems.
• Develop effective strategies for data delivery, including considerations for APIs, batch processing, load balancing, and scalability.
• Establish and maintain a robust QA/dev environment.
• Streamline the scheduling and processing of our ETL deployments for optimal performance.
• Implement unit tests for ETL tables to ensure data integrity.
• Improve the interaction between our business intelligence tools and our primary data sources.
• Develop and maintain comprehensive documentation for data processes, systems, and workflows.
• Enhance data quality and availability to support business development opportunities.
• Collaborate with product development teams to facilitate data integration and break down data silos.
• Support third-party integration with environmental data vendors to bolster our climate risk planning initiatives.

What you bring to our collaborative space:
• Bachelor's degree in Computer Science, Data Engineer, or a related field.
• Proven experience as a Data Engineer or similar role.
• Strong knowledge of databases and data processing technologies.
• Experience with data visualization tools, such as Power BI, Tableau, or similar.
• Proficiency in data manipulation languages, such as SQL or Python.
• Experience with ETL tools and processes.
• Knowledge of data integrity testing and QA processes in a data-centric environment.
• Excellent problem-solving abilities and attention to detail.
• Good communication skills with the ability to collaborate effectively in a team environment.

If you feel you meet 70% of the qualifications we are looking for and are determined to make a difference, we encourage you to apply. We promise to consider your application fully.

Why join us

A recognized, action-oriented, well-funded, scaling Canadian tech company, Manifest Climate is:
• Founded in 2020 by Laura Zizzo and Jeremy Greven, headquartered in Toronto
• Recognized in Fast Company’s World Changing Ideas Awards (2022) in AI + Data and certified as a “Great Place to Work”
• Recently completed a CAD $30 million Series A backed by leading technology investors: BDC, Climate Innovation Capital, Klass Capital, Golden Ventures and OMERS Ventures,)
• Trusted by leading corporations: Scotiabank, Manulife, Teck Resources, Hines, Loomis Sayles, Vancity
• Named to Corporate Knights Future 50 in 2022
• Continuously sought after for our perspective by renowned publications such as Forbes, Financial Times and Globe and Mail

Benefits and perks

We are a people first organization and we invest in our people accordingly. Benefits and perks we offer to our team, include but are not limited to:
• A competitive compensation package that includes a path to earn equity options
• An engaging hybrid culture with work-from-home flexibility and collaboration time at the Toronto based office
• Comprehensive health and dental benefits with a health spending account
• Flexible vacation policy
• Parental leave benefits with a top-up plan
• A yearly allowance for professional development and learning opportunities
• A membership to the Carbon Neutral Club

Evolve, improve, and thrive with us.

Our unified belief in our organizational purpose drives our culture, the caliber of talent we attract, our work ethic, resolve, and determination.

We are guided by our values, including Positive Attitude, Curiosity, Collaboration, Accountability, Radical Responsibility, People and Planet Centricity and well-being to build an amazing place to work where every team member is empowered, supported, and energized.

We are always on the lookout for committed and passionate professionals interested in contributing their talents in a challenging, energetic, and inter-disciplinary environment.

We strive to build a team that reflects the diversity of the communities where we live and work. We encourage applications from traditionally underrepresented groups such as women, visible minorities, Indigenous peoples, people identifying as LGBTQ2SI, veterans, and people with disabilities. If we can make this easier through accommodation in the recruitment process, please let us know.","['ETL', 'Python', 'SQL']",Toronto,CA,2023-07-20,
C-I17gv2DvoAAAAAAAAAAA==,http://www.bell.ca,FULLTIME,"Senior Data Engineer, Business Intelligence",https://www.careerbeacon.com/en/job/1905936/bell/senior-data-engineer-business-intelligence/toronto-on,"Req Id: 410238

At Bell, we do more than build world-class networks, develop innovative services and create original multiplatform media content – we're revolutionizing how Canadians communicate.

If you're ready to bring game-changing ideas to life and join a community that values bold ideas, professional growth and employee wellness, we want you on the Bell team.
Summary

As a Senior Data Engineer reporting to the Senior Manager of Finance, you will need to consult with various stakeholders to understand business and technology challenges, design solutions in partnership with other subject matter experts. The ideal candidate will deliver solutions that improve Finance efficiencies and insights. Using your Leadership, technical, Business Intelligence and Finance skills, you will build tools to improve speed and accuracy in reports and generate insights that will allow Finance Operations to analyze, predict and optimize business results.
In particular, you will lead a team to create solutions and insight that combine, transform, analyze, share and catalyze consumption of financial results and related performance KPIs. The ideal candidate will leverage their leadership, technical, business, digital and creative competencies to drive information and insight from the large amount of data stored within our environments reporting status to the Senior Leadership.
The ideal candidate will leverage your natural curiosity, in testing and learning various tools, models, media and design as you continually look for new ways to glean key insights from the data, display results, and engage the stakeholder community with relevant solutions.
This unique position will expose you to the finance, BI, visualization and analytics fields. Leveraging leading edge enterprise technologies, process and governance will enable you to gather and share valuable information allowing us to better evaluate operational strategies while at the same time, from a finance perspective, allowing us to determine potential impact on future financial results.
Working individually and leading a team, you will provide accurate and useful information to the appropriate audiences.
The opportunity to communicate and work with employees & senior finance leadership from across the company, advocating the D&A CoE vision, will provide excellent networking opportunities.
Key Responsibilities
• Lead a team of Data Engineers & Data Analyst
• Create and maintain optimal data pipeline architecture
• Assemble large, complex data sets that meet functional / non-functional business requirements.
• Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
• Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL 'big data' technologies.
• Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
• Work with stakeholders to assist with data-related technical issues and support their data infrastructure needs.
• Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
• Work with data and analytics experts to strive for greater functionality in our data systems.
Critical Qualifications
• Candidate with 5+ years of senior experience working in the telecom industry on complex data sets
• Strong leadership and high tolerance of ambiguity
• Polished written and verbal communication skills
• Experience leading team to build and optimize 'big data' data pipelines, architectures and data sets
• Proven Leadership and business acumen, with demonstrated ability to influence and to be a trusted advisor to senior management
• Advanced working SaS knowledge and experience working with relational databases, query authoring (PROC SQL).
• Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
• Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
• Strong analytic skills related to working with unstructured datasets.
• Build processes supporting data transformation, data structures, metadata, dependency and workload management.
• A successful history of manipulating, processing and extracting value from large disconnected datasets.
• Strong organizational and communication skills.
• Experience supporting and working with cross-functional teams in a dynamic environment.
• We are looking for a candidate with 3+ years of experience in a Data Engineer role
• Experience with relational databases SaS, Teradata, Oracle & MS SQL.
Preferred Qualifications
• Telecom systems knowledge required (ie: P77, PBW, Network Capital)
• Experience with data pipeline and workflow management tools.
• Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools: Alteryx, SaS EG, SQL Assistant, MS Office
• Experience with big data tools: Hadoop, Spark, Kafka, ""Cloud computing solutions"", an asset
• Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.

#EmployeeReferralProgram

Adequate knowledge of French is required for positions in Quebec.

Additional Information:

Position Type: Management
Job Status: Regular - Full Time
Job Location: Canada : Ontario : Toronto || Canada : Nova Scotia : Halifax || Canada : Quebec : Montreal
Work Arrangement: Hybrid
Application Deadline: 07/17/2023

For work arrangements that are 'Hybrid', successful candidates must be based in Canada and report to a set Bell office for a minimum of 3 days a week. Recognizing the importance of work-life balance, Bell offers flexibility in work hours based on the business needs.

Please apply directly online to be considered for this role. Applications through email will not be accepted.

At Bell, we don't just accept difference - we celebrate it. We're committed to fostering an inclusive, equitable, and accessible workplace where every team member feels valued, respected, and supported, and has the opportunity to reach their full potential. We welcome and encourage applications from people with disabilities.

Accommodations are available on request for candidates taking part in all aspects of the selection process. For a confidential inquiry, simply email your recruiter directly or recruitment@bell.ca to make arrangements. If you have questions regarding accessible employment at Bell please email our Diversity & Inclusion Team at inclusion@bell.ca.

Created: Canada, ON, Toronto

Bell, one of Canada's Top 100 Employers.","['Python', 'SQL', 'Spark', 'Hadoop']",Toronto,CA,2023-07-14,Information
sxkWRmSdZX0AAAAAAAAAAA==,,FULLTIME,Senior Data Engineer,https://ca.linkedin.com/jobs/view/senior-data-engineer-at-pentasia-3670429779,"We are looking for a skilled individual to join our IT team to develop and support reporting solutions using MySQL databases.

Responsibilities:
• Develop and optimize complex SQL scripts and stored functions to support reporting requests from business users;
• Design and develop ETL process using bash scripting in Linux to automate data loads and extracts from transactional database to the reporting database;
• Investigate reporting/sampling of possible data anomalies;
• Design and develop data validation scripts to ensure integrity of the data in the reporting system;
• Monitor and report abnormal data loads, slow queries, and data integrity anomalies.

Requirements:
• 3-5 year(s) experience writing SQL queries and stored procedures using MySQL database;
• 3-5 year(s) ETL experience using bash scripts in a Unix Linux environment;
• Experience with MS Excel – charts, graphs, pivot tables, vlookup and other Excel functions;
• Solid understanding of MySQL explain plans and optimization of slow queries;
• Experience developing scripts using bash and running cron jobs;
• Experience working with BIRT will be considered;
• Organized, detailed oriented, meticulous and precise when writing scripts;
• Solid analytical and problem solving skills;
• Ability to handle multiple tasks;
• High attention to detail with a structured and methodical way to review scripts and queries;
• Ability to work in a dynamic, fast-paced environment with competing priorities;
• Ability to adapt to new environments, and the ability to identify, propose and implement changes and improvements;
• Strong interpersonal and communication skills, both written and verbal;
• Formal Computer Science education.","['ETL', 'SQL', 'Linux']",Toronto,CA,2023-07-20,
KkvbepSTAuUAAAAAAAAAAA==,,FULLTIME,Senior AWS Data Engineer - 6-Month Contract,https://ca.linkedin.com/jobs/view/senior-aws-data-engineer-6-month-contract-at-hour-consulting-3669566641,"Our client, a rapidly growing Canadian Technology focused Consulting Agency renowned for its tech-forward approach, seeks an exceptionally skilled Senior Data Engineer. This is a six-month contract position with a strong potential for extension.

Are you passionate about building robust and scalable data engineering solutions in the cloud? Join our client's dynamic Data Squad as a Senior AWS Data Engineer to contribute to developing and maintaining their centralized Data Hub/Warehouse. In this role, you will play a critical part in data ingestion, system integrations, reporting, and analytics by designing and implementing data pipelines and data models.

Candidates must demonstrate proficiency in using programming languages like Python and SQL

Deep familiarity with AWS technologies and a strong data engineering background is essential.

You should have in-depth knowledge of AWS offerings, such as Amazon S3, AWS Glue, AWS Redshift, AWS Lambda, and AWS Step Function.

Responsibilities:
• Maintain and enhance the data warehouse and analytics environment for stability and scalability.
• Design, implement, and deploy reliable data engineering solutions and pipelines, integrating new data sources.
• Build reports and data visualizations to enable data-driven decision-making.
• Develop scalable and automated solutions to enhance data management efficiency.
• Implement top-notch security measures to safeguard sensitive information.

Requirements:
• Minimum 4 years of hands-on experience with AWS cloud services in data engineering.
• In-depth knowledge of AWS offerings related to data storage, processing, and analytics.
• Expertise in building scalable and secure data engineering solutions on AWS.
• Proficiency in Python and SQL for data manipulation, transformation, and analysis within AWS.
• Familiarity with cloud-native data architecture and best practices for data management on AWS.
• Desired experience in data modelling techniques for optimizing data storage and retrieval.

About You:
• Strong work ethic, technology enthusiasm, and passion for AWS ecosystem.
• Agility and stress tolerance to handle tight deadlines and fast-paced environments.
• Adaptability and team collaboration skills to work effectively with cross-functional teams.
• A drive for continuous learning and staying updated with advancements in cloud data engineering.

What's In It For You?
• Join an inclusive, collaborative work environment that celebrates success.
• Access to the latest tools and technology for creating exceptional customer experiences.
• Competitive compensation and additional benefits.
• After-work socials.

Location: Downtown Toronto

Work Arrangement: Hybrid - Toronto office, 3 days from the office.

If you are a skilled AWS Data Engineer passionate about data-driven solutions and want to be part of a collaborative team shaping the future of our business through data insights, we encourage you to apply. We welcome applicants from all backgrounds to contribute to our innovative and inclusive culture.","['Python', 'SQL', 'AWS', 'Amazon', 'S3']",Toronto,CA,2023-07-20,
cqvr2U10pHQAAAAAAAAAAA==,http://www.luxoft.com,FULLTIME,Senior Full Stack Data Engineer,https://ca.linkedin.com/jobs/view/senior-full-stack-data-engineer-at-luxoft-3670338780,"Project Description:

We are looking for an experienced Full Stack Data Engineer to join our team for building a next-generation data platform built on Data Mesh architecture/principles. The ideal candidate should have extensive hands-on experience in building a big data platform, Big Data Technologies, Data Pipelines, backend development using Python, BI/Analytics tools as well as experience with DevOps, AWS, and UI Development in Angular JS. You will be responsible for designing, developing, and maintaining our web applications and data pipelines, as well as implementing CI/CD best practices.

Responsibilities:

• Design, build, and maintain scalable and efficient data platforms using data engineering technologies such as Glue, EMR, Athena, Redshift, Lake Formation, Apache Spark, Hive, HDFS, and Trino.

• Build/manage data pipelines, and common data-related cross-cutting concerns like data catalog, data lineage, data quality, data profiling, data discovery, metadata management

• Develop and maintain web applications using AngularJS and Python.

• Build/manage BI/Analytical dashboard reducing time to insight for the business stakeholders.

• Implement CI/CD pipelines using Terraform, Jenkins, Github actions, and Gitflow.

• Collaborate with cross-functional teams to develop and implement new features.

• Write clean, reusable, and efficient code.

• Participate in code reviews and ensure code quality.

• Develop and maintain APIs using Python and ensure API security and best practices are implemented.

• Implement SSO integration with Microsoft Azure AD using oAuth, OIDC, and SAML.

• Implement integration with AWS Cognito for user authentication and authorization.

• Ensure the application is optimized for maximum speed and scalability.

• Troubleshoot and debug issues as they arise.

• Implement DevOps best practices to ensure efficient application deployment and management.

• Collaborate with data scientists and analysts to integrate data analytics solutions with web applications.

• Stay up to date with emerging trends and technologies.

Mandatory Skills Description:

• 8+ years of experience in similar positions;

• Hands-on Experience with data engineering technologies such as AWS Glue, EMR, Athena, Redshift, Lake Formation, Apache Spark, Apache Hive, Apache Airflow, S3FS, Apache Hudi, and Trino.

• Extensive experience in building data pipelines using orchestration tools like Apache Airflow. Hands-on experience in building cross-cutting concerns like data catalog, data lineage, data quality, data profiling, data discovery, metadata management

• Proven experience as a Full Stack Developer with AngularJS and Python.

• Strong understanding of web development technologies including HTML, CSS, and JavaScript.

• Experience working with RESTful APIs and JSON. Familiarity with microservices architecture.

• Experience with core AWS technologies such as EC2, ELB, Auto Scaling, S3, EFS, Lambda, API

Gateway, Step Functions, Cloudwatch, VPC, Route 53, ACM

• Hands on experience with SQL and NoSQL databases.

• Hands experience with BI tools like Tableau, AWS QuickSight

• Experience with Git or other version control systems.

• Understanding of agile development methodologies.

• Strong problem-solving skills.

• Excellent written and verbal communication skills.

• Ability to work independently and collaboratively in a team environment.

• Experience with cloud platforms such as AWS.

• Bachelor's degree in Computer Science, Engineering, or related field

Nice-to-Have Skills:

• Agile, Scrum framework 2+ years' experience on past projects

• Knowledge of containerization technologies like Docker or Kubernetes.

• Experience with front-end frameworks like React or Vue.js.

• Experience with DevOps and CI/CD best practices.

• Experience with Terraform and the AWS provider.

• Experience with API development, security, and best practices.

• Experience with SSO integration with Microsoft Azure AD using oAuth, OIDC, and SAML.

• Experience with integration with AWS Cognito for user authentication and authorization

Languages:English: C1 Advanced","['Orchestration', 'Python', 'SQL', 'Docker', 'AWS', 'Spark', 'Azure', 'S3', 'Kubernetes', 'Hive', 'Airflow', 'Ci/Cd']",Toronto,CA,2023-07-20,Computer Services
HSIiLWO1wuUAAAAAAAAAAA==,http://www.cgi.com,FULLTIME,Cloud Data Engineer,https://ca.linkedin.com/jobs/view/cloud-data-engineer-at-cgi-3669094810,"Position Description

Location: Hybrid/GTA

Job Type: Full-Time

We are seeking a highly skilled and experienced Cloud Data Engineer to join our dynamic team. In this role, you will play a crucial role in designing and implementing cutting-edge data solutions using a variety of technologies and platforms.

Your future duties and responsibilities
• Redesign, develop, and maintain scalable and efficient data solutions on the cloud platform.
• Implement and optimize data pipelines using GCP services such as BigQuery, Dataflow, PubSub, and Data Migration tools.
• Build and maintain data warehouses for efficient data storage, retrieval, and analysis.
• Troubleshoot and debug issues related to data pipelines, ensuring smooth and reliable data flow.
• Collaborate with the Data and Analytics team to design and implement data-driven solutions.
• Utilize ELK, Grafana, and other monitoring tools to ensure optimal performance and observability of data systems.
• Work with Looker to handle large-scale data processing and reporting requirements.
• Apply Site Reliability Engineering (SRE) principles to enhance system reliability, incident management, and operations.
• Develop and maintain a robust observability framework for monitoring, alerting, and tracking data system performance.
• Collaborate with cross-functional teams to integrate data systems via APIs and ensure seamless data integration.
• Familiarity with common data pipeline issues and their solutions

Required Qualifications To Be Successful In This Role
• Bachelor’s degree in computer science, Engineering, or a related field.
• Proven experience working with GCP services including BigQuery, Dataflow, PubSub, and Data Migration tools.
• Strong proficiency in data warehouse design and implementation.
• Proficiency in troubleshooting and debugging data pipeline issues.
• Solid understanding of data and analytics principles.
• Experience with ELK stack and Grafana for monitoring and observability.
• Familiarity with Hadoop, Looker, and other relevant data processing and reporting tools.
• Knowledge of Site Reliability Engineering (SRE) principles and incident management.
• Strong analytical and problem-solving skills.
• Excellent communication and teamwork abilities.
• Experience with Kafka and API integration is a plus.

Join our team of cloud professionals and contribute to the success of our cutting-edge projects. We offer a competitive salary, comprehensive benefits package, and a stimulating work environment.

Insights you can act on

While technology is at the heart of our clients’ digital transformation, we understand that people are at the heart of business success.

When you join CGI, you become a trusted advisor, collaborating with colleagues and clients to bring forward actionable insights that deliver meaningful and sustainable outcomes. We call our employees ""members"" because they are CGI shareholders and owners and owners who enjoy working and growing together to build a company we are proud of. This has been our Dream since 1976, and it has brought us to where we are today — one of the world’s largest independent providers of IT and business consulting services.

At CGI, we recognize the richness that diversity brings. We strive to create a work culture where all belong and collaborate with clients in building more inclusive communities. As an equal-opportunity employer, we want to empower all our members to succeed and grow. If you require an accommodation at any point during the recruitment process, please let us know. We will be happy to assist.

Ready to become part of our success story? Join CGI — where your ideas and actions make a difference.","['GCP', 'Hadoop']",Toronto,CA,2023-07-20,Computer Services
ZJqhTNEpPXkAAAAAAAAAAA==,http://www.spinmaster.com,FULLTIME,Data Engineer,https://ca.linkedin.com/jobs/view/data-engineer-at-spin-master-3666783692,"Please Note: If you are a current Spin Master employee with access to Workday, apply to this job via the Workday application.

Are you a kid at heart looking to build a career with a leading global children's toy, entertainment and digital gaming company?

At Spin Master, our unwavering commitment to open mindedness, integrity and innovation is a great part of what has made us an industry leader. How do we stay ahead of the pack? By hiring the best and brightest minds—and that’s why we want you!

Job Description

What will you work on?

You will be designing and developing tools, data products and data pipelines that can cater to Data Analysis Data Science needs of the organization

How will you create impact?
• Work with various business stakeholders and IT to identify and meet data requirements.
• Able to architect, develop, and manage our data infrastructure.
• Deliver scalable, testable, maintainable, and high-quality code.
• Build a solid foundation, that can easily scale to changing business needs, for calculating key business metrics.
• Enable cross functional collaboration by building an agile data insight driven culture.
• Communicating updates and changes to the broader data team as well as contributing to and maintaining data-related documentation.
• Have a high-level understanding of data (data sources, type of data, data definition)
• Remote First setup with 15-20% of time in office to start, but subject to change

What are your skills and experience?
• 2+ years’ experience in a Data Engineering type role.
• A degree in computer science/computer engineering or a resume full of practical experience.
• Proficient with SQL and familiar with relational / non-relational database approaches and knowing which to apply where and when.
• Understand the principles of software engineering and are comfortable coding in one or more programming languages (eg: Java, Python, Scala, etc).
• Understand the Data Lifecycle Management is a plus.
• Hands on experience in data warehouse (Azure, AWS or GCP) and related products.

What You Can Expect From Us

Our mission is to Make Life More Fun with a vision to push the boundaries of innovation, creativity, and fun.
• Growth and Career Opportunities
• Flexible Work Hours
• Innovation, Collaboration and Fun
• Comprehensive Benefits
• Other fun Perks!

What’s it like to work here?

Spin Master is a fast-paced, hands-on organization that provides many great opportunities for impactful decision-making; though our challenging start-up atmosphere isn’t for everyone, we have a proven record of opportunities for future advancement and internal transfers for our passionate and results driven team.

Everyone is welcome in our sandbox and we are committed to an accessible and inclusive hiring process that provides reasonable accommodation to all applicants.

Spin Master strives to create an accessible and inclusive application and selection process and is committed to working with and providing reasonable accommodation to job applicants who may require provisions to participate in the recruitment, selection and/or assessment processes. Should you require an accommodation, please contact our Talent Acquisition team, by email at TAinquiries@spinmaster.com or by phone at 416 364-6002 and we will work with you to meet your accessibility needs.

Follow us on Instagram and Twitter @SpinMaster to stay up to date on Spin Master career opportunities.

We do appreciate all interest; however only those selected for interview will be contacted.","['Python', 'SQL', 'AWS', 'GCP', 'Azure']",Toronto,CA,2023-07-18,Entertainment
wQlXijl0glcAAAAAAAAAAA==,,FULLTIME,"Data Engineer (Support Role) with Spark, Scala, AWS, Kafka :: Ontario Canada (Hybrid) :: Fulltime",https://ca.linkedin.com/jobs/view/data-engineer-support-role-with-spark-scala-aws-kafka-ontario-canada-hybrid-fulltime-at-testingxperts-3669571013,"Dear,

Role: Data Engineer (Support Role)

Location: Ontario Canada (Hybrid)

Only FTE

Must Have skills – Spark, Scala, AWS, Kafka

Good To have – Databricks

Job Description:
• On call support
• Maintain & support multiple data pipelines, includes error / bug fixes, platform upgrades & enhancements
• Establish best engineering practices and operation excellence
• Make a difference to the product in terms of the innovative thoughts, best designs and contribute towards identifying technical risks and find alternate solutions to various problems

Kind Regards,

Ayush Sharma (Executive Recruiter)

Office Number: 6317598044 EXT. 513

https://www.linkedin.com/in/ayush-sharma-a65b53243/

Ayush.sharma@testingxperts.com","['AWS', 'Spark', 'Databricks']",Toronto,CA,2023-07-20,
