{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "\n",
    "def scrape_amazon(categories):\n",
    "    # Setup driver\n",
    "    options = webdriver.EdgeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    driver = webdriver.Edge(service=Service(EdgeChromiumDriverManager().install()), options=options)\n",
    "\n",
    "    # Extract product details from each category\n",
    "    all_products = []\n",
    "    for category, base_url in categories.items():\n",
    "        products = []\n",
    "        for page in range (1,3):\n",
    "            # Navigate to the search results page\n",
    "            url = f'{base_url}&page={page}'\n",
    "            driver.get(url)\n",
    "            time.sleep(5) # wait for page to load\n",
    "\n",
    "            # Extract product details\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            for product in soup.find_all('div', class_='sg-col-inner'): \n",
    "                product_dict = {}\n",
    "\n",
    "                item_name = product.find('span', class_='a-size-base-plus a-color-base a-text-normal')\n",
    "                if item_name is not None:\n",
    "                    product_dict['product'] = item_name.text.strip()\n",
    "\n",
    "                product_price = product.find('span', class_='a-offscreen')\n",
    "                if product_price is not None:\n",
    "                    product_price = product_price.text.strip()\n",
    "                    product_dict['price'] = product_price.replace(\"$\", \"\").replace(\",\", \"\").strip()\n",
    "\n",
    "                rating = product.find('span', class_='a-icon-alt')\n",
    "                if rating is not None:\n",
    "                    product_dict['ratings'] = rating.text.strip().split(\" \")[0]\n",
    "\n",
    "                item_reviews = product.find('span', class_='a-size-base')\n",
    "                if item_reviews is not None:\n",
    "                    reviews_text = item_reviews.text.strip()\n",
    "                    reviews_count = reviews_text.split(\" \")[0]\n",
    "                    product_dict['reviews'] = reviews_count.strip()\n",
    "\n",
    "                # Add category to product_dict\n",
    "                product_dict['category'] = category\n",
    "\n",
    "                # Only append product_dict if it has some data\n",
    "                if product_dict:\n",
    "                    products.append(product_dict)\n",
    "\n",
    "        all_products.extend(products)\n",
    "\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "\n",
    "    # Output the result as JSON\n",
    "    return json.dumps(all_products)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  # Define the categories and their base URLs\n",
    "  categories = {\n",
    "    'electronics': 'https://www.amazon.com/s?rh=n%3A16225009011&fs=true&ref=lp_16225009011_sar',\n",
    "    'Computers': 'https://www.amazon.com/s?rh=n%3A16225007011&fs=true&ref=lp_16225007011_sar',\n",
    "    'video_games': 'https://www.amazon.com/s?rh=n%3A16225016011&fs=true&ref=lp_16225016011_sar',\n",
    "    'boys_clothing_8-20':'https://www.amazon.com/s?i=specialty-aps&bbn=16225021011&rh=n%3A16225021011%2Cn%3A1040666%2Cp_n_size_six_browse-vebin%3A4940401011&dc&fst=as%3Aoff&pf_rd_i=16225021011&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=d84623b2-8aff-40df-9701-224067aef31e&pf_rd_r=BRC30MHTYX0XAFZA3N94&pf_rd_s=merchandised-search-3&pf_rd_t=101&qid=1511397964&rnid=49403&ref=s9_acss_bw_cg_AEGFVN2E_1a1_w',\n",
    "    'boys_clothing_2-7':'https://www.amazon.com/s?i=specialty-aps&bbn=16225021011&rh=n%3A16225021011%2Cn%3A1040666%2Cp_n_size_six_browse-vebin%3A4940400011&dc&fst=as%3Aoff&pf_rd_i=16225021011&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=d84623b2-8aff-40df-9701-224067aef31e&pf_rd_r=BRC30MHTYX0XAFZA3N94&pf_rd_s=merchandised-search-3&pf_rd_t=101&qid=1511397964&rnid=49&ref=s9_acss_bw_cg_AEGFVN2E_1b1_w',\n",
    "    'boys_clothing_ 0-24M':'https://www.amazon.com/s?i=specialty-aps&bbn=16225005011&rh=n%3A%2116225005011%2Cn%3A7147444011%2Cn%3A7628013011&dc&fst=as%3Aoff&pf_rd_i=16225021011&pf_rd_m=ATVPDKIKX0DER&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=d84623b2-8aff-40df-9701-224067aef31e&pf_rd_r=BRC30MHTYX0XAFZA3N94&pf_rd_s=merchandised-sear&pf_rd_s=merchandised-search-3&pf_rd_t=101&qid=1510445001&rnid=7147444011&ref=s9_acss_bw_cg_AEGFVN2E_1c1_w',\n",
    "    'girls_clothing_7-16':'https://www.amazon.com/s?bbn=16225020011&rh=n%3A7141123011%2Cn%3A16225020011%2Cn%3A1040664%2Cp_n_size_six_browse-vebin%3A4940398011&dc&fst=as%3Aoff&pf_rd_i=16225020011&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=2a239f2b-0318-4c5d-be33-9cc1f0eed9b3&pf_rd_r=QGPCTNEA3G2MD9BWY49S&pf_rd_s=merchandised-search-3&pf_rd_t=101&qid=1489098061&rnid=4940396011&ref=s9_acss_bw_cg_AEGFVN2E_1a1_w',\n",
    "    'girls_clothing_2-6X':'https://www.amazon.com/s?bbn=16225020011&rh=n%3A7141123011%2Cn%3A16225020011%2Cn%3A1040664%2Cp_n_size_six_browse-vebin%3A4940397011&dc&fst=as%3Aoff&pf_rd_i=16225020011&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=2a239f2b-0318-4c5d-be33-9cc1f0eed9b3&pf_rd_r=QGPCTNEA3G2MD9BWY49S&pf_rd_s=merchandised-search-3&pf_rd_t=101&qid=1489098061&rnid=4940396011&ref=s9_acss_bw_cg_AEGFVN2E_1b1_w',\n",
    "    'girls_clothing_0-24M':'https://www.amazon.com/s?keywords=Baby+Girls%27+Clothing+%26+Shoes&bbn=7628012011&rh=n%3A7141123011%2Cn%3A7147444011%2Cn%3A7628012011%2Cp_n_shipping_option-bin%3A3242350011&dc&c=ts&ts_id=7628012011&ref=s9_acss_bw_cg_AEGFVN2E_1c1_w',\n",
    "    \n",
    "\n",
    "    # Add more categories as needed\n",
    "  }\n",
    "\n",
    "  # Load the JSON output string into a Python List of dictionaries for further processing\n",
    "  amazon_data = json.loads(scrape_amazon(categories))\n",
    "\n",
    "  # Save the JSON data to a file\n",
    "  with open('amazon_data_cat.json', 'w') as file:\n",
    "    json.dump(amazon_data, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "\n",
    "def scrape_amazon(categories):\n",
    "    # Setup driver\n",
    "    options = webdriver.EdgeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    driver = webdriver.Edge(service=Service(EdgeChromiumDriverManager().install()), options=options)\n",
    "\n",
    "    # Extract product details from each category\n",
    "    all_products = []\n",
    "    for category, base_url in categories.items():\n",
    "        products = []\n",
    "        for page in range (1,2):\n",
    "            # Navigate to the search results page\n",
    "            url = f\"{base_url}&page={page}\"\n",
    "            driver.get(url)\n",
    "            time.sleep(5) # wait for page to load\n",
    "\n",
    "            # Extract product details\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            for product in soup.find_all('div', class_='sg-col-inner'): \n",
    "                product_dict = {}\n",
    "\n",
    "                item_name = product.find('span', class_='a-size-base-plus a-color-base a-text-normal')\n",
    "                if review_responder_population is not None:\n",
    "                    product_dict['product'] = review_responder_population.text.strip()\n",
    "\n",
    "                product_price = product.find('span', class_='a-offscreen')\n",
    "                if product_price is not None:\n",
    "                    product_price = product_price.text.strip()\n",
    "                    product_dict['price'] = product_price.replace(\"$\", \"\").replace(\",\", \"\").strip()\n",
    "\n",
    "                rating = product.find('span', class_='a-icon-alt')\n",
    "                if rating is not None:\n",
    "                    product_dict['ratings'] = rating.text.strip().split(\" \")[0]\n",
    "\n",
    "                review_responder_population = product.find('span', class_='a-size-base s-underline-text')\n",
    "                if review_responder_population is not None:\n",
    "                    review_responders = review_responder_population.text.strip().split(\" \")[0]\n",
    "                else:\n",
    "                    review_responders = None\n",
    "\n",
    "                product_dict['review_responders'] = review_responders\n",
    "\n",
    "\n",
    "                item_reviews = product.find('span', class_='a-size-base')\n",
    "                if item_reviews is not None:\n",
    "                    reviews_text = item_reviews.text.strip()\n",
    "                    reviews_count = reviews_text.split(\" \")[0]\n",
    "                    product_dict['reviews'] = reviews_count.strip()\n",
    "\n",
    "                # Add category to product_dict\n",
    "                product_dict['category'] = category\n",
    "\n",
    "                # Only append product_dict if it has some data\n",
    "                if product_dict:\n",
    "                    products.append(product_dict)\n",
    "\n",
    "        all_products.extend(products)\n",
    "\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "\n",
    "    # Output the result as JSON\n",
    "    return json.dumps(all_products)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  # Define the categories and their base URLs\n",
    "  categories = {\n",
    "\n",
    "    'Smartphones': 'https://www.amazon.com/s?k=smartphone&crid=3L33Q517U80C6&sprefix=smartphone%2Caps%2C934&ref=nb_sb_noss_1',\n",
    "    'Laptops': 'https://www.amazon.com/s?k=Laptop&crid=3GD9HX5GPNA0R&sprefix=laptop%2Caps%2C569&ref=nb_sb_noss_1',\n",
    "    'video_games': 'https://www.amazon.com/s?rh=n%3A16225016011&fs=true&ref=lp_16225016011_sar',\n",
    "    'Dresses':'https://www.amazon.com/s?k=dress&crid=1ZKY6X4I4VARF&sprefix=dress%2Caps%2C1200&ref=nb_sb_noss_1',\n",
    "    'Shoes':'https://www.amazon.com/s?k=shoes&crid=BWUHHU3UQ38H&sprefix=shoes%2Caps%2C346&ref=nb_sb_noss_1',\n",
    "    'Accessories':'https://www.amazon.com/s?k=accessories+for+clothes&crid=17CJNG61JS7OX&sprefix=accessories+for+clo%2Caps%2C750&ref=nb_sb_ss_ts-doa-p_3_19', # Accessories for Clothes\n",
    "\n",
    "\n",
    "    # Add more categories as needed\n",
    "  }\n",
    "\n",
    "  # Load the JSON output string into a Python List of dictionaries for further processing\n",
    "  amazon_data = json.loads(scrape_amazon(categories))\n",
    "\n",
    "  # Save the JSON data to a file\n",
    "  with open('amazon_data_cat.json', 'w') as file:\n",
    "    json.dump(amazon_data, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "\n",
    "\n",
    "def scrape_amazon(categories):\n",
    "    # Setup driver\n",
    "    options = webdriver.EdgeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    driver = webdriver.Edge(service=Service(EdgeChromiumDriverManager().install()), options=options)\n",
    "\n",
    "    # Extract product details from each category\n",
    "    all_products = []\n",
    "\n",
    "    for category, base_url in categories.items():\n",
    "        products = []\n",
    "\n",
    "        # Iterate through pages for this category\n",
    "        for page in range(1, 3):\n",
    "\n",
    "            url = f\"{base_url}&page={page}\"\n",
    "            driver.get(url)\n",
    "            time.sleep(5) # wait for page to load\n",
    "\n",
    "            # Extract product details\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            for product in soup.find_all('div', class_='sg-col-inner'):\n",
    "                product_dict = {}\n",
    "\n",
    "                # Item name\n",
    "                item_name = product.find('span', class_='a-size-base-plus a-color-base a-text-normal')\n",
    "\n",
    "                # If item name is None, then it is a mouse pad\n",
    "                if item_name is None:\n",
    "                    item_name = product.find('span', class_='a-size-medium a-color-base a-text-normal')\n",
    "\n",
    "                if item_name is not None:\n",
    "                    product_dict['product'] = item_name.text.strip()\n",
    "\n",
    "                  \n",
    "\n",
    "                # Item price\n",
    "                product_price = product.find('span', class_='a-offscreen')\n",
    "                rating = product.find('span', class_='a-icon-alt')\n",
    "                review_responder_population = product.find('span', class_='a-size-base s-underline-text')\n",
    "                item_reviews = product.find('span', class_='a-size-base')\n",
    "\n",
    "                # Check if the product_price variable is None before adding it to the product_dict\n",
    "                if product_price is not None:\n",
    "                    product_price = product_price.text.strip()\n",
    "                    product_dict['price'] = product_price.replace(\"$\", \"\").replace(\",\", \"\").strip()\n",
    "\n",
    "                # Check if the rating variable is None before adding it to the product_dict\n",
    "                if rating is not None:\n",
    "                    product_dict['ratings'] = rating.text.strip().split(\" \")[0]\n",
    "\n",
    "                # Check if the review_responder_population variable is None before adding it to the product_dict\n",
    "                if review_responder_population is not None:\n",
    "                    review_responders = review_responder_population.text.strip().split(\" \")[0]\n",
    "                    product_dict['review_responders'] = review_responders\n",
    "\n",
    "                # Check if the item_reviews variable is None before adding it to the product_dict\n",
    "                if item_reviews is not None:\n",
    "                    reviews_text = item_reviews.text.strip()\n",
    "                    reviews_count = reviews_text.split(\" \")[0]\n",
    "                    product_dict['reviews'] = reviews_count.strip()\n",
    "\n",
    "                # Add category to product_dict\n",
    "                product_dict['category'] = category\n",
    "\n",
    "                # Only append product_dict if it has some data\n",
    "                if len(product_dict.keys()) > 0:\n",
    "                    products.append(product_dict)\n",
    "\n",
    "        all_products.extend(products)\n",
    "\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "\n",
    "    # Output the result as JSON\n",
    "    return json.dumps(all_products)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "  # Define the categories and their base URLs\n",
    "\n",
    "  categories = {\n",
    "\n",
    "    'Smartphones': 'https://www.amazon.com/s?k=smartphone&crid=3L33Q517U80C6&sprefix=smartphone%2Caps%2C934&ref=nb_sb_noss_1',\n",
    "    'Laptops': 'https://www.amazon.com/s?k=Laptop&crid=3GD9HX5GPNA0R&sprefix=laptop%2Caps%2C569&ref=nb_sb_noss_1',\n",
    "    'video_games': 'https://www.amazon.com/s?rh=n%3A16225016011&fs=true&ref=lp_16225016011_sar',\n",
    "    'Dresses':'https://www.amazon.com/s?k=dress&crid=1ZKY6X4I4VARF&sprefix=dress%2Caps%2C1200&ref=nb_sb_noss_1',\n",
    "    'Shoes':'https://www.amazon.com/s?k=shoes&crid=BWUHHU3UQ38H&sprefix=shoes%2Caps%2C346&ref=nb_sb_noss_1',\n",
    "    'Accessories':'https://www.amazon.com/s?k=accessories+for+clothes&crid=17CJNG61JS7OX&sprefix=accessories+for+clo%2Caps%2C750&ref=nb_sb_ss_ts-doa-p_3_19', # Accessories for Clothes\n",
    "\n",
    "    # Add more categories as needed\n",
    "  }\n",
    "\n",
    "  # Load the JSON output string into a Python List of dictionaries for further processing\n",
    "  amazon_data = json.loads(scrape_amazon(categories))\n",
    "\n",
    "  # Save the JSON data to a file\n",
    "  with open('amazon_data_cat.json', 'w') as file:\n",
    "    json.dump(amazon_data, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "\n",
    "def setup_driver():\n",
    "    options = webdriver.EdgeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    return webdriver.Edge(service=Service(EdgeChromiumDriverManager().install()), options=options)\n",
    "\n",
    "def extract_product_details(product, category, seen_products):\n",
    "    product_dict = {\n",
    "        'uuid': None,\n",
    "        'product': None,\n",
    "        'price': None,\n",
    "        'ratings': None,\n",
    "        'reviews': None,\n",
    "        'review_responders': None,\n",
    "        'category': category,\n",
    "    }\n",
    "\n",
    "    # Item ID\n",
    "    item_id = product.find('div', class_='data_uuid')\n",
    "    if item_id is not None:\n",
    "        product_dict['uuid'] = item_id.get('data-uuid')\n",
    "\n",
    "    # Item name\n",
    "    item_name = product.find('span', class_='a-size-base-plus a-color-base a-text-normal') or product.find('span', class_='a-size-medium a-color-base a-text-normal')\n",
    "    if item_name is not None:\n",
    "        product_name = item_name.text.strip()\n",
    "        if product_name in seen_products:\n",
    "            return None\n",
    "        seen_products.add(product_name)\n",
    "        product_dict['product'] = product_name\n",
    "\n",
    "    # Item price\n",
    "    product_price = product.find('span', class_='a-offscreen')\n",
    "    if product_price is not None:\n",
    "        product_price = product_price.text.strip()\n",
    "        product_dict['price'] = product_price.replace(\"$\", \"\").replace(\",\", \"\").strip()\n",
    "\n",
    "    rating = product.find('span', class_='a-icon-alt')\n",
    "    review_responder_population = product.find('span', class_='a-size-base s-underline-text')\n",
    "    item_reviews = product.find('span', class_='a-size-base')\n",
    "\n",
    "    # Check if the rating variable is None before adding it to the product_dict\n",
    "    if rating is not None:\n",
    "        product_dict['ratings'] = rating.text.strip().split(\" \")[0]\n",
    "\n",
    "    # Check if the review_responder_population variable is None before adding it to the product_dict\n",
    "    if review_responder_population is not None:\n",
    "        review_responders = review_responder_population.text.strip().split(\" \")[0]\n",
    "        product_dict['review_responders'] = review_responders\n",
    "\n",
    "    # Check if the item_reviews variable is None before adding it to the product_dict\n",
    "    if item_reviews is not None:\n",
    "        reviews_text = item_reviews.text.strip()\n",
    "        reviews_count = reviews_text.split(\" \")[0]\n",
    "        product_dict['reviews'] = reviews_count.strip()\n",
    "\n",
    "    return product_dict\n",
    "\n",
    "def scrape_amazon(categories, max_pages=2):\n",
    "    driver = setup_driver()\n",
    "    all_products = []\n",
    "    seen_products = set()\n",
    "\n",
    "    for category, base_url in categories.items():\n",
    "        products = []\n",
    "        for page in range(1, max_pages + 1):\n",
    "            url = f\"{base_url}&page={page}\"\n",
    "            driver.get(url)\n",
    "            time.sleep(5) # wait for page to load\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            next_page_button = soup.find('li', class_='a-last')\n",
    "            if next_page_button and next_page_button.find('a') is None:\n",
    "                break\n",
    "\n",
    "            for product in soup.find_all('div', class_='sg-col-inner'):\n",
    "                product_dict = extract_product_details(product, category, seen_products)\n",
    "                if product_dict:\n",
    "                    products.append(product_dict)\n",
    "\n",
    "\n",
    "        all_products.extend(products)\n",
    "\n",
    "    driver.quit()\n",
    "    return json.dumps(all_products)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  categories = {\n",
    "\n",
    "    'Smartphones': 'https://www.amazon.com/s?k=smartphone&crid=3L33Q517U80C6&sprefix=smartphone%2Caps%2C934&ref=nb_sb_noss_1',\n",
    "    'Laptops': 'https://www.amazon.com/s?k=Laptop&crid=3GD9HX5GPNA0R&sprefix=laptop%2Caps%2C569&ref=nb_sb_noss_1',\n",
    "    'video_games': 'https://www.amazon.com/s?rh=n%3A16225016011&fs=true&ref=lp_16225016011_sar',\n",
    "    'Dresses':'https://www.amazon.com/s?k=dress&crid=1ZKY6X4I4VARF&sprefix=dress%2Caps%2C1200&ref=nb_sb_noss_1',\n",
    "    'Shoes':'https://www.amazon.com/s?k=shoes&crid=BWUHHU3UQ38H&sprefix=shoes%2Caps%2C346&ref=nb_sb_noss_1',\n",
    "    'Accessories':'https://www.amazon.com/s?k=accessories+for+clothes&crid=17CJNG61JS7OX&sprefix=accessories+for+clo%2Caps%2C750&ref=nb_sb_ss_ts-doa-p_3_19', # Accessories for Clothes\n",
    "\n",
    "    # Add more categories as needed\n",
    "  }\n",
    "\n",
    "    # Load the JSON output string into a Python List of dictionaries for further processing\n",
    "  amazon_data = json.loads(scrape_amazon(categories))\n",
    "\n",
    "  # Save the JSON data to a file\n",
    "  with open('amazon_data_cat.json', 'w') as file:\n",
    "    json.dump(amazon_data, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "Could not reach host. Are you offline?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\urllib3\\connection.py:203\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     sock \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[0;32m    204\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport),\n\u001b[0;32m    205\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout,\n\u001b[0;32m    206\u001b[0m         source_address\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msource_address,\n\u001b[0;32m    207\u001b[0m         socket_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msocket_options,\n\u001b[0;32m    208\u001b[0m     )\n\u001b[0;32m    209\u001b[0m \u001b[39mexcept\u001b[39;00m socket\u001b[39m.\u001b[39mgaierror \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\urllib3\\util\\connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 85\u001b[0m     \u001b[39mraise\u001b[39;00m err\n\u001b[0;32m     86\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m     \u001b[39m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\urllib3\\util\\connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     72\u001b[0m     sock\u001b[39m.\u001b[39mbind(source_address)\n\u001b[1;32m---> 73\u001b[0m sock\u001b[39m.\u001b[39;49mconnect(sa)\n\u001b[0;32m     74\u001b[0m \u001b[39m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[1;31mTimeoutError\u001b[0m: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\urllib3\\connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    789\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 790\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_request(\n\u001b[0;32m    791\u001b[0m     conn,\n\u001b[0;32m    792\u001b[0m     method,\n\u001b[0;32m    793\u001b[0m     url,\n\u001b[0;32m    794\u001b[0m     timeout\u001b[39m=\u001b[39mtimeout_obj,\n\u001b[0;32m    795\u001b[0m     body\u001b[39m=\u001b[39mbody,\n\u001b[0;32m    796\u001b[0m     headers\u001b[39m=\u001b[39mheaders,\n\u001b[0;32m    797\u001b[0m     chunked\u001b[39m=\u001b[39mchunked,\n\u001b[0;32m    798\u001b[0m     retries\u001b[39m=\u001b[39mretries,\n\u001b[0;32m    799\u001b[0m     response_conn\u001b[39m=\u001b[39mresponse_conn,\n\u001b[0;32m    800\u001b[0m     preload_content\u001b[39m=\u001b[39mpreload_content,\n\u001b[0;32m    801\u001b[0m     decode_content\u001b[39m=\u001b[39mdecode_content,\n\u001b[0;32m    802\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mresponse_kw,\n\u001b[0;32m    803\u001b[0m )\n\u001b[0;32m    805\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\urllib3\\connectionpool.py:491\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    490\u001b[0m         new_e \u001b[39m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[39m.\u001b[39mproxy\u001b[39m.\u001b[39mscheme)\n\u001b[1;32m--> 491\u001b[0m     \u001b[39mraise\u001b[39;00m new_e\n\u001b[0;32m    493\u001b[0m \u001b[39m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[39m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\urllib3\\connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 467\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[0;32m    468\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\urllib3\\connectionpool.py:1092\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m conn\u001b[39m.\u001b[39mis_closed:\n\u001b[1;32m-> 1092\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[0;32m   1094\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\urllib3\\connection.py:611\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    610\u001b[0m sock: socket\u001b[39m.\u001b[39msocket \u001b[39m|\u001b[39m ssl\u001b[39m.\u001b[39mSSLSocket\n\u001b[1;32m--> 611\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m sock \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[0;32m    612\u001b[0m server_hostname: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\urllib3\\connection.py:218\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 218\u001b[0m     \u001b[39mraise\u001b[39;00m NewConnectionError(\n\u001b[0;32m    219\u001b[0m         \u001b[39mself\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to establish a new connection: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    220\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[39m# Audit hooks are only available in Python 3.8+\u001b[39;00m\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x000001AD3C68DA30>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\urllib3\\connectionpool.py:844\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    842\u001b[0m     new_e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, new_e)\n\u001b[1;32m--> 844\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[0;32m    845\u001b[0m     method, url, error\u001b[39m=\u001b[39;49mnew_e, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[0;32m    846\u001b[0m )\n\u001b[0;32m    847\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\urllib3\\util\\retry.py:515\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    514\u001b[0m     reason \u001b[39m=\u001b[39m error \u001b[39mor\u001b[39;00m ResponseError(cause)\n\u001b[1;32m--> 515\u001b[0m     \u001b[39mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[39mfrom\u001b[39;00m \u001b[39mreason\u001b[39;00m  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    517\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mIncremented Retry for (url=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m): \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='edgedl.me.gvt1.com', port=443): Max retries exceeded with url: /edgedl/chrome/chrome-for-testing/117.0.5938.88/win32/chromedriver-win32.zip (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001AD3C68DA30>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\webdriver_manager\\core\\http.py:32\u001b[0m, in \u001b[0;36mWDMHttpClient.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 32\u001b[0m     resp \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(\n\u001b[0;32m     33\u001b[0m         url\u001b[39m=\u001b[39murl, verify\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ssl_verify, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mConnectionError:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[39m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39m:rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39m\u001b[39mget\u001b[39m\u001b[39m\"\u001b[39m, url, params\u001b[39m=\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39msend(request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\requests\\adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    517\u001b[0m         \u001b[39mraise\u001b[39;00m SSLError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m--> 519\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[0;32m    521\u001b[0m \u001b[39mexcept\u001b[39;00m ClosedPoolError \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='edgedl.me.gvt1.com', port=443): Max retries exceeded with url: /edgedl/chrome/chrome-for-testing/117.0.5938.88/win32/chromedriver-win32.zip (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001AD3C68DA30>: Failed to establish a new connection: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Kasim\\OneDrive\\Desktop\\ebejale_Desk_Folder\\CICD\\9.  future tasks.cloud computing, capstone, and IT projects\\Internship Project\\Amazon_reviews_edge_cat.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Kasim/OneDrive/Desktop/ebejale_Desk_Folder/CICD/9.%20%20future%20tasks.cloud%20computing%2C%20capstone%2C%20and%20IT%20projects/Internship%20Project/Amazon_reviews_edge_cat.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mselenium\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwebdriver\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchrome\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptions\u001b[39;00m \u001b[39mimport\u001b[39;00m Options\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Kasim/OneDrive/Desktop/ebejale_Desk_Folder/CICD/9.%20%20future%20tasks.cloud%20computing%2C%20capstone%2C%20and%20IT%20projects/Internship%20Project/Amazon_reviews_edge_cat.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwebdriver_manager\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchrome\u001b[39;00m \u001b[39mimport\u001b[39;00m ChromeDriverManager\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Kasim/OneDrive/Desktop/ebejale_Desk_Folder/CICD/9.%20%20future%20tasks.cloud%20computing%2C%20capstone%2C%20and%20IT%20projects/Internship%20Project/Amazon_reviews_edge_cat.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m driver \u001b[39m=\u001b[39m webdriver\u001b[39m.\u001b[39mChrome(service\u001b[39m=\u001b[39mService(ChromeDriverManager()\u001b[39m.\u001b[39;49minstall()))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kasim/OneDrive/Desktop/ebejale_Desk_Folder/CICD/9.%20%20future%20tasks.cloud%20computing%2C%20capstone%2C%20and%20IT%20projects/Internship%20Project/Amazon_reviews_edge_cat.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m options \u001b[39m=\u001b[39m Options()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kasim/OneDrive/Desktop/ebejale_Desk_Folder/CICD/9.%20%20future%20tasks.cloud%20computing%2C%20capstone%2C%20and%20IT%20projects/Internship%20Project/Amazon_reviews_edge_cat.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m options\u001b[39m.\u001b[39mheadless \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\webdriver_manager\\chrome.py:40\u001b[0m, in \u001b[0;36mChromeDriverManager.install\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minstall\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m---> 40\u001b[0m     driver_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_driver_binary_path(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdriver)\n\u001b[0;32m     41\u001b[0m     os\u001b[39m.\u001b[39mchmod(driver_path, \u001b[39m0o755\u001b[39m)\n\u001b[0;32m     42\u001b[0m     \u001b[39mreturn\u001b[39;00m driver_path\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\webdriver_manager\\core\\manager.py:40\u001b[0m, in \u001b[0;36mDriverManager._get_driver_binary_path\u001b[1;34m(self, driver)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m binary_path\n\u001b[0;32m     39\u001b[0m os_type \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_os_type()\n\u001b[1;32m---> 40\u001b[0m file \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_download_manager\u001b[39m.\u001b[39;49mdownload_file(driver\u001b[39m.\u001b[39;49mget_driver_download_url(os_type))\n\u001b[0;32m     41\u001b[0m binary_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache_manager\u001b[39m.\u001b[39msave_file_to_cache(driver, file)\n\u001b[0;32m     42\u001b[0m \u001b[39mreturn\u001b[39;00m binary_path\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\webdriver_manager\\core\\download_manager.py:29\u001b[0m, in \u001b[0;36mWDMDownloadManager.download_file\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdownload_file\u001b[39m(\u001b[39mself\u001b[39m, url: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m File:\n\u001b[0;32m     28\u001b[0m     log(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAbout to download new driver from \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 29\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_http_client\u001b[39m.\u001b[39;49mget(url)\n\u001b[0;32m     30\u001b[0m     log(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDriver downloading response is \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m     file_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextract_filename_from_url(url)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\webdriver_manager\\core\\http.py:35\u001b[0m, in \u001b[0;36mWDMHttpClient.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m     resp \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(\n\u001b[0;32m     33\u001b[0m         url\u001b[39m=\u001b[39murl, verify\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ssl_verify, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mConnectionError:\n\u001b[1;32m---> 35\u001b[0m     \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mConnectionError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not reach host. Are you offline?\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidate_response(resp)\n\u001b[0;32m     37\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mConnectionError\u001b[0m: Could not reach host. Are you offline?"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver \n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "options = Options()\n",
    "options.headless = True\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "driver.set_page_load_timeout(30) \n",
    "driver.implicitly_wait(20)\n",
    "\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "def scrape_amazon(categories):\n",
    "\n",
    "    options = webdriver.ChromeOptions()\n",
    "\n",
    "    # Run Chrome in headless mode\n",
    "    options.add_argument('--headless')\n",
    "\n",
    "    # Set a higher timeout value\n",
    "    options.add_argument('--timeout=10')\n",
    "\n",
    "    # Install latest ChromeDriver and get its path\n",
    "    webdriver_path = ChromeDriverManager().install()\n",
    "\n",
    "    # Use the installed webdriver_path\n",
    "    driver = webdriver.Chrome(service=Service(webdriver_path), options=options)\n",
    "\n",
    "    all_products = []\n",
    "\n",
    "    for category, base_url in categories.items():\n",
    "        products = []\n",
    "        \n",
    "        # Iterate through pages for this category\n",
    "        for page in range(1, 2):\n",
    "            \n",
    "            url = f\"{base_url}&page={page}\"\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                time.sleep(5) # wait for page to load\n",
    "            except TimeoutException:\n",
    "                print(f\"Timeout occurred while loading {url}. Skipping this page.\")\n",
    "                continue\n",
    "\n",
    "            # Extract product details\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            for product in soup.find_all('div', class_='sg-col-inner'): \n",
    "                product_dict = {}\n",
    "\n",
    "                # Item name\n",
    "                item_name = product.find('span', class_='a-size-base-plus a-color-base a-text-normal')\n",
    "                item_price = product.find('span', class_='a-offscreen')\n",
    "\n",
    "                # If item name is None, then it is a mouse pad\n",
    "                if item_name is None:\n",
    "                    item_name = product.find('span', class_='a-size-medium a-color-base a-text-normal')\n",
    "                    product_dict['product'] = item_name.text.strip()\n",
    "\n",
    "                # If item price is None, then it is not a mouse pad\n",
    "                if item_price is not None:\n",
    "                    product_price = item_price.text.strip()\n",
    "                    product_dict['price'] = product_price.replace(\"$\", \"\").replace(\",\", \"\").strip()\n",
    "\n",
    "                rating = product.find('span', class_='a-icon-alt')\n",
    "                if rating is not None:\n",
    "                    product_dict['ratings'] = rating.text.strip().split(\" \")[0]\n",
    "\n",
    "                review_responder_population = product.find('span', class_='a-size-base s-underline-text')\n",
    "                \n",
    "                if review_responder_population is not None:\n",
    "                    review_responders = review_responder_population.text.strip().split(\" \")[0]\n",
    "                else:\n",
    "                    review_responders = None\n",
    "\n",
    "\n",
    "                try:\n",
    "                    review_responder_population = product.find('span', class_='a-size-base s-underline-text')\n",
    "                    review_responders = review_responder_population.text.strip().split(\" \")[0]\n",
    "                except:\n",
    "                    review_responders = None\n",
    "\n",
    "                product_dict['review_responders'] = review_responders\n",
    "\n",
    "\n",
    "                item_reviews = product.find('span', class_='a-size-base')\n",
    "                if item_reviews is not None:\n",
    "                    reviews_text = item_reviews.text.strip()\n",
    "                    reviews_count = reviews_text.split(\" \")[0]\n",
    "                    product_dict['reviews'] = reviews_count.strip()\n",
    "\n",
    "                # Add category to product_dict\n",
    "                product_dict['category'] = category\n",
    "\n",
    "                # Only append product_dict if it has some data\n",
    "                if product_dict:\n",
    "                    products.append(product_dict)\n",
    "\n",
    "        all_products.extend(products)\n",
    "\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "\n",
    "    # Output the result as JSON\n",
    "    return json.dumps(all_products)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  # Define the categories and their base URLs\n",
    "\n",
    "  categories = {\n",
    "    'Smartphones': 'https://www.amazon.com/s?k=smartphone&crid=3L33Q517U80C6&sprefix=smartphone%2Caps%2C934&ref=nb_sb_noss_1',\n",
    "    'Laptops': 'https://www.amazon.com/s?k=Laptop&crid=3GD9HX5GPNA0R&sprefix=laptop%2Caps%2C569&ref=nb_sb_noss_1',\n",
    "    'video_games': 'https://www.amazon.com/s?rh=n%3A16225016011&fs=true&ref=lp_16225016011_sar',\n",
    "    'Dresses':'https://www.amazon.com/s?k=dress&crid=1ZKY6X4I4VARF&sprefix=dress%2Caps%2C1200&ref=nb_sb_noss_1',\n",
    "    'Shoes':'https://www.amazon.com/s?k=shoes&crid=BWUHHU3UQ38H&sprefix=shoes%2Caps%2C346&ref=nb_sb_noss_1',\n",
    "    'Accessories':'https://www.amazon.com/s?k=accessories+for+clothes&crid=17CJNG61JS7OX&sprefix=accessories+for+clo%2Caps%2C750&ref=nb_sb_ss_ts-doa-p_3_19', # Accessories for Clothes\n",
    "\n",
    "    # Add more categories as needed\n",
    "  }\n",
    "\n",
    "  # Load the JSON output string into a Python List of dictionaries for further processing\n",
    "  amazon_data = json.loads(scrape_amazon(categories))\n",
    "\n",
    "  # Save the JSON data to a file\n",
    "  with open('amazon_data_cat.json', 'w') as file:\n",
    "    json.dump(amazon_data, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      category                                            product  price  \\\n",
      "0  Smartphones                                                NaN    NaN   \n",
      "1  Smartphones                                                NaN    NaN   \n",
      "2  Smartphones                                                NaN    NaN   \n",
      "3  Smartphones                                                NaN    NaN   \n",
      "4  Smartphones  Google Pixel 4a - Unlocked Android Smartphone ...  99.99   \n",
      "\n",
      "  ratings review_responders   reviews  \n",
      "0     NaN               NaN       NaN  \n",
      "1     NaN               NaN       NaN  \n",
      "2     NaN               NaN       NaN  \n",
      "3     NaN               NaN       NaN  \n",
      "4     4.4               163  Typical:  \n"
     ]
    }
   ],
   "source": [
    "# Load the JSON data into a pandas DataFrame\n",
    "with open('amazon_data_cat.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Now, print out a few lines of the DataFrame to check if 'product' key is present\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "\n",
    "def scrape_amazon(categories):\n",
    "    # Setup driver\n",
    "    options = webdriver.EdgeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    driver = webdriver.Edge(service=Service(EdgeChromiumDriverManager().install()), options=options)\n",
    "\n",
    "    # Extract product details from each category\n",
    "    all_products = []\n",
    "    seen_products = set()  # Keep track of products we've already seen\n",
    "\n",
    "    for category, base_url in categories.items():\n",
    "        products = []\n",
    "\n",
    "        # Iterate through pages for this category\n",
    "        for page in range(1, 5):\n",
    "\n",
    "            url = f\"{base_url}&page={page}\"\n",
    "            driver.get(url)\n",
    "            time.sleep(5) # wait for page to load\n",
    "\n",
    "            # Extract product details\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            for product in soup.find_all('div', class_='sg-col-inner'):\n",
    "                product_dict = {}\n",
    "\n",
    "                # Item name\n",
    "                item_name = product.find('span', class_='a-size-base-plus a-color-base a-text-normal')\n",
    "\n",
    "                # If item name is None, then it is a mouse pad\n",
    "                if item_name is None:\n",
    "                    item_name = product.find('span', class_='a-size-medium a-color-base a-text-normal')\n",
    "\n",
    "                if item_name is not None:\n",
    "                    product_name = item_name.text.strip()\n",
    "\n",
    "                    # Skip this product if we've already seen it\n",
    "                    if product_name in seen_products:\n",
    "                        continue\n",
    "\n",
    "                    seen_products.add(product_name)\n",
    "                    product_dict['product'] = product_name\n",
    "\n",
    "                # Item price\n",
    "                product_price = product.find('span', class_='a-offscreen')\n",
    "                rating = product.find('span', class_='a-icon-alt')\n",
    "                review_responder_population = product.find('span', class_='a-size-base s-underline-text')\n",
    "                item_reviews = product.find('span', class_='a-size-base')\n",
    "\n",
    "                # Check if the product_price variable is None before adding it to the product_dict\n",
    "                if product_price is not None:\n",
    "                    product_price = product_price.text.strip()\n",
    "                    product_dict['price'] = product_price.replace(\"$\", \"\").replace(\",\", \"\").strip()\n",
    "\n",
    "                # Check if the rating variable is None before adding it to the product_dict\n",
    "                if rating is not None:\n",
    "                    product_dict['ratings'] = rating.text.strip().split(\" \")[0]\n",
    "\n",
    "                # Check if the review_responder_population variable is None before adding it to the product_dict\n",
    "                if review_responder_population is not None:\n",
    "                    review_responders = review_responder_population.text.strip().split(\" \")[0]\n",
    "                    product_dict['review_responders'] = review_responders\n",
    "\n",
    "                # Check if the item_reviews variable is None before adding it to the product_dict\n",
    "                if item_reviews is not None:\n",
    "                    reviews_text = item_reviews.text.strip()\n",
    "                    reviews_count = reviews_text.split(\" \")[0]\n",
    "                    product_dict['reviews'] = reviews_count.strip()\n",
    "\n",
    "                # Add category to product_dict\n",
    "                product_dict['category'] = category\n",
    "\n",
    "                # Only append product_dict if it has some data\n",
    "                if len(product_dict.keys()) > 0:\n",
    "                    products.append(product_dict)\n",
    "\n",
    "        all_products.extend(products)\n",
    "\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "\n",
    "    # Output the result as JSON\n",
    "    return json.dumps(all_products)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  categories = {\n",
    "    'Smartphones': 'https://www.amazon.com/s?k=smartphone&crid=3L33Q517U80C6&sprefix=smartphone%2Caps%2C934&ref=nb_sb_noss_1',\n",
    "    'Laptops': 'https://www.amazon.com/s?k=Laptop&crid=3GD9HX5GPNA0R&sprefix=laptop%2Caps%2C569&ref=nb_sb_noss_1',\n",
    "    'video_games': 'https://www.amazon.com/s?rh=n%3A16225016011&fs=true&ref=lp_16225016011_sar',\n",
    "    'Dresses':'https://www.amazon.com/s?k=dress&crid=1ZKY6X4I4VARF&sprefix=dress%2Caps%2C1200&ref=nb_sb_noss_1',\n",
    "    'Shoes':'https://www.amazon.com/s?k=shoes&crid=BWUHHU3UQ38H&sprefix=shoes%2Caps%2C346&ref=nb_sb_noss_1',\n",
    "    'Accessories':'https://www.amazon.com/s?k=accessories+for+clothes&crid=17CJNG61JS7OX&sprefix=accessories+for+clo%2Caps%2C750&ref=nb_sb_ss_ts-doa-p_3_19', # Accessories for Clothes\n",
    "    # Add more categories as needed\n",
    "  }\n",
    "\n",
    "  amazon_data = json.loads(scrape_amazon(categories))\n",
    "\n",
    "  # Save the JSON data to a file\n",
    "  with open('amazon_data_cat.json', 'w') as file:\n",
    "   json.dump(amazon_data, file)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "\n",
    "def scrape_amazon(categories):\n",
    "    # Setup driver\n",
    "    options = webdriver.EdgeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    driver = webdriver.Edge(service=Service(EdgeChromiumDriverManager().install()), options=options)\n",
    "\n",
    "    # Extract product details from each category\n",
    "    all_products = []\n",
    "\n",
    "    for category, base_url in categories.items():\n",
    "        products = []\n",
    "\n",
    "        # Iterate through pages for this category\n",
    "        for page in range(1, 5):\n",
    "\n",
    "            url = f\"{base_url}&page={page}\"\n",
    "            driver.get(url)\n",
    "            time.sleep(5) # wait for page to load\n",
    "\n",
    "            # Extract product details\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            for product in soup.find_all('div', class_='sg-col-inner'):\n",
    "                product_dict = {}\n",
    "\n",
    "                # Item name\n",
    "                item_name = product.find('span', class_='a-size-base-plus a-color-base a-text-normal')\n",
    "\n",
    "                # If item name is None, then it is a mouse pad\n",
    "                if item_name is None:\n",
    "                    item_name = product.find('span', class_='a-size-medium a-color-base a-text-normal')\n",
    "\n",
    "                if item_name is not None:\n",
    "                    product_dict['product'] = item_name.text.strip()\n",
    "\n",
    "                # Item price\n",
    "                product_price = product.find('span', class_='a-offscreen')\n",
    "                rating = product.find('span', class_='a-icon-alt')\n",
    "\n",
    "                \n",
    "                item_reviews = product.find('span', class_='a-size-base')\n",
    "\n",
    "                # Check if the product_price variable is None before adding it to the product_dict\n",
    "                if product_price is not None:\n",
    "                    product_price = product_price.text.strip()\n",
    "                    product_dict['price'] = product_price.replace(\"$\", \"\").replace(\",\", \"\").strip()\n",
    "\n",
    "                # Check if the rating variable is None before adding it to the product_dict\n",
    "                if rating is not None:\n",
    "                    product_dict['ratings'] = rating.text.strip().split(\" \")[0]\n",
    "\n",
    "                # Check if the review_responder_population variable is None before adding it to the product_dict\n",
    "                review_responder_population = product.find('span', class_='a-size-base s-underline-text')\n",
    "                if review_responder_population is not None:\n",
    "                    review_responders = review_responder_population.text.strip().split(\" \")[0]\n",
    "                    product_dict['review_responders'] = review_responders\n",
    "\n",
    "                # Extract number of ratings\n",
    "                num_ratings = product.find('span', 'a-size-base')\n",
    "                if num_ratings is not None:\n",
    "                    product_dict['num_ratings'] = num_ratings['aria-label'].split()[0]\n",
    "\n",
    "                # If item name is None, then it is a mouse pad\n",
    "                # Find review_responders\n",
    "                review_responder_population_1 = product.find('span', class_='a-size-base s-underline-text')\n",
    "                review_responder_population_2 = product.find('span', 'a-size-base')\n",
    "\n",
    "                if review_responder_population_1 is not None:\n",
    "                    review_responders1 = review_responder_population_1.text.strip().split(\" \")[0]\n",
    "                    if review_responders1 > 0:\n",
    "                     product_dict['review_responders'] = review_responders1\n",
    "                    else:\n",
    "                        review_responders2 = review_responder_population_2['aria-label'].split()[0]\n",
    "                        product_dict['review_responders'] = review_responders2\n",
    "                    \n",
    "    \n",
    "                # Check if the item_reviews variable is None before adding it to the product_dict\n",
    "                if item_reviews is not None:\n",
    "                    reviews_text = item_reviews.text.strip()\n",
    "                    reviews_count = reviews_text.split(\" \")[0]\n",
    "                    product_dict['reviews'] = reviews_count.strip()\n",
    "\n",
    "                # Add category to product_dict\n",
    "                product_dict['category'] = category\n",
    "\n",
    "                # Only append product_dict if it has some data\n",
    "                if len(product_dict.keys()) > 0:\n",
    "                    products.append(product_dict)\n",
    "\n",
    "        all_products.extend(products)\n",
    "\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "\n",
    "    # Output the result as JSON\n",
    "    return json.dumps(all_products)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  categories = {\n",
    "    'Smartphones': 'https://www.amazon.com/s?k=smartphone&crid=3L33Q517U80C6&sprefix=smartphone%2Caps%2C934&ref=nb_sb_noss_1',\n",
    "    'Laptops': 'https://www.amazon.com/s?k=Laptop&crid=3GD9HX5GPNA0R&sprefix=laptop%2Caps%2C569&ref=nb_sb_noss_1',\n",
    "    'video_games': 'https://www.amazon.com/s?rh=n%3A16225016011&fs=true&ref=lp_16225016011_sar',\n",
    "    'Dresses':'https://www.amazon.com/s?k=dress&crid=1ZKY6X4I4VARF&sprefix=dress%2Caps%2C1200&ref=nb_sb_noss_1',\n",
    "    'Shoes':'https://www.amazon.com/s?k=shoes&crid=BWUHHU3UQ38H&sprefix=shoes%2Caps%2C346&ref=nb_sb_noss_1',\n",
    "    'Accessories':'https://www.amazon.com/s?k=accessories+for+clothes&crid=17CJNG61JS7OX&sprefix=accessories+for+clo%2Caps%2C750&ref=nb_sb_ss_ts-doa-p_3_19', # Accessories for Clothes\n",
    "\n",
    "  }\n",
    "\n",
    "  amazon_data = json.loads(scrape_amazon(categories))\n",
    "\n",
    "  # Save the JSON data to a file\n",
    "  with open('amazon_data_cat.json', 'w') as file:\n",
    "   json.dump(amazon_data, file)\n",
    "# BAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "\n",
    "def scrape_amazon(categories):\n",
    "    # Setup driver\n",
    "    options = webdriver.EdgeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    driver = webdriver.Edge(service=Service(EdgeChromiumDriverManager().install()), options=options)\n",
    "\n",
    "    # Extract product details from each category\n",
    "    all_products = []\n",
    "\n",
    "    for category, base_url in categories.items():\n",
    "        products = []\n",
    "\n",
    "        # Iterate through pages for this category\n",
    "        for page in range(1, 5):\n",
    "\n",
    "            url = f\"{base_url}&page={page}\"\n",
    "            driver.get(url)\n",
    "            time.sleep(5) # wait for page to load\n",
    "\n",
    "            # Extract product details\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            for product in soup.find_all('div', class_='sg-col-inner'):\n",
    "                product_dict = {}\n",
    "\n",
    "                # Item name\n",
    "                item_name = product.find('span', class_='a-size-base-plus a-color-base a-text-normal')\n",
    "\n",
    "                # If item name is None, then it is a mouse pad\n",
    "                if item_name is None:\n",
    "                    item_name = product.find('span', class_='a-size-medium a-color-base a-text-normal')\n",
    "\n",
    "                if item_name is not None:\n",
    "                    product_dict['product'] = item_name.text.strip()\n",
    "\n",
    "                # Item price\n",
    "                product_price = product.find('span', class_='a-offscreen')\n",
    "                rating = product.find('span', class_='a-icon-alt')\n",
    "                review_responder_population = product.find('span', class_='a-size-base s-underline-text')\n",
    "                item_reviews = product.find('span', class_='a-size-base')\n",
    "\n",
    "                # Check if the product_price variable is None before adding it to the product_dict\n",
    "                if product_price is not None:\n",
    "                    product_price = product_price.text.strip()\n",
    "                    product_dict['price'] = product_price.replace(\"$\", \"\").replace(\",\", \"\").strip()\n",
    "\n",
    "                # Check if the rating variable is None before adding it to the product_dict\n",
    "                if rating is not None:\n",
    "                    product_dict['ratings'] = rating.text.strip().split(\" \")[0]\n",
    "\n",
    "                # Check if the review_responder_population variable is None before adding it to the product_dict\n",
    "                if review_responder_population is not None:\n",
    "                    review_responders = review_responder_population.text.strip().split(\" \")[0]\n",
    "                    product_dict['review_responders'] = review_responders\n",
    "\n",
    "                # Check if the item_reviews variable is None before adding it to the product_dict\n",
    "                if item_reviews is not None:\n",
    "                    reviews_text = item_reviews.text.strip()\n",
    "                    reviews_count = reviews_text.split(\" \")[0]\n",
    "                    product_dict['reviews'] = reviews_count.strip()\n",
    "\n",
    "                # Add category to product_dict\n",
    "                product_dict['category'] = category\n",
    "\n",
    "                # Only append product_dict if it has some data\n",
    "                if len(product_dict.keys()) > 0:\n",
    "                    products.append(product_dict)\n",
    "\n",
    "        all_products.extend(products)\n",
    "\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "\n",
    "    # Output the result as JSON\n",
    "    return json.dumps(all_products)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  categories = {\n",
    "    'Smartphones': 'https://www.amazon.com/s?k=smartphone&crid=3L33Q517U80C6&sprefix=smartphone%2Caps%2C934&ref=nb_sb_noss_1',\n",
    "    'Laptops': 'https://www.amazon.com/s?k=Laptop&crid=3GD9HX5GPNA0R&sprefix=laptop%2Caps%2C569&ref=nb_sb_noss_1',\n",
    "    'video_games': 'https://www.amazon.com/s?rh=n%3A16225016011&fs=true&ref=lp_16225016011_sar',\n",
    "    'Dresses':'https://www.amazon.com/s?k=dress&crid=1ZKY6X4I4VARF&sprefix=dress%2Caps%2C1200&ref=nb_sb_noss_1',\n",
    "    'Shoes':'https://www.amazon.com/s?k=shoes&crid=BWUHHU3UQ38H&sprefix=shoes%2Caps%2C346&ref=nb_sb_noss_1',\n",
    "    'Accessories':'https://www.amazon.com/s?k=accessories+for+clothes&crid=17CJNG61JS7OX&sprefix=accessories+for+clo%2Caps%2C750&ref=nb_sb_ss_ts-doa-p_3_19', # Accessories for Clothes\n",
    "\n",
    "  }\n",
    "\n",
    "  amazon_data = json.loads(scrape_amazon(categories))\n",
    "\n",
    "  # Save the JSON data to a file\n",
    "  with open('amazon_data_cat.json', 'w') as file:\n",
    "   json.dump(amazon_data, file)\n",
    "# GOOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "\n",
    "def scrape_amazon(categories):\n",
    "    # Setup driver\n",
    "    options = webdriver.EdgeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    driver = webdriver.Edge(service=Service(EdgeChromiumDriverManager().install()), options=options)\n",
    "\n",
    "    # Extract product details from each category\n",
    "    all_products = []\n",
    "\n",
    "    for category, base_url in categories.items():\n",
    "        products = []\n",
    "\n",
    "        # Iterate through pages for this category\n",
    "        for page in range(1, 5):\n",
    "\n",
    "            url = f\"{base_url}&page={page}\"\n",
    "            driver.get(url)\n",
    "            time.sleep(5) # wait for page to load\n",
    "\n",
    "            # Extract product details\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            for product in soup.find_all('div', class_='sg-col-inner'):\n",
    "                product_dict = {}\n",
    "\n",
    "                # Item name\n",
    "                item_name = product.find('span', class_='a-size-base-plus a-color-base a-text-normal')\n",
    "\n",
    "                # If item name is None, then it is a mouse pad\n",
    "                if item_name is None:\n",
    "                    item_name = product.find('span', class_='a-size-medium a-color-base a-text-normal')\n",
    "\n",
    "                if item_name is not None:\n",
    "                    product_dict['product'] = item_name.text.strip()\n",
    "\n",
    "                # Item price\n",
    "                product_price = product.find('span', class_='a-offscreen')\n",
    "                rating = product.find('span', class_='a-icon-alt')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # **Modified:** Get the review responder population from the `aria-label` attribute of the span element with class `a-size-base s-underline-text`\n",
    "                review_responder_population_element = product.find('span', class_='a-size-base s-underline-text')\n",
    "                if review_responder_population_element is not None and 'aria-label' in review_responder_population_element.attrs:\n",
    "                    review_responder_population = review_responder_population_element['aria-label']\n",
    "                else:\n",
    "                    review_responder_population = None\n",
    "\n",
    "                item_reviews = product.find('span', class_='a-size-base')\n",
    "                \n",
    "                # Check if the product_price variable is None before adding it to the product_dict\n",
    "                if product_price is not None:\n",
    "                    product_price = product_price.text.strip()\n",
    "                    product_dict['price'] = product_price.replace(\"$\", \"\").replace(\",\", \"\").strip()\n",
    "\n",
    "                # Check if the rating variable is None before adding it to the product_dict\n",
    "                if rating is not None:\n",
    "                    product_dict['ratings'] = rating.text.strip().split(\" \")[0]\n",
    "\n",
    "                # **Modified:** Check if the review_responder_population variable is None before adding it to the product_dict\n",
    "                if review_responder_population is not None:\n",
    "                    product_dict['review_responders'] = review_responder_population\n",
    "\n",
    "                # Check if the item_reviews variable is None before adding it to the product_dict\n",
    "                if item_reviews is not None:\n",
    "                    reviews_text = item_reviews.text.strip()\n",
    "                    reviews_count = reviews_text.split(\" \")[0]\n",
    "                    product_dict['reviews'] = reviews_count.strip()\n",
    "\n",
    "                # Add category to product_dict\n",
    "                product_dict['category'] = category\n",
    "\n",
    "                # Only append product_dict if it has some data\n",
    "                if len(product_dict.keys()) > 0:\n",
    "                    products.append(product_dict)\n",
    "\n",
    "        all_products.extend(products)\n",
    "\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "\n",
    "    # Output the result as JSON\n",
    "    return json.dumps(all_products)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  categories = {\n",
    "    'Smartphones': 'https://www.amazon.com/s?k=smartphone&crid=3L33Q517U80C6&sprefix=smartphone%2Caps%2C934&ref=nb_sb_noss_1',\n",
    "    'Laptops': 'https://www.amazon.com/s?k=Laptop&crid=3GD9HX5GPNA0R&sprefix=laptop%2Caps%2C569&ref=nb_sb_noss_1',\n",
    "    'video_games': 'https://www.amazon.com/s?rh=n%3A16225016011&fs=true&ref=lp_16225016011_sar',\n",
    "    'Dresses':'https://www.amazon.com/s?k=dress&crid=1ZKY6X4I4VARF&sprefix=dress%2Caps%2C1200&ref=nb_sb_noss_1',\n",
    "    'Shoes':'https://www.amazon.com/s?k=shoes&crid=BWUHHU3UQ38H&sprefix=shoes%2Caps%2C346&ref=nb_sb_noss_1',\n",
    "    'Accessories':'https://www.amazon.com/s?k=accessories+for+clothes&crid=17CJNG61JS7OX&sprefix=accessories+for+clo%2Caps%2C750&ref=nb_sb_ss_ts-doa-p_3_19', # Accessories for Clothes\n",
    "\n",
    "  }\n",
    "\n",
    "  amazon_data = json.loads(scrape_amazon(categories))\n",
    "\n",
    "  # Save the JSON data to a file\n",
    "  with open('amazon_data_cat.json', 'w') as file:\n",
    "   json.dump(amazon_data, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ORIGINAL DB QUERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'reviews'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\indexes\\base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3789\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3790\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3791\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'reviews'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Kasim\\OneDrive\\Desktop\\ebejale_Desk_Folder\\CICD\\9.  future tasks.cloud computing, capstone, and IT projects\\Internship Project\\Amazon_reviews_edge_cat.ipynb Cell 13\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kasim/OneDrive/Desktop/ebejale_Desk_Folder/CICD/9.%20%20future%20tasks.cloud%20computing%2C%20capstone%2C%20and%20IT%20projects/Internship%20Project/Amazon_reviews_edge_cat.ipynb#W1sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39m# Insert the data from the pandas DataFrame into the PostgreSQL table\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kasim/OneDrive/Desktop/ebejale_Desk_Folder/CICD/9.%20%20future%20tasks.cloud%20computing%2C%20capstone%2C%20and%20IT%20projects/Internship%20Project/Amazon_reviews_edge_cat.ipynb#W1sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39mfor\u001b[39;00m index, row \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39miterrows():\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Kasim/OneDrive/Desktop/ebejale_Desk_Folder/CICD/9.%20%20future%20tasks.cloud%20computing%2C%20capstone%2C%20and%20IT%20projects/Internship%20Project/Amazon_reviews_edge_cat.ipynb#W1sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m     product, price, ratings, reviews, review_responders, category \u001b[39m=\u001b[39m clean_format_data(row)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kasim/OneDrive/Desktop/ebejale_Desk_Folder/CICD/9.%20%20future%20tasks.cloud%20computing%2C%20capstone%2C%20and%20IT%20projects/Internship%20Project/Amazon_reviews_edge_cat.ipynb#W1sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m     insert_query \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kasim/OneDrive/Desktop/ebejale_Desk_Folder/CICD/9.%20%20future%20tasks.cloud%20computing%2C%20capstone%2C%20and%20IT%20projects/Internship%20Project/Amazon_reviews_edge_cat.ipynb#W1sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39m    INSERT INTO amazon_data (product, price, ratings, reviews, review_responders, category) \u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kasim/OneDrive/Desktop/ebejale_Desk_Folder/CICD/9.%20%20future%20tasks.cloud%20computing%2C%20capstone%2C%20and%20IT%20projects/Internship%20Project/Amazon_reviews_edge_cat.ipynb#W1sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39m    VALUES (\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kasim/OneDrive/Desktop/ebejale_Desk_Folder/CICD/9.%20%20future%20tasks.cloud%20computing%2C%20capstone%2C%20and%20IT%20projects/Internship%20Project/Amazon_reviews_edge_cat.ipynb#W1sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kasim/OneDrive/Desktop/ebejale_Desk_Folder/CICD/9.%20%20future%20tasks.cloud%20computing%2C%20capstone%2C%20and%20IT%20projects/Internship%20Project/Amazon_reviews_edge_cat.ipynb#W1sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m     cur\u001b[39m.\u001b[39mexecute(insert_query, (product, price, ratings, reviews, review_responders, category))\n",
      "\u001b[1;32mc:\\Users\\Kasim\\OneDrive\\Desktop\\ebejale_Desk_Folder\\CICD\\9.  future tasks.cloud computing, capstone, and IT projects\\Internship Project\\Amazon_reviews_edge_cat.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kasim/OneDrive/Desktop/ebejale_Desk_Folder/CICD/9.%20%20future%20tasks.cloud%20computing%2C%20capstone%2C%20and%20IT%20projects/Internship%20Project/Amazon_reviews_edge_cat.ipynb#W1sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclean_format_data\u001b[39m(row):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kasim/OneDrive/Desktop/ebejale_Desk_Folder/CICD/9.%20%20future%20tasks.cloud%20computing%2C%20capstone%2C%20and%20IT%20projects/Internship%20Project/Amazon_reviews_edge_cat.ipynb#W1sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     \u001b[39m# Remove commas from 'reviews' and 'price' fields\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Kasim/OneDrive/Desktop/ebejale_Desk_Folder/CICD/9.%20%20future%20tasks.cloud%20computing%2C%20capstone%2C%20and%20IT%20projects/Internship%20Project/Amazon_reviews_edge_cat.ipynb#W1sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     reviews \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(row[\u001b[39m'\u001b[39;49m\u001b[39mreviews\u001b[39;49m\u001b[39m'\u001b[39;49m])\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kasim/OneDrive/Desktop/ebejale_Desk_Folder/CICD/9.%20%20future%20tasks.cloud%20computing%2C%20capstone%2C%20and%20IT%20projects/Internship%20Project/Amazon_reviews_edge_cat.ipynb#W1sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     review_responders \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(row[\u001b[39m'\u001b[39m\u001b[39mreview_responders\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Kasim/OneDrive/Desktop/ebejale_Desk_Folder/CICD/9.%20%20future%20tasks.cloud%20computing%2C%20capstone%2C%20and%20IT%20projects/Internship%20Project/Amazon_reviews_edge_cat.ipynb#W1sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     price \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(row[\u001b[39m'\u001b[39m\u001b[39mprice\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\series.py:1040\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1037\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[0;32m   1039\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1040\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[0;32m   1042\u001b[0m \u001b[39m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1043\u001b[0m \u001b[39m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1044\u001b[0m \u001b[39mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\series.py:1156\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[0;32m   1155\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1156\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[0;32m   1158\u001b[0m \u001b[39mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1159\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\indexes\\base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3792\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(casted_key, \u001b[39mslice\u001b[39m) \u001b[39mor\u001b[39;00m (\n\u001b[0;32m   3793\u001b[0m         \u001b[39misinstance\u001b[39m(casted_key, abc\u001b[39m.\u001b[39mIterable)\n\u001b[0;32m   3794\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39misinstance\u001b[39m(x, \u001b[39mslice\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m casted_key)\n\u001b[0;32m   3795\u001b[0m     ):\n\u001b[0;32m   3796\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3797\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3798\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3799\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3800\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'reviews'"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# Load the JSON data into a pandas DataFrame\n",
    "df = pd.read_json('amazon_data_cat.json')\n",
    "\n",
    "# Remove empty rows\n",
    "df = df.dropna()\n",
    "\n",
    "# Create a connection to your PostgreSQL database\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"demopass\"\n",
    ")\n",
    "\n",
    "# Create a cursor object\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Create a table in your PostgreSQL database to store the data\n",
    "create_table_query = \"\"\"\n",
    "DROP TABLE IF EXISTS amazon_date;\n",
    "CREATE TABLE IF NOT EXISTS amazon_data (\n",
    "    product TEXT NOT NULL,\n",
    "    price NUMERIC NOT NULL,\n",
    "    ratings NUMERIC NOT NULL,\n",
    "    reviews NUMERIC,\n",
    "    review_responders NUMERIC,\n",
    "    category TEXT NOT NULL\n",
    ")\n",
    "\"\"\"\n",
    "cur.execute(create_table_query)\n",
    "conn.commit()\n",
    "\n",
    "# Function to clean and format data\n",
    "def clean_format_data(row):\n",
    "    # Remove commas from 'reviews' and 'price' fields\n",
    "    reviews = str(row['reviews']).replace(',', '')\n",
    "    review_responders = str(row['review_responders']).replace(',', '')\n",
    "    price = str(row['price']).replace(',', '')\n",
    "\n",
    "    # Convert reviews to int, if not possible set to 0\n",
    "    try:\n",
    "        reviews = int(reviews)\n",
    "    except ValueError:\n",
    "        reviews = 0\n",
    "    \n",
    "    # Convert reviews to int, if not possible set to 0\n",
    "    try:\n",
    "        review_responders = int(review_responders)\n",
    "    except ValueError:\n",
    "        review_responders = 0\n",
    "\n",
    "    # Convert the ratings value to a float\n",
    "    ratings = float(row['ratings'])\n",
    "\n",
    "    # Adapt data to correct format for SQL insertion and remove quotes\n",
    "    product = psycopg2.extensions.adapt(row['product'].encode('utf-8')).getquoted().decode('utf-8')[1:-1]\n",
    "\n",
    "    # Convert price to float, if not possible set to 0\n",
    "    try:\n",
    "        price = float(price)\n",
    "    except ValueError:\n",
    "        price = 0\n",
    "\n",
    "    category = psycopg2.extensions.adapt(row['category']).getquoted().decode('utf-8')[1:-1]\n",
    "\n",
    "    return product, price, ratings, reviews, review_responders, category\n",
    "\n",
    "# Insert the data from the pandas DataFrame into the PostgreSQL table\n",
    "for index, row in df.iterrows():\n",
    "    product, price, ratings, reviews, review_responders, category = clean_format_data(row)\n",
    "\n",
    "    insert_query = \"\"\"\n",
    "    INSERT INTO amazon_data (product, price, ratings, reviews, review_responders, category) \n",
    "    VALUES (%s, %s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "\n",
    "    cur.execute(insert_query, (product, price, ratings, reviews, review_responders, category))\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOME EMPTY COL NO EMPTY ROWS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# Load the JSON data into a pandas DataFrame\n",
    "df = pd.read_json('amazon_data_cat.json')\n",
    "\n",
    "# Remove empty rows\n",
    "df = df.dropna()\n",
    "\n",
    "# Create a connection to your PostgreSQL database\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"demopass\"\n",
    ")\n",
    "\n",
    "# Create a cursor object\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Create a table in your PostgreSQL database to store the data\n",
    "create_table_query = \"\"\"\n",
    "DROP TABLE IF EXISTS amazon_date;\n",
    "CREATE TABLE IF NOT EXISTS amazon_data (\n",
    "    product TEXT NOT NULL,\n",
    "    price NUMERIC NOT NULL,\n",
    "    ratings NUMERIC NOT NULL,\n",
    "    reviews NUMERIC,\n",
    "    review_responders NUMERIC,\n",
    "    category TEXT NOT NULL\n",
    ")\n",
    "\"\"\"\n",
    "cur.execute(create_table_query)\n",
    "conn.commit()\n",
    "\n",
    "# Function to clean and format data\n",
    "def clean_format_data(row):\n",
    "    # Remove commas from 'reviews' and 'price' fields\n",
    "    reviews = str(row['reviews']).replace(',', '')\n",
    "    review_responders = str(row['review_responders']).replace(',', '')\n",
    "    price = str(row['price']).replace(',', '')\n",
    "\n",
    "    # Convert reviews to int, if not possible set to 0\n",
    "    try:\n",
    "        reviews = int(reviews)\n",
    "    except ValueError:\n",
    "        reviews = 0\n",
    "    \n",
    "    # Convert reviews to int, if not possible set to 0\n",
    "    try:\n",
    "        review_responders = int(review_responders)\n",
    "    except ValueError:\n",
    "        review_responders = 0\n",
    "\n",
    "    # Convert the ratings value to a float\n",
    "    ratings = float(row['ratings'])\n",
    "\n",
    "    # Adapt data to correct format for SQL insertion and remove quotes\n",
    "    product = psycopg2.extensions.adapt(row['product'].encode('utf-8')).getquoted().decode('utf-8')[1:-1]\n",
    "\n",
    "    # Convert price to float, if not possible set to 0\n",
    "    try:\n",
    "        price = float(price)\n",
    "    except ValueError:\n",
    "        price = 0\n",
    "\n",
    "    category = psycopg2.extensions.adapt(row['category']).getquoted().decode('utf-8')[1:-1]\n",
    "\n",
    "    return product, price, ratings, reviews, review_responders, category\n",
    "\n",
    "# Insert the data from the pandas DataFrame into the PostgreSQL table\n",
    "for index, row in df.iterrows():\n",
    "    product, price, ratings, reviews, review_responders, category = clean_format_data(row)\n",
    "\n",
    "    insert_query = \"\"\"\n",
    "    INSERT INTO amazon_data (product, price, ratings, reviews, review_responders, category) \n",
    "    VALUES (%s, %s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "\n",
    "    cur.execute(insert_query, (product, price, ratings, reviews, review_responders, category))\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('amazon_data_cat.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NO EMPTY ROWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# Load the JSON data into a pandas DataFrame\n",
    "df = pd.read_json('amazon_data_cat.json')\n",
    "\n",
    "# Remove empty rows\n",
    "df = df.dropna()\n",
    "\n",
    "# Group by category and take the first 100 rows of each group\n",
    "df = df.groupby('category').apply(lambda x: x.head(100)).reset_index(drop=True)\n",
    "\n",
    "# Create a connection to your PostgreSQL database\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"demopass\"\n",
    ")\n",
    "\n",
    "# Create a cursor object\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Create a table in your PostgreSQL database to store the data\n",
    "create_table_query = \"\"\"\n",
    "DROP TABLE IF EXISTS amazon_date;\n",
    "CREATE TABLE IF NOT EXISTS amazon_data (\n",
    "    product TEXT NOT NULL,\n",
    "    price NUMERIC NOT NULL,\n",
    "    ratings NUMERIC NOT NULL,\n",
    "    reviews NUMERIC,\n",
    "    review_responders NUMERIC,\n",
    "    category TEXT NOT NULL\n",
    ")\n",
    "\"\"\"\n",
    "cur.execute(create_table_query)\n",
    "conn.commit()\n",
    "\n",
    "# Function to clean and format data\n",
    "def clean_format_data(row):\n",
    "    # Remove commas from 'reviews' and 'price' fields\n",
    "    reviews = str(row['reviews']).replace(',', '')\n",
    "    review_responders = str(row['review_responders']).replace(',', '')\n",
    "    price = str(row['price']).replace(',', '')\n",
    "\n",
    "    # Convert reviews to int, if not possible set to 0\n",
    "    try:\n",
    "        reviews = int(reviews)\n",
    "    except ValueError:\n",
    "        reviews = 0\n",
    "    \n",
    "    # Convert reviews to int, if not possible set to 0\n",
    "    try:\n",
    "        review_responders = int(review_responders)\n",
    "    except ValueError:\n",
    "        review_responders = 0\n",
    "\n",
    "    # Convert the ratings value to a float\n",
    "    ratings = float(row['ratings'])\n",
    "\n",
    "    # Adapt data to correct format for SQL insertion and remove quotes\n",
    "    product = psycopg2.extensions.adapt(row['product'].encode('utf-8')).getquoted().decode('utf-8')[1:-1]\n",
    "\n",
    "    # Convert price to float, if not possible set to 0\n",
    "    try:\n",
    "        price = float(price)\n",
    "    except ValueError:\n",
    "        price = 0\n",
    "\n",
    "    category = psycopg2.extensions.adapt(row['category']).getquoted().decode('utf-8')[1:-1]\n",
    "\n",
    "    return product, price, ratings, reviews, review_responders, category\n",
    "\n",
    "# Insert the data from the pandas DataFrame into the PostgreSQL table\n",
    "for index, row in df.iterrows():\n",
    "    product, price, ratings, reviews, review_responders, category = clean_format_data(row)\n",
    "\n",
    "    insert_query = \"\"\"\n",
    "    INSERT INTO amazon_data (product, price, ratings, reviews, review_responders, category) \n",
    "    VALUES (%s, %s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "\n",
    "    cur.execute(insert_query, (product, price, ratings, reviews, review_responders, category))\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('amazon_data.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
