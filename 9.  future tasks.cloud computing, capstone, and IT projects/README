DATA ENGINEERING
CAPSTONE PROJECTS
PART 1

Introduction:

The COVID-19 pandemic has had far-reaching consequences,
causing significant loss of lives and disruptions to societies.
This project delves into the analysis of sample data pertaining
to COVID-19 cases recorded between January 2019 and
December 2020. The dataset is provided in CSV format.

https://drive.google.com/file/d/1SzmRIwlpL5PrFuaUe_1TAcMV0HYHMD_b/view

Instructions:
It is essential to utilize PostgreSQL as the chosen database tool.
1. Establish a database named 'covid_19_data' and create a table
within it named 'covid_19_data' to house the dataset.
2. Modify the data type of 'ObservationDate' to 'DATE' within the
database, as opposed to its initial 'String' representation.
3. Utilize a Python script to procure the 'Covid_19_data.csv' file
and subsequently load it into the PostgreSQL database.
4. Employ PostgreSQL PG4 Admin for the creation and execution
of SQL queries.
5. Present all queries within a singular SQL file.

Your Task:
1. Devise a concise Python script to download the 'Covid_19_data.csv' file via this link.
2. Employ this script to upload the data into the previously established PostgreSQL
table.
3. Construct relevant SQL queries to dissect and uncover insights from the data. The
ensuing inquiries warrant your attention:
a. Retrieve the cumulative counts of confirmed, deceased, and recovered cases.
b. Extract the aggregate counts of confirmed, deceased, and recovered cases for
the first quarter of each observation year.
c. Formulate a comprehensive summary encompassing the following for each
country:
Total confirmed cases
Total deaths
Total recoveries
d. Determine the percentage increase in the number of death cases from 2019
to 2020.
e. Compile data for the top 5 countries with the highest confirmed cases.
f. Calculate the net change (increase or decrease) in confirmed cases on a
monthly basis over the two-year period.

Submission:
Conclude by combining all queries within a singular .sql file. Create a GitHub
repository containing the following:
1. A solitary SQL file containing all formulated queries.
2. An "outputs" directory within the repository, housing screenshots of query
outputs.
3. A distinct Python file designated for data download and loading into PostgreSQL.
Consequently, furnish a GitHub repository link as your submission.



PART 2
World Port Index Data Migration
Introduction:

GoFrieghts, a prominent logistics company, has enlisted your
expertise as a Data Engineer. Your remit involves migrating
World Port Index data from an archaic Access database to a
contemporary PostgreSQL relational database. The resultant
analysis will facilitate the creation of data marts.

https://drive.google.com/file/d/1VyCGCAfFuEK7vB1C9Vq8iPdgBdu-LDM4/view

Your Task:

Construct an Extract Load (EL) pipeline in Python to transition
the World Port Index data from the Access database to
PostgreSQL.

Questions:
For each question posed below, compose an individual Python script that generates
a corresponding table in your PostgreSQL database.
1. Establish the five nearest ports to Singapore's JURONG ISLAND port (country =
'SG', port_name = 'JURONG ISLAND'). The output should encompass 'port_name'
and 'distance_in_meters' exclusively.
2. Determine the country with the highest count of ports boasting a cargo_wharf.
The outcome should encapsulate 'country' and 'port_count' solely.
3. Respond to a distress call situated at lat: 32.610982, long: -38.706256 within the
middle of the North Atlantic Ocean. The caller requires the nearest port offering
provisions, water, fuel oil, and diesel. Your solution should encompass 'country',
'port_name', 'port_latitude', and 'port_longitude'.

Submission:

Your submission mandates the presentation of a solitary
GitHub repository link. Please ensure two separate
repositories, each furnished with a README elucidating your
code's implementation and operational guidelines.